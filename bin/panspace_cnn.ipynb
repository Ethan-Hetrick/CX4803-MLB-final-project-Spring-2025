{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95d1cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightning.pytorch as pl\n",
    "import torchmetrics\n",
    "from torchmetrics import F1Score\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a491315",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586f12d8",
   "metadata": {},
   "source": [
    "### Modified pre-processing per the panspace paper\n",
    "\n",
    "- Removed flattening of FCGRs\n",
    "- Normalized FCGRS to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d1899b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scicomp/scratch/rqu4/tmp/ipykernel_2254263/3979613452.py:21: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  sample_id = row[0]\n",
      "/scicomp/scratch/rqu4/tmp/ipykernel_2254263/3979613452.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label = row[1]\n"
     ]
    }
   ],
   "source": [
    "# --- Load data ---\n",
    "mlst_train_df = pd.read_csv('../assets/mlst_train_set.csv')\n",
    "mlst_test_df = pd.read_csv('../assets/mlst_test_set.csv')\n",
    "mlst_val_df = pd.read_csv('../assets/mlst_val_set.csv')\n",
    "\n",
    "serotype_train_df = pd.read_csv('../assets/serotype_train_set.csv')\n",
    "serotype_test_df = pd.read_csv('../assets/serotype_test_set.csv')\n",
    "serotype_val_df = pd.read_csv('../assets/serotype_val_set.csv')\n",
    "\n",
    "subspecies_train_df = pd.read_csv('../assets/subspecies_train_set.csv')\n",
    "subspecies_test_df = pd.read_csv('../assets/subspecies_test_set.csv')\n",
    "subspecies_val_df = pd.read_csv('../assets/subspecies_val_set.csv')\n",
    "\n",
    "kmc5_arrays = os.path.expanduser('~/PROJECTS/GaTech/FCGR_classifier/salmonella_kmc5_arrays/')\n",
    "kmc7_arrays = os.path.expanduser('~/PROJECTS/GaTech/FCGR_classifier/salmonella_kmc7_arrays/')\n",
    "\n",
    "def load_kmer_arrays(df, array_dir, suffix):\n",
    "    arrays = []\n",
    "    labels = []\n",
    "    for idx, row in df.iterrows():\n",
    "        sample_id = row[0]\n",
    "        label = row[1]\n",
    "        array_path = os.path.join(array_dir, f\"{sample_id}{suffix}.npy\")\n",
    "        if os.path.exists(array_path):\n",
    "            # No need to flatten for CNN input\n",
    "            array = np.load(array_path)#.flatten()\n",
    "\n",
    "            # Normalize array to [0, 1] as per panpace paper\n",
    "            array = array / array.max()\n",
    "\n",
    "            arrays.append(array)\n",
    "            labels.append(label)\n",
    "        else:\n",
    "            print(f\"Warning: Array file {array_path} not found.\")\n",
    "    return np.array(arrays), np.array(labels)\n",
    "\n",
    "# MLST\n",
    "X_train_mlst_5, y_train_mlst_5 = load_kmer_arrays(mlst_train_df, kmc5_arrays, '_k5_k5')\n",
    "X_val_mlst_5, y_val_mlst_5 = load_kmer_arrays(mlst_val_df, kmc5_arrays, '_k5_k5')\n",
    "X_test_mlst_5, y_test_mlst_5 = load_kmer_arrays(mlst_test_df, kmc5_arrays, '_k5_k5')\n",
    "\n",
    "X_train_mlst_7, y_train_mlst_7 = load_kmer_arrays(mlst_train_df, kmc7_arrays, '_k7_k7')\n",
    "X_val_mlst_7, y_val_mlst_7 = load_kmer_arrays(mlst_val_df, kmc7_arrays, '_k7_k7')\n",
    "X_test_mlst_7, y_test_mlst_7 = load_kmer_arrays(mlst_test_df, kmc7_arrays, '_k7_k7')\n",
    "\n",
    "# Serotype\n",
    "X_train_sero_5, y_train_sero_5 = load_kmer_arrays(serotype_train_df, kmc5_arrays, '_k5_k5')\n",
    "X_val_sero_5, y_val_sero_5 = load_kmer_arrays(serotype_val_df, kmc5_arrays, '_k5_k5')\n",
    "X_test_sero_5, y_test_sero_5 = load_kmer_arrays(serotype_test_df, kmc5_arrays, '_k5_k5')\n",
    "\n",
    "X_train_sero_7, y_train_sero_7 = load_kmer_arrays(serotype_train_df, kmc7_arrays, '_k7_k7')\n",
    "X_val_sero_7, y_val_sero_7 = load_kmer_arrays(serotype_val_df, kmc7_arrays, '_k7_k7')\n",
    "X_test_sero_7, y_test_sero_7 = load_kmer_arrays(serotype_test_df, kmc7_arrays, '_k7_k7')\n",
    "\n",
    "# Subspecies\n",
    "X_train_sub_5, y_train_sub_5 = load_kmer_arrays(subspecies_train_df, kmc5_arrays, '_k5_k5')\n",
    "X_val_sub_5, y_val_sub_5 = load_kmer_arrays(subspecies_val_df, kmc5_arrays, '_k5_k5')\n",
    "X_test_sub_5, y_test_sub_5 = load_kmer_arrays(subspecies_test_df, kmc5_arrays, '_k5_k5')\n",
    "\n",
    "X_train_sub_7, y_train_sub_7 = load_kmer_arrays(subspecies_train_df, kmc7_arrays, '_k7_k7')\n",
    "X_val_sub_7, y_val_sub_7 = load_kmer_arrays(subspecies_val_df, kmc7_arrays, '_k7_k7')\n",
    "X_test_sub_7, y_test_sub_7 = load_kmer_arrays(subspecies_test_df, kmc7_arrays, '_k7_k7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98c04a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11731637 0.11785498 0.16777755 ... 0.32464822 0.50700195 0.86235104]\n",
      " [0.1970309  0.         0.13293611 ... 0.33797886 0.63000741 0.53921767]\n",
      " [0.44189726 0.21921497 0.14003905 ... 0.39298458 0.18720124 0.56749478]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.133946   0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "(32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Check to make sure it is normalized\n",
    "print(X_train_sub_5[0])\n",
    "print(X_train_sub_5[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77aee67",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "These are the hyperparameters I was able to glean from the panspace paper <https://www.biorxiv.org/content/10.1101/2025.03.19.644115v3> and their code <https://github.com/pg-space/panspace/blob/master/src/panspace/dnn/models/cnn_fcgr_levels.py>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f12d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001 # From paper\n",
    "latent_dim = 128 # From code\n",
    "hidden_activation = 'relu' # From code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed5c08",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "Panspace used a combination of a Rectified Adam optimizer and Lookahead optimizer called \"Ranger\" optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ddd1acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 25.2 from /scicomp/home-pure/rqu4/.conda/envs/pytorch-final-project/lib/python3.13/site-packages/pip (python 3.13)\n",
      "Collecting torch-optimi\n",
      "  Obtaining dependency information for torch-optimi from https://files.pythonhosted.org/packages/a6/92/4b445290b0d8350273f26576b52b8034e19e993a78563156d4b302bb3d2a/torch_optimi-0.3.2-py3-none-any.whl.metadata\n",
      "  Downloading torch_optimi-0.3.2-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: torch>=2.2 in /scicomp/home-pure/rqu4/.conda/envs/pytorch-final-project/lib/python3.13/site-packages (from torch-optimi) (2.8.0)\n",
      "Requirement already satisfied: filelock in /scicomp/home-pure/rqu4/.conda/envs/pytorch-final-project/lib/python3.13/site-packages (from torch>=2.2->torch-optimi) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /scicomp/home-pure/rqu4/cache/python/lib/python3.13/site-packages (from torch>=2.2->torch-optimi) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /scicomp/home-pure/rqu4/cache/python/lib/python3.13/site-packages (from torch>=2.2->torch-optimi) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /scicomp/home-pure/rqu4/.conda/envs/pytorch-final-project/lib/python3.13/site-packages (from torch>=2.2->torch-optimi) (1.14.0)\n",
      "Requirement already satisfied: networkx in /scicomp/home-pure/rqu4/.conda/envs/pytorch-final-project/lib/python3.13/site-packages (from torch>=2.2->torch-optimi) (3.5)\n",
      "Requirement already satisfied: jinja2 in /scicomp/home-pure/rqu4/.conda/envs/pytorch-final-project/lib/python3.13/site-packages (from torch>=2.2->torch-optimi) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /scicomp/home-pure/rqu4/.conda/envs/pytorch-final-project/lib/python3.13/site-packages (from torch>=2.2->torch-optimi) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /scicomp/home-pure/rqu4/.conda/envs/pytorch-final-project/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.2->torch-optimi) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /scicomp/home-pure/rqu4/cache/python/lib/python3.13/site-packages (from jinja2->torch>=2.2->torch-optimi) (3.0.3)\n",
      "Downloading torch_optimi-0.3.2-py3-none-any.whl (49 kB)\n",
      "Installing collected packages: torch-optimi\n",
      "Successfully installed torch-optimi-0.3.2\n"
     ]
    }
   ],
   "source": [
    "# Optimi has an implementation <https://optimi.benjaminwarner.dev/optimizers/ranger/>\n",
    "!pip install torch-optimi\n",
    "\n",
    "from optimi import Ranger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f778fca",
   "metadata": {},
   "source": [
    "## Panspace architecture implementation\n",
    "\n",
    "Attempted to install tensorflow for an apples-to-apples comparison but failed due to incompatibility with my GPU and lack of clear documentation on building the environment from panspace. Going with oytorch instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05625249",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNFCGR(pl.LightningModule):\n",
    "    def __init__(self, kmer=5, latent_dim=128, hidden_activation='relu', batch_normalization=True, learning_rate=1e-3, num_classes=10):\n",
    "        super(CNNFCGR, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.kmer = kmer\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Activation function\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        # Input shape: (batch, 1, 2**k, 2**k)\n",
    "        in_channels = 1\n",
    "        rows = 2 ** kmer\n",
    "\n",
    "        # Level 2 convolution\n",
    "        level2_kernel = 2 ** 2\n",
    "        level2_stride = 2 ** 2\n",
    "        filters_level2 = 4 * kmer\n",
    "        self.conv1 = nn.Conv2d(in_channels, filters_level2, kernel_size=level2_kernel, stride=level2_stride)\n",
    "        self.bn1 = nn.BatchNorm2d(filters_level2)\n",
    "\n",
    "        # Level 1 convolution\n",
    "        level1_kernel = 2 ** 1\n",
    "        level1_stride = 2 ** 1\n",
    "        filters_level1 = 4 * (kmer - 1)\n",
    "        self.conv2 = nn.Conv2d(filters_level2, filters_level1, kernel_size=level1_kernel, stride=level1_stride)\n",
    "        self.bn2 = nn.BatchNorm2d(filters_level1)\n",
    "\n",
    "        # Flatten and dense\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, rows, rows)\n",
    "            dummy_out = self.forward_features(dummy)\n",
    "            flattened_dim = dummy_out.shape[1]\n",
    "\n",
    "        self.fc = nn.Linear(flattened_dim, num_classes)\n",
    "\n",
    "        # Macro F1 metrics\n",
    "        self.train_f1 = torchmetrics.F1Score(task='multiclass', num_classes=num_classes, average='macro')\n",
    "        self.val_f1   = torchmetrics.F1Score(task='multiclass', num_classes=num_classes, average='macro')\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        # flatten\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # final dense layer: outputs latent_dim features\n",
    "        logits = self.fc(x)  # treat this as logits for classification\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Ranger(self.parameters(), lr=self.learning_rate, weight_decay=1e-4)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch  # x: input, y: one-hot labels\n",
    "        logits = self(x)  # forward pass\n",
    "\n",
    "        # If y is one-hot, convert to class indices\n",
    "        if y.ndim == 2:\n",
    "            y = torch.argmax(y, dim=1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        # Compute macro F1\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        f1 = self.train_f1(preds, y)\n",
    "\n",
    "        # Log loss and F1\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('train_macro_f1', f1, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        if y.ndim == 2:\n",
    "            y = torch.argmax(y, dim=1)\n",
    "\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        f1 = self.val_f1(preds, y)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('val_macro_f1', f1, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        if y.ndim == 2:\n",
    "            y = torch.argmax(y, dim=1)\n",
    "\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        f1 = self.val_f1(preds, y)  # or define a separate test_f1 metric if you want\n",
    "\n",
    "        self.log('test_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('test_macro_f1', f1, prog_bar=True, on_step=False, on_epoch=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66013bd",
   "metadata": {},
   "source": [
    "## Prepare data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "787b8c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Scale classes for integer labels\n",
    "# -----------------------------\n",
    "def scale(y):\n",
    "    \"\"\"\n",
    "    Converts string labels to integer indices starting from 0.\n",
    "    Returns scaled labels and the mapping (class -> index).\n",
    "    \"\"\"\n",
    "    classes = np.sort(np.unique(y))\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "    y_scaled = np.array([class_to_idx[label] for label in y], dtype=np.int64)\n",
    "    return y_scaled, classes\n",
    "\n",
    "def prepare_data(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    # Convert labels to integer indices\n",
    "    y_train_scaled, classes = scale(y_train)\n",
    "    y_val_scaled, _ = scale(y_val)\n",
    "    y_test_scaled, _ = scale(y_test)\n",
    "\n",
    "    # Convert inputs to tensors and add channel dimension\n",
    "    # Assume X_* are already (num_samples, H, W)\n",
    "    X_train_tensor = torch.tensor(X_train).unsqueeze(1).float()  # (N, 1, H, W)\n",
    "    X_val_tensor   = torch.tensor(X_val).unsqueeze(1).float()\n",
    "    X_test_tensor  = torch.tensor(X_test).unsqueeze(1).float()\n",
    "\n",
    "    y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.long)\n",
    "    y_val_tensor   = torch.tensor(y_val_scaled, dtype=torch.long)\n",
    "    y_test_tensor  = torch.tensor(y_test_scaled, dtype=torch.long)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset   = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d17027",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd69581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(x_train, X_val, X_test, y_train, y_val, y_test, kmer=5):\n",
    "    # -----------------------------\n",
    "    # Prepare data\n",
    "    # -----------------------------\n",
    "    train_loader, val_loader, test_loader, classes = prepare_data(\n",
    "        x_train, X_val, X_test,\n",
    "        y_train, y_val, y_test\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Instantiate model\n",
    "    # -----------------------------\n",
    "    num_classes = len(classes)      # number of unique labels\n",
    "\n",
    "    model = CNNFCGR(kmer=kmer, latent_dim=128, learning_rate=1e-3, num_classes=num_classes)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Train model\n",
    "    # -----------------------------\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        enable_progress_bar=True\n",
    "    )\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Test model\n",
    "    # -----------------------------\n",
    "    results = trainer.test(model, dataloaders=test_loader)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8838fdb8",
   "metadata": {},
   "source": [
    "## Run on kmer=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1fafd02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type              | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | act      | ReLU              | 0      | train\n",
      "1 | conv1    | Conv2d            | 340    | train\n",
      "2 | bn1      | BatchNorm2d       | 40     | train\n",
      "3 | conv2    | Conv2d            | 1.3 K  | train\n",
      "4 | bn2      | BatchNorm2d       | 32     | train\n",
      "5 | fc       | Linear            | 59.6 K | train\n",
      "6 | train_f1 | MulticlassF1Score | 0      | train\n",
      "7 | val_f1   | MulticlassF1Score | 0      | train\n",
      "-------------------------------------------------------\n",
      "61.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "61.3 K    Total params\n",
      "0.245     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:02<00:00, 112.70it/s, v_num=7, val_loss=0.310, val_macro_f1=0.827, train_loss=0.198, train_macro_f1=0.886]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:02<00:00, 110.79it/s, v_num=7, val_loss=0.310, val_macro_f1=0.827, train_loss=0.198, train_macro_f1=0.886]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:00<00:00, 226.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.3091360032558441     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">       test_macro_f1       </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.8389042019844055     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.3091360032558441    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m      test_macro_f1      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.8389042019844055    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type              | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | act      | ReLU              | 0      | train\n",
      "1 | conv1    | Conv2d            | 340    | train\n",
      "2 | bn1      | BatchNorm2d       | 40     | train\n",
      "3 | conv2    | Conv2d            | 1.3 K  | train\n",
      "4 | bn2      | BatchNorm2d       | 32     | train\n",
      "5 | fc       | Linear            | 26.7 K | train\n",
      "6 | train_f1 | MulticlassF1Score | 0      | train\n",
      "7 | val_f1   | MulticlassF1Score | 0      | train\n",
      "-------------------------------------------------------\n",
      "28.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "28.4 K    Total params\n",
      "0.114     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:00<00:00, 113.53it/s, v_num=8, val_loss=0.896, val_macro_f1=0.696, train_loss=0.235, train_macro_f1=0.907]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:00<00:00, 109.92it/s, v_num=8, val_loss=0.896, val_macro_f1=0.696, train_loss=0.235, train_macro_f1=0.907]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 228.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.8715744018554688     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">       test_macro_f1       </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.6924231648445129     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.8715744018554688    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m      test_macro_f1      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.6924231648445129    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type              | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | act      | ReLU              | 0      | train\n",
      "1 | conv1    | Conv2d            | 340    | train\n",
      "2 | bn1      | BatchNorm2d       | 40     | train\n",
      "3 | conv2    | Conv2d            | 1.3 K  | train\n",
      "4 | bn2      | BatchNorm2d       | 32     | train\n",
      "5 | fc       | Linear            | 1.5 K  | train\n",
      "6 | train_f1 | MulticlassF1Score | 0      | train\n",
      "7 | val_f1   | MulticlassF1Score | 0      | train\n",
      "-------------------------------------------------------\n",
      "3.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scicomp/home-pure/rqu4/cache/python/lib/python3.13/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 92.81it/s, v_num=9, val_loss=0.623, val_macro_f1=0.931, train_loss=0.626, train_macro_f1=0.858]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 72.30it/s, v_num=9, val_loss=0.623, val_macro_f1=0.931, train_loss=0.626, train_macro_f1=0.858]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 186.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.6471318006515503     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">       test_macro_f1       </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">     0.88706374168396      </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.6471318006515503    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m      test_macro_f1      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m    0.88706374168396     \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K5 complete\n"
     ]
    }
   ],
   "source": [
    "results_k5 = []\n",
    "\n",
    "# -----------------------------\n",
    "# MLST\n",
    "# -----------------------------\n",
    "results = run_model(\n",
    "    X_train_mlst_5, X_val_mlst_5, X_test_mlst_5,\n",
    "    y_train_mlst_5, y_val_mlst_5, y_test_mlst_5,\n",
    "    kmer=5\n",
    ")\n",
    "f1_val = results[0].get('val_macro_f1', None)\n",
    "f1_test = results[0]['test_macro_f1']\n",
    "\n",
    "results_k5.append({\n",
    "    \"dataset\": \"MLST\",\n",
    "    \"kmer\": 5,\n",
    "    \"f1_val_macro\": f1_val,\n",
    "    \"f1_test_macro\": f1_test\n",
    "})\n",
    "\n",
    "# -----------------------------\n",
    "# Serotype\n",
    "# -----------------------------\n",
    "results = run_model(\n",
    "    X_train_sero_5, X_val_sero_5, X_test_sero_5,\n",
    "    y_train_sero_5, y_val_sero_5, y_test_sero_5,\n",
    "    kmer=5\n",
    ")\n",
    "f1_val = results[0].get('val_macro_f1', None)\n",
    "f1_test = results[0]['test_macro_f1']\n",
    "\n",
    "results_k5.append({\n",
    "    \"dataset\": \"Serotype\",\n",
    "    \"kmer\": 5,\n",
    "    \"f1_val_macro\": f1_val,\n",
    "    \"f1_test_macro\": f1_test\n",
    "})\n",
    "\n",
    "# -----------------------------\n",
    "# Subspecies\n",
    "# -----------------------------\n",
    "results = run_model(\n",
    "    X_train_sub_5, X_val_sub_5, X_test_sub_5,\n",
    "    y_train_sub_5, y_val_sub_5, y_test_sub_5,\n",
    "    kmer=5\n",
    ")\n",
    "f1_val = results[0].get('val_macro_f1', None)\n",
    "f1_test = results[0]['test_macro_f1']\n",
    "\n",
    "results_k5.append({\n",
    "    \"dataset\": \"Subspecies\",\n",
    "    \"kmer\": 5,\n",
    "    \"f1_val_macro\": f1_val,\n",
    "    \"f1_test_macro\": f1_test\n",
    "})\n",
    "\n",
    "# -----------------------------\n",
    "# Save k5 results\n",
    "# -----------------------------\n",
    "results_df_k5 = pd.DataFrame(results_k5)\n",
    "results_df_k5.to_csv(\"../results/panspace_cnn_macro_f1_results_k5.csv\", index=False)\n",
    "print(\"K5 complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be3927",
   "metadata": {},
   "source": [
    "## Run on kmer=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15bca748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type              | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | act      | ReLU              | 0      | train\n",
      "1 | conv1    | Conv2d            | 476    | train\n",
      "2 | bn1      | BatchNorm2d       | 56     | train\n",
      "3 | conv2    | Conv2d            | 2.7 K  | train\n",
      "4 | bn2      | BatchNorm2d       | 48     | train\n",
      "5 | fc       | Linear            | 1.4 M  | train\n",
      "6 | train_f1 | MulticlassF1Score | 0      | train\n",
      "7 | val_f1   | MulticlassF1Score | 0      | train\n",
      "-------------------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.716     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:02<00:00, 99.12it/s, v_num=10, val_loss=0.069, val_macro_f1=0.958, train_loss=0.0447, train_macro_f1=0.971]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:02<00:00, 91.15it/s, v_num=10, val_loss=0.069, val_macro_f1=0.958, train_loss=0.0447, train_macro_f1=0.971]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:00<00:00, 183.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.07062975317239761    </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">       test_macro_f1       </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.9571107029914856     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.07062975317239761   \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m      test_macro_f1      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.9571107029914856    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type              | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | act      | ReLU              | 0      | train\n",
      "1 | conv1    | Conv2d            | 476    | train\n",
      "2 | bn1      | BatchNorm2d       | 56     | train\n",
      "3 | conv2    | Conv2d            | 2.7 K  | train\n",
      "4 | bn2      | BatchNorm2d       | 48     | train\n",
      "5 | fc       | Linear            | 639 K  | train\n",
      "6 | train_f1 | MulticlassF1Score | 0      | train\n",
      "7 | val_f1   | MulticlassF1Score | 0      | train\n",
      "-------------------------------------------------------\n",
      "642 K     Trainable params\n",
      "0         Non-trainable params\n",
      "642 K     Total params\n",
      "2.569     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:01<00:00, 98.19it/s, v_num=11, val_loss=0.124, val_macro_f1=0.956, train_loss=0.0219, train_macro_f1=0.988]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:01<00:00, 87.82it/s, v_num=11, val_loss=0.124, val_macro_f1=0.956, train_loss=0.0219, train_macro_f1=0.988]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 188.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.09244489669799805    </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">       test_macro_f1       </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.9584417939186096     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.09244489669799805   \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m      test_macro_f1      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.9584417939186096    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type              | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | act      | ReLU              | 0      | train\n",
      "1 | conv1    | Conv2d            | 476    | train\n",
      "2 | bn1      | BatchNorm2d       | 56     | train\n",
      "3 | conv2    | Conv2d            | 2.7 K  | train\n",
      "4 | bn2      | BatchNorm2d       | 48     | train\n",
      "5 | fc       | Linear            | 36.9 K | train\n",
      "6 | train_f1 | MulticlassF1Score | 0      | train\n",
      "7 | val_f1   | MulticlassF1Score | 0      | train\n",
      "-------------------------------------------------------\n",
      "40.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "40.2 K    Total params\n",
      "0.161     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 82.57it/s, v_num=12, val_loss=0.0191, val_macro_f1=1.000, train_loss=0.0187, train_macro_f1=1.000] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 56.47it/s, v_num=12, val_loss=0.0191, val_macro_f1=1.000, train_loss=0.0187, train_macro_f1=1.000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 135.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.02224724553525448    </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">       test_macro_f1       </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">            1.0            </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.02224724553525448   \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m      test_macro_f1      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m           1.0           \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K7 complete\n"
     ]
    }
   ],
   "source": [
    "results_k7 = []\n",
    "\n",
    "# -----------------------------\n",
    "# MLST\n",
    "# -----------------------------\n",
    "results = run_model(\n",
    "    X_train_mlst_7, X_val_mlst_7, X_test_mlst_7,\n",
    "    y_train_mlst_7, y_val_mlst_7, y_test_mlst_7,\n",
    "    kmer=7\n",
    ")\n",
    "f1_val = results[0].get('val_macro_f1', None)\n",
    "f1_test = results[0]['test_macro_f1']\n",
    "\n",
    "results_k7.append({\n",
    "    \"dataset\": \"MLST\",\n",
    "    \"kmer\": 7,\n",
    "    \"f1_val_macro\": f1_val,\n",
    "    \"f1_test_macro\": f1_test\n",
    "})\n",
    "\n",
    "# -----------------------------\n",
    "# Serotype\n",
    "# -----------------------------\n",
    "results = run_model(\n",
    "    X_train_sero_7, X_val_sero_7, X_test_sero_7,\n",
    "    y_train_sero_7, y_val_sero_7, y_test_sero_7,\n",
    "    kmer=7\n",
    ")\n",
    "f1_val = results[0].get('val_macro_f1', None)\n",
    "f1_test = results[0]['test_macro_f1']\n",
    "\n",
    "results_k7.append({\n",
    "    \"dataset\": \"Serotype\",\n",
    "    \"kmer\": 7,\n",
    "    \"f1_val_macro\": f1_val,\n",
    "    \"f1_test_macro\": f1_test\n",
    "})\n",
    "\n",
    "# -----------------------------\n",
    "# Subspecies\n",
    "# -----------------------------\n",
    "results = run_model(\n",
    "    X_train_sub_7, X_val_sub_7, X_test_sub_7,\n",
    "    y_train_sub_7, y_val_sub_7, y_test_sub_7,\n",
    "    kmer=7\n",
    ")\n",
    "f1_val = results[0].get('val_macro_f1', None)\n",
    "f1_test = results[0]['test_macro_f1']\n",
    "\n",
    "results_k7.append({\n",
    "    \"dataset\": \"Subspecies\",\n",
    "    \"kmer\": 7,\n",
    "    \"f1_val_macro\": f1_val,\n",
    "    \"f1_test_macro\": f1_test\n",
    "})\n",
    "\n",
    "# -----------------------------\n",
    "# Save k7 results\n",
    "# -----------------------------\n",
    "results_df_k7 = pd.DataFrame(results_k7)\n",
    "results_df_k7.to_csv(\"../results/panspace_cnn_macro_f1_results_k7.csv\", index=False)\n",
    "print(\"K7 complete\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
