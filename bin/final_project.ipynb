{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d88da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import os\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ebb2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='salmonella_data/data/'\n",
    "kmc5_arrays='salmonella_kmc5_arrays/'\n",
    "kmc7_arrays='salmonella_kmc7_arrays/'\n",
    "mlst_results = data_dir + 'mlst_results_combined.tsv'\n",
    "seqsero_results = data_dir + 'seqsero2_results.tsv'\n",
    "quast_result = data_dir + 'quast_results_combined.tsv'\n",
    "mlst_df = pd.read_csv(mlst_results, sep='\\t', header=None)\n",
    "seqsero_df = pd.read_csv(seqsero_results, sep='\\t')\n",
    "quast_df = pd.read_csv(quast_result, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3e749a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.94561379299148 % of samples retained after N50 filtering\n",
      "98.45705018821108 % of samples retained after mlst filtering\n",
      "92.41693518679263 % of samples retained after merging quast and mlst filtered dataframes\n",
      "436524 # samples retained after merging quast and mlst filtered dataframes\n",
      "5.377459552612303 % of samples retained after sampling 50 samples from each sequence type\n",
      "25400 # samples retained after sampling 50 samples from each sequence type\n",
      "254 # unique sequence types\n",
      "16256 # samples in training set\n",
      "4064 # samples in validation set\n",
      "5080 # samples in test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11237/763891927.py:26: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mlst_df_filt = mlst_df_filt.groupby(mlst_df_filt.columns[1]).apply(lambda x: x.sample(100, random_state=42)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# --- PREPROCESSING ---\n",
    "\n",
    "# Remove very low quality assemblies\n",
    "quast_df_filt=quast_df[quast_df['N50'] >= 10000]['Assembly']\n",
    "print(quast_df_filt.shape[0]/quast_df.shape[0]*100, \"% of samples retained after N50 filtering\")\n",
    "\n",
    "# Remove samples with no MLST calls\n",
    "mlst_df_filt = mlst_df[mlst_df.iloc[:, 2] != '-'].iloc[:, [0, 2]]\n",
    "print(mlst_df_filt.shape[0]/mlst_df.shape[0]*100, \"% of samples retained after mlst filtering\")\n",
    "\n",
    "# Basename the first column of mlst_df_filt\n",
    "mlst_df_filt.iloc[:, 0] = mlst_df_filt.iloc[:, 0].apply(lambda x: x.split('/')[-1].replace('.fna', ''))\n",
    "\n",
    "# Keep only samples with 50 replicates of the same sequence type\n",
    "mlst_counts = mlst_df_filt.iloc[:, 1].value_counts()\n",
    "mlst_df_filt = mlst_df_filt[mlst_df_filt.iloc[:, 1].isin(mlst_counts[mlst_counts >= 100].index)]\n",
    "\n",
    "# Keep only samples present in both quast and mlst filtered dataframes\n",
    "common_samples = set(quast_df_filt).intersection(set(mlst_df_filt.iloc[:, 0]))\n",
    "mlst_df_filt = mlst_df_filt[mlst_df_filt.iloc[:, 0].isin(common_samples)]\n",
    "\n",
    "print(mlst_df_filt.shape[0]/mlst_df.shape[0]*100, \"% of samples retained after merging quast and mlst filtered dataframes\")\n",
    "print(mlst_df_filt.shape[0], \"# samples retained after merging quast and mlst filtered dataframes\")\n",
    "\n",
    "# Randomly sample 100 samples from each sequence type\n",
    "mlst_df_filt = mlst_df_filt.groupby(mlst_df_filt.columns[1]).apply(lambda x: x.sample(100, random_state=42)).reset_index(drop=True)\n",
    "print(mlst_df_filt.shape[0]/mlst_df.shape[0]*100, \"% of samples retained after sampling 50 samples from each sequence type\")\n",
    "print(mlst_df_filt.shape[0], \"# samples retained after sampling 50 samples from each sequence type\")\n",
    "\n",
    "# Print number of unique sequence types\n",
    "print(mlst_df_filt.iloc[:, 1].nunique(), \"# unique sequence types\")\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_val_df, test_df = train_test_split(mlst_df_filt, test_size=0.2, stratify=mlst_df_filt.iloc[:, 1], random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.2, stratify=train_val_df.iloc[:, 1], random_state=42)\n",
    "print(train_df.shape[0], \"# samples in training set\")\n",
    "print(val_df.shape[0], \"# samples in validation set\")\n",
    "print(test_df.shape[0], \"# samples in test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be93b5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16256, 1024) shape of training data\n",
      "(4064, 1024) shape of validation data\n",
      "(5080, 1024) shape of test data\n",
      "(16256, 16384) shape of training data\n",
      "(4064, 16384) shape of validation data\n",
      "(5080, 16384) shape of test data\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD DATA ---\n",
    "\n",
    "# Load kmer count arrays\n",
    "def load_kmer_arrays(df, array_dir, suffix):\n",
    "    arrays = []\n",
    "    labels = []\n",
    "    for idx, row in df.iterrows():\n",
    "        sample_id = row[0]  # Assuming the first column is the sample ID\n",
    "        label = row[2]      # Assuming the third column is the label\n",
    "        array_path = os.path.join(array_dir, f\"{sample_id}{suffix}.npy\")\n",
    "        \n",
    "        if os.path.exists(array_path):\n",
    "            array = np.load(array_path)\n",
    "            # Flatten the array\n",
    "            array = array.flatten()\n",
    "            arrays.append(array)\n",
    "            labels.append(label)\n",
    "        else:\n",
    "            print(f\"Warning: Array file {array_path} not found.\")\n",
    "        \n",
    "    return np.array(arrays), np.array(labels)\n",
    "\n",
    "# Load the data\n",
    "X_train_5, y_train_5 = load_kmer_arrays(train_df, kmc5_arrays, '_k5_k5')\n",
    "X_val_5, y_val_5 = load_kmer_arrays(val_df, kmc5_arrays, '_k5_k5')\n",
    "X_test_5, y_test_5 = load_kmer_arrays(test_df, kmc5_arrays, '_k5_k5')\n",
    "\n",
    "X_train_7, y_train_7 = load_kmer_arrays(train_df, kmc7_arrays, '_k7_k7')\n",
    "X_val_7, y_val_7 = load_kmer_arrays(val_df, kmc7_arrays, '_k7_k7')\n",
    "X_test_7, y_test_7 = load_kmer_arrays(test_df, kmc7_arrays, '_k7_k7')\n",
    "\n",
    "# Print shapes of the loaded data\n",
    "print(X_train_5.shape, \"shape of training data\")\n",
    "print(X_val_5.shape, \"shape of validation data\")\n",
    "print(X_test_5.shape, \"shape of test data\")\n",
    "\n",
    "# Print shapes of the loaded data\n",
    "print(X_train_7.shape, \"shape of training data\")\n",
    "print(X_val_7.shape, \"shape of validation data\")\n",
    "print(X_test_7.shape, \"shape of test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8fb00c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16256,) shape of 5mer training ground truth\n",
      "(16256,) shape of 7mer training ground truth\n",
      "['128' '216' '814']\n",
      "254\n"
     ]
    }
   ],
   "source": [
    "print(y_train_5.shape, \"shape of 5mer training ground truth\")\n",
    "print(y_train_7.shape, \"shape of 7mer training ground truth\")\n",
    "print(y_train_5[0:3])\n",
    "print(len(np.unique(y_train_5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b019b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODELS ---\n",
    "\n",
    "# --- LOGISTIC REGRESSION MODEL ---\n",
    "def train_logistic_regression(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    # Create and train logistic regression model with standard scaler\n",
    "    model = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000, random_state=42))\n",
    "    model.fit(X_train, y_train)  # Train the model on the training set\n",
    "    y_val_pred = model.predict(X_val)  # Predict on the validation set\n",
    "\n",
    "    # Print classification report for validation set\n",
    "    print(\"Validation Set Classification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "    y_test_pred = model.predict(X_test)  # Predict on the test set\n",
    "\n",
    "    # Print classification report for test set\n",
    "    print(\"Test Set Classification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# --- XGBOOST MODEL ---\n",
    "\n",
    "def train_xgboost(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    # Encode labels if they are categorical\n",
    "    le = LabelEncoder()\n",
    "    y_train_encoded = le.fit_transform(y_train)  # Fit on training labels\n",
    "    y_val_encoded = le.transform(y_val)           # Transform validation labels\n",
    "    y_test_encoded = le.transform(y_test)         # Transform test labels\n",
    "    \n",
    "\n",
    "    # Create and fit the model\n",
    "    xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42, device=\"cuda\")\n",
    "    xgb_model.fit(X_train,y_train_encoded)\n",
    "\n",
    "    # Predictions\n",
    "    y_val_pred_xgb = xgb_model.predict(X_val)\n",
    "    print(\"XGBoost Validation Set Classification Report:\")\n",
    "    print(classification_report(y_val_encoded, y_val_pred_xgb))\n",
    "\n",
    "    y_test_pred_xgb = xgb_model.predict(X_test)\n",
    "    print(\"XGBoost Test Set Classification Report:\")\n",
    "    print(classification_report(y_test_encoded, y_test_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77fe7811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.94      0.97        16\n",
      "          10       1.00      1.00      1.00        16\n",
      "         101       1.00      1.00      1.00        16\n",
      "          11       0.87      0.81      0.84        16\n",
      "         112       0.85      0.69      0.76        16\n",
      "         118       0.87      0.81      0.84        16\n",
      "        1208       1.00      1.00      1.00        16\n",
      "        1256       1.00      1.00      1.00        16\n",
      "         126       1.00      1.00      1.00        16\n",
      "         127       1.00      0.94      0.97        16\n",
      "         128       0.73      1.00      0.84        16\n",
      "         129       0.65      0.94      0.77        16\n",
      "        1291       1.00      1.00      1.00        16\n",
      "        1298       1.00      1.00      1.00        16\n",
      "          13       1.00      1.00      1.00        16\n",
      "        1316       1.00      1.00      1.00        16\n",
      "         132       0.89      1.00      0.94        16\n",
      "         135       1.00      1.00      1.00        16\n",
      "        1370       1.00      1.00      1.00        16\n",
      "         138       0.94      1.00      0.97        16\n",
      "        1384       1.00      1.00      1.00        16\n",
      "        1392       1.00      1.00      1.00        16\n",
      "          14       1.00      1.00      1.00        16\n",
      "         142       1.00      1.00      1.00        16\n",
      "         145       1.00      1.00      1.00        16\n",
      "        1498       1.00      1.00      1.00        16\n",
      "        1499       1.00      1.00      1.00        16\n",
      "          15       1.00      1.00      1.00        16\n",
      "         150       1.00      1.00      1.00        16\n",
      "        1515       1.00      1.00      1.00        16\n",
      "         152       0.92      0.69      0.79        16\n",
      "        1541       1.00      1.00      1.00        16\n",
      "        1542       1.00      1.00      1.00        16\n",
      "        1543       1.00      1.00      1.00        16\n",
      "        1547       1.00      1.00      1.00        16\n",
      "        1549       1.00      1.00      1.00        16\n",
      "         155       1.00      1.00      1.00        16\n",
      "        1567       1.00      1.00      1.00        16\n",
      "        1575       1.00      1.00      1.00        16\n",
      "        1583       1.00      1.00      1.00        16\n",
      "        1593       1.00      1.00      1.00        16\n",
      "          16       1.00      1.00      1.00        16\n",
      "        1602       1.00      1.00      1.00        16\n",
      "        1606       0.80      1.00      0.89        16\n",
      "        1610       1.00      1.00      1.00        16\n",
      "        1612       1.00      1.00      1.00        16\n",
      "        1628       1.00      1.00      1.00        16\n",
      "         164       0.93      0.88      0.90        16\n",
      "        1659       1.00      1.00      1.00        16\n",
      "         166       1.00      1.00      1.00        16\n",
      "        1674       1.00      1.00      1.00        16\n",
      "        1675       0.84      1.00      0.91        16\n",
      "          17       1.00      1.00      1.00        16\n",
      "         174       1.00      0.94      0.97        16\n",
      "          18       1.00      1.00      1.00        16\n",
      "         180       1.00      0.88      0.93        16\n",
      "        1815       1.00      1.00      1.00        16\n",
      "         183       1.00      1.00      1.00        16\n",
      "        1838       1.00      1.00      1.00        16\n",
      "        1848       1.00      1.00      1.00        16\n",
      "         185       1.00      1.00      1.00        16\n",
      "        1891       1.00      1.00      1.00        16\n",
      "          19       0.64      0.56      0.60        16\n",
      "        1925       1.00      1.00      1.00        16\n",
      "         195       1.00      1.00      1.00        16\n",
      "        1954       1.00      0.94      0.97        16\n",
      "        1959       1.00      1.00      1.00        16\n",
      "         197       1.00      1.00      1.00        16\n",
      "        1973       0.80      0.75      0.77        16\n",
      "        1974       1.00      1.00      1.00        16\n",
      "         198       1.00      1.00      1.00        16\n",
      "        1995       1.00      1.00      1.00        16\n",
      "           2       0.94      0.94      0.94        16\n",
      "          20       0.94      1.00      0.97        16\n",
      "        2009       1.00      1.00      1.00        16\n",
      "        2015       1.00      1.00      1.00        16\n",
      "        2026       1.00      1.00      1.00        16\n",
      "         203       1.00      1.00      1.00        16\n",
      "        2039       1.00      1.00      1.00        16\n",
      "        2041       1.00      1.00      1.00        16\n",
      "        2045       0.94      1.00      0.97        16\n",
      "        2053       1.00      1.00      1.00        16\n",
      "        2058       1.00      1.00      1.00        16\n",
      "        2063       1.00      1.00      1.00        16\n",
      "        2065       1.00      1.00      1.00        16\n",
      "        2072       1.00      1.00      1.00        16\n",
      "        2076       0.87      0.81      0.84        16\n",
      "         210       1.00      1.00      1.00        16\n",
      "        2119       1.00      1.00      1.00        16\n",
      "         212       1.00      1.00      1.00        16\n",
      "        2125       1.00      1.00      1.00        16\n",
      "        2129       1.00      1.00      1.00        16\n",
      "         213       1.00      0.81      0.90        16\n",
      "        2131       1.00      1.00      1.00        16\n",
      "        2132       0.71      0.94      0.81        16\n",
      "         214       1.00      1.00      1.00        16\n",
      "         216       1.00      1.00      1.00        16\n",
      "        2164       1.00      1.00      1.00        16\n",
      "        2166       1.00      1.00      1.00        16\n",
      "        2197       1.00      1.00      1.00        16\n",
      "          22       1.00      1.00      1.00        16\n",
      "        2209       0.94      1.00      0.97        16\n",
      "         226       1.00      1.00      1.00        16\n",
      "        2269       1.00      0.94      0.97        16\n",
      "          23       1.00      0.81      0.90        16\n",
      "        2328       1.00      1.00      1.00        16\n",
      "        2330       1.00      1.00      1.00        16\n",
      "        2361       1.00      1.00      1.00        16\n",
      "        2370       0.84      1.00      0.91        16\n",
      "        2379       0.76      0.81      0.79        16\n",
      "         239       0.94      1.00      0.97        16\n",
      "        2390       1.00      1.00      1.00        16\n",
      "          24       1.00      0.94      0.97        16\n",
      "         241       1.00      1.00      1.00        16\n",
      "         243       1.00      1.00      1.00        16\n",
      "        2440       1.00      0.94      0.97        16\n",
      "        2501       1.00      1.00      1.00        16\n",
      "        2507       1.00      1.00      1.00        16\n",
      "        2519       1.00      1.00      1.00        16\n",
      "        2553       1.00      1.00      1.00        16\n",
      "          26       1.00      1.00      1.00        16\n",
      "        2606       1.00      1.00      1.00        16\n",
      "        2692       0.94      1.00      0.97        16\n",
      "          27       1.00      1.00      1.00        16\n",
      "        2711       1.00      1.00      1.00        16\n",
      "         279       1.00      1.00      1.00        16\n",
      "          28       0.94      1.00      0.97        16\n",
      "         286       1.00      1.00      1.00        16\n",
      "        2870       0.94      1.00      0.97        16\n",
      "          29       1.00      1.00      1.00        16\n",
      "         292       1.00      1.00      1.00        16\n",
      "         293       1.00      1.00      1.00        16\n",
      "         297       1.00      1.00      1.00        16\n",
      "         302       0.89      1.00      0.94        16\n",
      "         306       1.00      1.00      1.00        16\n",
      "         307       1.00      0.94      0.97        16\n",
      "         308       1.00      1.00      1.00        16\n",
      "         309       1.00      1.00      1.00        16\n",
      "        3095       1.00      0.94      0.97        16\n",
      "          31       1.00      0.88      0.93        16\n",
      "         311       1.00      1.00      1.00        16\n",
      "         313       1.00      0.94      0.97        16\n",
      "         314       1.00      1.00      1.00        16\n",
      "         316       1.00      1.00      1.00        16\n",
      "         319       1.00      1.00      1.00        16\n",
      "          32       1.00      1.00      1.00        16\n",
      "         321       1.00      0.94      0.97        16\n",
      "         323       1.00      0.94      0.97        16\n",
      "        3233       0.79      0.94      0.86        16\n",
      "         329       1.00      1.00      1.00        16\n",
      "          33       0.94      1.00      0.97        16\n",
      "          34       0.80      0.75      0.77        16\n",
      "         343       1.00      1.00      1.00        16\n",
      "         350       1.00      0.94      0.97        16\n",
      "         356       1.00      1.00      1.00        16\n",
      "         358       1.00      1.00      1.00        16\n",
      "         359       1.00      1.00      1.00        16\n",
      "          36       1.00      1.00      1.00        16\n",
      "         362       1.00      1.00      1.00        16\n",
      "         365       1.00      1.00      1.00        16\n",
      "         367       1.00      1.00      1.00        16\n",
      "         371       1.00      1.00      1.00        16\n",
      "         377       1.00      1.00      1.00        16\n",
      "          39       1.00      1.00      1.00        16\n",
      "           4       1.00      1.00      1.00        16\n",
      "          40       1.00      1.00      1.00        16\n",
      "         404       1.00      1.00      1.00        16\n",
      "         405       1.00      1.00      1.00        16\n",
      "         411       0.94      1.00      0.97        16\n",
      "         412       0.94      1.00      0.97        16\n",
      "         413       1.00      1.00      1.00        16\n",
      "         414       0.84      1.00      0.91        16\n",
      "          42       1.00      0.81      0.90        16\n",
      "         423       0.84      1.00      0.91        16\n",
      "         425       1.00      1.00      1.00        16\n",
      "          43       0.94      1.00      0.97        16\n",
      "         432       1.00      1.00      1.00        16\n",
      "         433       1.00      1.00      1.00        16\n",
      "         434       1.00      1.00      1.00        16\n",
      "         435       1.00      1.00      1.00        16\n",
      "         440       1.00      1.00      1.00        16\n",
      "         443       1.00      1.00      1.00        16\n",
      "         446       1.00      1.00      1.00        16\n",
      "         447       0.94      1.00      0.97        16\n",
      "         448       1.00      1.00      1.00        16\n",
      "          45       1.00      1.00      1.00        16\n",
      "         450       1.00      1.00      1.00        16\n",
      "         451       1.00      1.00      1.00        16\n",
      "         457       1.00      1.00      1.00        16\n",
      "          46       1.00      1.00      1.00        16\n",
      "         463       1.00      1.00      1.00        16\n",
      "         464       1.00      1.00      1.00        16\n",
      "         469       1.00      1.00      1.00        16\n",
      "         471       0.94      1.00      0.97        16\n",
      "         473       1.00      0.94      0.97        16\n",
      "         474       1.00      0.94      0.97        16\n",
      "          48       1.00      1.00      1.00        16\n",
      "          49       1.00      1.00      1.00        16\n",
      "           5       1.00      1.00      1.00        16\n",
      "          50       1.00      1.00      1.00        16\n",
      "         505       1.00      1.00      1.00        16\n",
      "          51       1.00      0.94      0.97        16\n",
      "         515       0.88      0.88      0.88        16\n",
      "         516       1.00      0.94      0.97        16\n",
      "          52       1.00      1.00      1.00        16\n",
      "         523       1.00      1.00      1.00        16\n",
      "         524       0.94      1.00      0.97        16\n",
      "         532       1.00      1.00      1.00        16\n",
      "         543       1.00      1.00      1.00        16\n",
      "         548       1.00      1.00      1.00        16\n",
      "         568       1.00      1.00      1.00        16\n",
      "         578       1.00      1.00      1.00        16\n",
      "         582       1.00      1.00      1.00        16\n",
      "        5834       1.00      1.00      1.00        16\n",
      "         588       1.00      1.00      1.00        16\n",
      "         592       1.00      1.00      1.00        16\n",
      "         603       1.00      1.00      1.00        16\n",
      "         617       1.00      1.00      1.00        16\n",
      "          63       1.00      1.00      1.00        16\n",
      "         638       1.00      1.00      1.00        16\n",
      "         639       1.00      1.00      1.00        16\n",
      "          64       1.00      1.00      1.00        16\n",
      "         649       1.00      1.00      1.00        16\n",
      "          65       1.00      0.94      0.97        16\n",
      "         654       1.00      1.00      1.00        16\n",
      "         679       1.00      1.00      1.00        16\n",
      "         682       1.00      1.00      1.00        16\n",
      "         684       1.00      1.00      1.00        16\n",
      "         699       1.00      1.00      1.00        16\n",
      "          71       1.00      1.00      1.00        16\n",
      "          72       1.00      1.00      1.00        16\n",
      "         754       0.93      0.88      0.90        16\n",
      "          80       1.00      1.00      1.00        16\n",
      "          81       1.00      1.00      1.00        16\n",
      "         814       0.82      0.88      0.85        16\n",
      "         816       1.00      1.00      1.00        16\n",
      "          82       1.00      0.88      0.93        16\n",
      "          83       1.00      1.00      1.00        16\n",
      "          85       0.89      0.50      0.64        16\n",
      "          86       1.00      1.00      1.00        16\n",
      "         869       1.00      1.00      1.00        16\n",
      "         873       0.94      1.00      0.97        16\n",
      "          88       0.94      1.00      0.97        16\n",
      "         885       1.00      1.00      1.00        16\n",
      "         909       1.00      0.94      0.97        16\n",
      "          92       1.00      1.00      1.00        16\n",
      "          93       1.00      0.81      0.90        16\n",
      "          94       1.00      1.00      1.00        16\n",
      "         949       1.00      1.00      1.00        16\n",
      "          95       1.00      1.00      1.00        16\n",
      "          96       1.00      1.00      1.00        16\n",
      "         964       1.00      1.00      1.00        16\n",
      "         970       1.00      1.00      1.00        16\n",
      "          99       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           0.98      4064\n",
      "   macro avg       0.98      0.98      0.98      4064\n",
      "weighted avg       0.98      0.98      0.98      4064\n",
      "\n",
      "Test Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      1.00      0.98        20\n",
      "          10       1.00      1.00      1.00        20\n",
      "         101       1.00      1.00      1.00        20\n",
      "          11       0.85      0.55      0.67        20\n",
      "         112       0.75      0.75      0.75        20\n",
      "         118       0.95      1.00      0.98        20\n",
      "        1208       1.00      1.00      1.00        20\n",
      "        1256       1.00      1.00      1.00        20\n",
      "         126       0.95      1.00      0.98        20\n",
      "         127       0.90      0.95      0.93        20\n",
      "         128       0.83      1.00      0.91        20\n",
      "         129       0.74      1.00      0.85        20\n",
      "        1291       1.00      1.00      1.00        20\n",
      "        1298       1.00      1.00      1.00        20\n",
      "          13       0.95      1.00      0.98        20\n",
      "        1316       0.95      1.00      0.98        20\n",
      "         132       0.87      1.00      0.93        20\n",
      "         135       1.00      1.00      1.00        20\n",
      "        1370       1.00      1.00      1.00        20\n",
      "         138       0.95      0.95      0.95        20\n",
      "        1384       1.00      1.00      1.00        20\n",
      "        1392       1.00      1.00      1.00        20\n",
      "          14       1.00      1.00      1.00        20\n",
      "         142       0.95      0.95      0.95        20\n",
      "         145       1.00      1.00      1.00        20\n",
      "        1498       1.00      1.00      1.00        20\n",
      "        1499       0.95      1.00      0.98        20\n",
      "          15       1.00      1.00      1.00        20\n",
      "         150       1.00      1.00      1.00        20\n",
      "        1515       1.00      1.00      1.00        20\n",
      "         152       0.94      0.75      0.83        20\n",
      "        1541       1.00      1.00      1.00        20\n",
      "        1542       1.00      1.00      1.00        20\n",
      "        1543       1.00      1.00      1.00        20\n",
      "        1547       1.00      1.00      1.00        20\n",
      "        1549       1.00      1.00      1.00        20\n",
      "         155       1.00      1.00      1.00        20\n",
      "        1567       1.00      1.00      1.00        20\n",
      "        1575       1.00      1.00      1.00        20\n",
      "        1583       1.00      1.00      1.00        20\n",
      "        1593       1.00      1.00      1.00        20\n",
      "          16       1.00      1.00      1.00        20\n",
      "        1602       0.95      0.90      0.92        20\n",
      "        1606       0.95      0.95      0.95        20\n",
      "        1610       1.00      1.00      1.00        20\n",
      "        1612       1.00      1.00      1.00        20\n",
      "        1628       1.00      1.00      1.00        20\n",
      "         164       1.00      1.00      1.00        20\n",
      "        1659       0.95      1.00      0.98        20\n",
      "         166       1.00      1.00      1.00        20\n",
      "        1674       1.00      1.00      1.00        20\n",
      "        1675       0.81      0.85      0.83        20\n",
      "          17       1.00      1.00      1.00        20\n",
      "         174       1.00      1.00      1.00        20\n",
      "          18       1.00      1.00      1.00        20\n",
      "         180       1.00      0.90      0.95        20\n",
      "        1815       1.00      1.00      1.00        20\n",
      "         183       1.00      1.00      1.00        20\n",
      "        1838       1.00      1.00      1.00        20\n",
      "        1848       1.00      1.00      1.00        20\n",
      "         185       1.00      1.00      1.00        20\n",
      "        1891       1.00      1.00      1.00        20\n",
      "          19       0.87      0.65      0.74        20\n",
      "        1925       1.00      0.95      0.97        20\n",
      "         195       1.00      1.00      1.00        20\n",
      "        1954       1.00      1.00      1.00        20\n",
      "        1959       1.00      1.00      1.00        20\n",
      "         197       1.00      1.00      1.00        20\n",
      "        1973       0.83      0.75      0.79        20\n",
      "        1974       1.00      1.00      1.00        20\n",
      "         198       1.00      1.00      1.00        20\n",
      "        1995       1.00      0.95      0.97        20\n",
      "           2       1.00      0.95      0.97        20\n",
      "          20       1.00      0.95      0.97        20\n",
      "        2009       1.00      1.00      1.00        20\n",
      "        2015       1.00      1.00      1.00        20\n",
      "        2026       1.00      0.95      0.97        20\n",
      "         203       1.00      1.00      1.00        20\n",
      "        2039       1.00      0.95      0.97        20\n",
      "        2041       1.00      1.00      1.00        20\n",
      "        2045       1.00      0.95      0.97        20\n",
      "        2053       1.00      1.00      1.00        20\n",
      "        2058       1.00      1.00      1.00        20\n",
      "        2063       1.00      1.00      1.00        20\n",
      "        2065       1.00      1.00      1.00        20\n",
      "        2072       1.00      1.00      1.00        20\n",
      "        2076       0.95      1.00      0.98        20\n",
      "         210       1.00      0.95      0.97        20\n",
      "        2119       1.00      1.00      1.00        20\n",
      "         212       1.00      1.00      1.00        20\n",
      "        2125       1.00      1.00      1.00        20\n",
      "        2129       1.00      1.00      1.00        20\n",
      "         213       0.90      0.90      0.90        20\n",
      "        2131       1.00      1.00      1.00        20\n",
      "        2132       0.79      0.95      0.86        20\n",
      "         214       1.00      1.00      1.00        20\n",
      "         216       1.00      1.00      1.00        20\n",
      "        2164       1.00      1.00      1.00        20\n",
      "        2166       1.00      1.00      1.00        20\n",
      "        2197       1.00      1.00      1.00        20\n",
      "          22       0.95      1.00      0.98        20\n",
      "        2209       1.00      1.00      1.00        20\n",
      "         226       1.00      1.00      1.00        20\n",
      "        2269       0.95      0.95      0.95        20\n",
      "          23       0.84      0.80      0.82        20\n",
      "        2328       1.00      1.00      1.00        20\n",
      "        2330       1.00      1.00      1.00        20\n",
      "        2361       1.00      1.00      1.00        20\n",
      "        2370       1.00      0.95      0.97        20\n",
      "        2379       0.81      0.85      0.83        20\n",
      "         239       0.87      1.00      0.93        20\n",
      "        2390       1.00      1.00      1.00        20\n",
      "          24       1.00      1.00      1.00        20\n",
      "         241       1.00      1.00      1.00        20\n",
      "         243       1.00      1.00      1.00        20\n",
      "        2440       1.00      0.85      0.92        20\n",
      "        2501       1.00      1.00      1.00        20\n",
      "        2507       1.00      1.00      1.00        20\n",
      "        2519       1.00      1.00      1.00        20\n",
      "        2553       1.00      1.00      1.00        20\n",
      "          26       1.00      1.00      1.00        20\n",
      "        2606       1.00      1.00      1.00        20\n",
      "        2692       0.95      1.00      0.98        20\n",
      "          27       1.00      1.00      1.00        20\n",
      "        2711       1.00      1.00      1.00        20\n",
      "         279       1.00      1.00      1.00        20\n",
      "          28       1.00      1.00      1.00        20\n",
      "         286       1.00      1.00      1.00        20\n",
      "        2870       0.95      1.00      0.98        20\n",
      "          29       0.95      1.00      0.98        20\n",
      "         292       1.00      1.00      1.00        20\n",
      "         293       1.00      1.00      1.00        20\n",
      "         297       1.00      1.00      1.00        20\n",
      "         302       0.83      1.00      0.91        20\n",
      "         306       1.00      1.00      1.00        20\n",
      "         307       1.00      0.95      0.97        20\n",
      "         308       1.00      1.00      1.00        20\n",
      "         309       1.00      1.00      1.00        20\n",
      "        3095       0.94      0.85      0.89        20\n",
      "          31       1.00      0.85      0.92        20\n",
      "         311       1.00      0.95      0.97        20\n",
      "         313       1.00      0.85      0.92        20\n",
      "         314       1.00      1.00      1.00        20\n",
      "         316       1.00      1.00      1.00        20\n",
      "         319       1.00      1.00      1.00        20\n",
      "          32       1.00      1.00      1.00        20\n",
      "         321       1.00      0.95      0.97        20\n",
      "         323       1.00      1.00      1.00        20\n",
      "        3233       0.77      1.00      0.87        20\n",
      "         329       1.00      1.00      1.00        20\n",
      "          33       1.00      1.00      1.00        20\n",
      "          34       0.84      0.80      0.82        20\n",
      "         343       1.00      0.95      0.97        20\n",
      "         350       1.00      1.00      1.00        20\n",
      "         356       1.00      1.00      1.00        20\n",
      "         358       1.00      1.00      1.00        20\n",
      "         359       1.00      1.00      1.00        20\n",
      "          36       1.00      1.00      1.00        20\n",
      "         362       1.00      1.00      1.00        20\n",
      "         365       1.00      1.00      1.00        20\n",
      "         367       1.00      1.00      1.00        20\n",
      "         371       0.95      1.00      0.98        20\n",
      "         377       1.00      0.95      0.97        20\n",
      "          39       1.00      1.00      1.00        20\n",
      "           4       1.00      1.00      1.00        20\n",
      "          40       0.95      1.00      0.98        20\n",
      "         404       1.00      1.00      1.00        20\n",
      "         405       1.00      1.00      1.00        20\n",
      "         411       1.00      1.00      1.00        20\n",
      "         412       1.00      1.00      1.00        20\n",
      "         413       0.90      0.95      0.93        20\n",
      "         414       1.00      1.00      1.00        20\n",
      "          42       0.95      0.95      0.95        20\n",
      "         423       0.90      0.95      0.93        20\n",
      "         425       1.00      1.00      1.00        20\n",
      "          43       0.95      1.00      0.98        20\n",
      "         432       1.00      1.00      1.00        20\n",
      "         433       1.00      1.00      1.00        20\n",
      "         434       1.00      1.00      1.00        20\n",
      "         435       1.00      1.00      1.00        20\n",
      "         440       1.00      1.00      1.00        20\n",
      "         443       1.00      1.00      1.00        20\n",
      "         446       1.00      1.00      1.00        20\n",
      "         447       0.86      0.95      0.90        20\n",
      "         448       1.00      1.00      1.00        20\n",
      "          45       1.00      1.00      1.00        20\n",
      "         450       1.00      1.00      1.00        20\n",
      "         451       1.00      1.00      1.00        20\n",
      "         457       1.00      1.00      1.00        20\n",
      "          46       1.00      1.00      1.00        20\n",
      "         463       1.00      1.00      1.00        20\n",
      "         464       1.00      1.00      1.00        20\n",
      "         469       1.00      1.00      1.00        20\n",
      "         471       1.00      1.00      1.00        20\n",
      "         473       1.00      1.00      1.00        20\n",
      "         474       1.00      1.00      1.00        20\n",
      "          48       1.00      1.00      1.00        20\n",
      "          49       1.00      1.00      1.00        20\n",
      "           5       1.00      1.00      1.00        20\n",
      "          50       1.00      1.00      1.00        20\n",
      "         505       1.00      1.00      1.00        20\n",
      "          51       0.95      0.95      0.95        20\n",
      "         515       0.90      0.95      0.93        20\n",
      "         516       0.95      0.95      0.95        20\n",
      "          52       1.00      1.00      1.00        20\n",
      "         523       1.00      1.00      1.00        20\n",
      "         524       1.00      0.95      0.97        20\n",
      "         532       1.00      1.00      1.00        20\n",
      "         543       1.00      1.00      1.00        20\n",
      "         548       1.00      1.00      1.00        20\n",
      "         568       1.00      0.95      0.97        20\n",
      "         578       1.00      1.00      1.00        20\n",
      "         582       1.00      1.00      1.00        20\n",
      "        5834       1.00      1.00      1.00        20\n",
      "         588       1.00      1.00      1.00        20\n",
      "         592       1.00      1.00      1.00        20\n",
      "         603       1.00      1.00      1.00        20\n",
      "         617       1.00      1.00      1.00        20\n",
      "          63       1.00      1.00      1.00        20\n",
      "         638       1.00      1.00      1.00        20\n",
      "         639       1.00      1.00      1.00        20\n",
      "          64       1.00      1.00      1.00        20\n",
      "         649       1.00      1.00      1.00        20\n",
      "          65       1.00      0.90      0.95        20\n",
      "         654       1.00      1.00      1.00        20\n",
      "         679       1.00      1.00      1.00        20\n",
      "         682       1.00      1.00      1.00        20\n",
      "         684       1.00      1.00      1.00        20\n",
      "         699       1.00      1.00      1.00        20\n",
      "          71       1.00      1.00      1.00        20\n",
      "          72       1.00      1.00      1.00        20\n",
      "         754       0.95      0.90      0.92        20\n",
      "          80       1.00      1.00      1.00        20\n",
      "          81       1.00      1.00      1.00        20\n",
      "         814       0.72      0.90      0.80        20\n",
      "         816       1.00      1.00      1.00        20\n",
      "          82       0.84      0.80      0.82        20\n",
      "          83       1.00      1.00      1.00        20\n",
      "          85       1.00      0.65      0.79        20\n",
      "          86       1.00      1.00      1.00        20\n",
      "         869       1.00      1.00      1.00        20\n",
      "         873       0.91      1.00      0.95        20\n",
      "          88       0.95      0.90      0.92        20\n",
      "         885       1.00      0.95      0.97        20\n",
      "         909       1.00      1.00      1.00        20\n",
      "          92       0.95      1.00      0.98        20\n",
      "          93       0.95      1.00      0.98        20\n",
      "          94       1.00      1.00      1.00        20\n",
      "         949       1.00      1.00      1.00        20\n",
      "          95       1.00      1.00      1.00        20\n",
      "          96       1.00      1.00      1.00        20\n",
      "         964       1.00      1.00      1.00        20\n",
      "         970       1.00      1.00      1.00        20\n",
      "          99       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           0.98      5080\n",
      "   macro avg       0.98      0.98      0.98      5080\n",
      "weighted avg       0.98      0.98      0.98      5080\n",
      "\n",
      "Validation Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        16\n",
      "          10       1.00      1.00      1.00        16\n",
      "         101       1.00      1.00      1.00        16\n",
      "          11       0.93      0.81      0.87        16\n",
      "         112       1.00      0.69      0.81        16\n",
      "         118       0.94      0.94      0.94        16\n",
      "        1208       1.00      1.00      1.00        16\n",
      "        1256       0.89      1.00      0.94        16\n",
      "         126       1.00      1.00      1.00        16\n",
      "         127       1.00      1.00      1.00        16\n",
      "         128       0.94      1.00      0.97        16\n",
      "         129       0.79      0.94      0.86        16\n",
      "        1291       1.00      1.00      1.00        16\n",
      "        1298       1.00      1.00      1.00        16\n",
      "          13       1.00      1.00      1.00        16\n",
      "        1316       1.00      1.00      1.00        16\n",
      "         132       0.94      1.00      0.97        16\n",
      "         135       1.00      1.00      1.00        16\n",
      "        1370       1.00      1.00      1.00        16\n",
      "         138       0.94      1.00      0.97        16\n",
      "        1384       1.00      1.00      1.00        16\n",
      "        1392       1.00      1.00      1.00        16\n",
      "          14       1.00      1.00      1.00        16\n",
      "         142       1.00      1.00      1.00        16\n",
      "         145       1.00      1.00      1.00        16\n",
      "        1498       1.00      1.00      1.00        16\n",
      "        1499       1.00      1.00      1.00        16\n",
      "          15       1.00      1.00      1.00        16\n",
      "         150       1.00      1.00      1.00        16\n",
      "        1515       1.00      1.00      1.00        16\n",
      "         152       0.93      0.88      0.90        16\n",
      "        1541       1.00      1.00      1.00        16\n",
      "        1542       1.00      1.00      1.00        16\n",
      "        1543       1.00      1.00      1.00        16\n",
      "        1547       1.00      1.00      1.00        16\n",
      "        1549       1.00      1.00      1.00        16\n",
      "         155       1.00      1.00      1.00        16\n",
      "        1567       1.00      1.00      1.00        16\n",
      "        1575       1.00      1.00      1.00        16\n",
      "        1583       1.00      1.00      1.00        16\n",
      "        1593       1.00      1.00      1.00        16\n",
      "          16       1.00      1.00      1.00        16\n",
      "        1602       1.00      1.00      1.00        16\n",
      "        1606       0.80      1.00      0.89        16\n",
      "        1610       1.00      1.00      1.00        16\n",
      "        1612       1.00      1.00      1.00        16\n",
      "        1628       1.00      1.00      1.00        16\n",
      "         164       0.93      0.88      0.90        16\n",
      "        1659       1.00      1.00      1.00        16\n",
      "         166       1.00      1.00      1.00        16\n",
      "        1674       1.00      1.00      1.00        16\n",
      "        1675       0.84      1.00      0.91        16\n",
      "          17       1.00      1.00      1.00        16\n",
      "         174       1.00      0.81      0.90        16\n",
      "          18       1.00      1.00      1.00        16\n",
      "         180       1.00      1.00      1.00        16\n",
      "        1815       1.00      1.00      1.00        16\n",
      "         183       1.00      1.00      1.00        16\n",
      "        1838       1.00      1.00      1.00        16\n",
      "        1848       1.00      1.00      1.00        16\n",
      "         185       1.00      1.00      1.00        16\n",
      "        1891       1.00      1.00      1.00        16\n",
      "          19       1.00      0.81      0.90        16\n",
      "        1925       1.00      1.00      1.00        16\n",
      "         195       1.00      1.00      1.00        16\n",
      "        1954       1.00      1.00      1.00        16\n",
      "        1959       1.00      1.00      1.00        16\n",
      "         197       1.00      1.00      1.00        16\n",
      "        1973       1.00      0.94      0.97        16\n",
      "        1974       1.00      1.00      1.00        16\n",
      "         198       1.00      1.00      1.00        16\n",
      "        1995       1.00      1.00      1.00        16\n",
      "           2       1.00      0.94      0.97        16\n",
      "          20       0.94      1.00      0.97        16\n",
      "        2009       1.00      1.00      1.00        16\n",
      "        2015       1.00      1.00      1.00        16\n",
      "        2026       1.00      1.00      1.00        16\n",
      "         203       1.00      1.00      1.00        16\n",
      "        2039       1.00      1.00      1.00        16\n",
      "        2041       1.00      1.00      1.00        16\n",
      "        2045       1.00      1.00      1.00        16\n",
      "        2053       1.00      1.00      1.00        16\n",
      "        2058       1.00      1.00      1.00        16\n",
      "        2063       1.00      1.00      1.00        16\n",
      "        2065       1.00      1.00      1.00        16\n",
      "        2072       1.00      1.00      1.00        16\n",
      "        2076       0.94      0.94      0.94        16\n",
      "         210       1.00      1.00      1.00        16\n",
      "        2119       1.00      1.00      1.00        16\n",
      "         212       1.00      1.00      1.00        16\n",
      "        2125       1.00      1.00      1.00        16\n",
      "        2129       0.89      1.00      0.94        16\n",
      "         213       0.89      1.00      0.94        16\n",
      "        2131       1.00      1.00      1.00        16\n",
      "        2132       0.94      0.94      0.94        16\n",
      "         214       1.00      1.00      1.00        16\n",
      "         216       1.00      1.00      1.00        16\n",
      "        2164       1.00      1.00      1.00        16\n",
      "        2166       1.00      1.00      1.00        16\n",
      "        2197       1.00      1.00      1.00        16\n",
      "          22       1.00      1.00      1.00        16\n",
      "        2209       1.00      1.00      1.00        16\n",
      "         226       1.00      1.00      1.00        16\n",
      "        2269       1.00      0.94      0.97        16\n",
      "          23       0.87      0.81      0.84        16\n",
      "        2328       1.00      1.00      1.00        16\n",
      "        2330       1.00      1.00      1.00        16\n",
      "        2361       1.00      1.00      1.00        16\n",
      "        2370       0.89      1.00      0.94        16\n",
      "        2379       0.82      0.88      0.85        16\n",
      "         239       1.00      1.00      1.00        16\n",
      "        2390       0.94      1.00      0.97        16\n",
      "          24       1.00      0.94      0.97        16\n",
      "         241       1.00      1.00      1.00        16\n",
      "         243       0.94      1.00      0.97        16\n",
      "        2440       1.00      1.00      1.00        16\n",
      "        2501       1.00      1.00      1.00        16\n",
      "        2507       1.00      1.00      1.00        16\n",
      "        2519       1.00      1.00      1.00        16\n",
      "        2553       1.00      1.00      1.00        16\n",
      "          26       1.00      1.00      1.00        16\n",
      "        2606       1.00      1.00      1.00        16\n",
      "        2692       1.00      1.00      1.00        16\n",
      "          27       1.00      1.00      1.00        16\n",
      "        2711       1.00      1.00      1.00        16\n",
      "         279       1.00      1.00      1.00        16\n",
      "          28       1.00      0.94      0.97        16\n",
      "         286       1.00      1.00      1.00        16\n",
      "        2870       0.94      1.00      0.97        16\n",
      "          29       1.00      1.00      1.00        16\n",
      "         292       1.00      1.00      1.00        16\n",
      "         293       1.00      1.00      1.00        16\n",
      "         297       1.00      1.00      1.00        16\n",
      "         302       0.94      1.00      0.97        16\n",
      "         306       1.00      1.00      1.00        16\n",
      "         307       1.00      1.00      1.00        16\n",
      "         308       1.00      1.00      1.00        16\n",
      "         309       1.00      1.00      1.00        16\n",
      "        3095       1.00      0.94      0.97        16\n",
      "          31       1.00      0.88      0.93        16\n",
      "         311       1.00      1.00      1.00        16\n",
      "         313       1.00      0.94      0.97        16\n",
      "         314       1.00      1.00      1.00        16\n",
      "         316       1.00      1.00      1.00        16\n",
      "         319       1.00      1.00      1.00        16\n",
      "          32       1.00      1.00      1.00        16\n",
      "         321       1.00      1.00      1.00        16\n",
      "         323       1.00      0.94      0.97        16\n",
      "        3233       0.80      1.00      0.89        16\n",
      "         329       1.00      1.00      1.00        16\n",
      "          33       1.00      1.00      1.00        16\n",
      "          34       0.87      0.81      0.84        16\n",
      "         343       1.00      1.00      1.00        16\n",
      "         350       1.00      0.94      0.97        16\n",
      "         356       1.00      1.00      1.00        16\n",
      "         358       1.00      1.00      1.00        16\n",
      "         359       1.00      1.00      1.00        16\n",
      "          36       1.00      1.00      1.00        16\n",
      "         362       1.00      1.00      1.00        16\n",
      "         365       1.00      1.00      1.00        16\n",
      "         367       1.00      1.00      1.00        16\n",
      "         371       1.00      1.00      1.00        16\n",
      "         377       1.00      1.00      1.00        16\n",
      "          39       1.00      1.00      1.00        16\n",
      "           4       1.00      1.00      1.00        16\n",
      "          40       1.00      1.00      1.00        16\n",
      "         404       1.00      1.00      1.00        16\n",
      "         405       1.00      1.00      1.00        16\n",
      "         411       1.00      1.00      1.00        16\n",
      "         412       1.00      1.00      1.00        16\n",
      "         413       1.00      1.00      1.00        16\n",
      "         414       0.84      1.00      0.91        16\n",
      "          42       1.00      0.88      0.93        16\n",
      "         423       1.00      1.00      1.00        16\n",
      "         425       1.00      1.00      1.00        16\n",
      "          43       1.00      1.00      1.00        16\n",
      "         432       1.00      1.00      1.00        16\n",
      "         433       1.00      1.00      1.00        16\n",
      "         434       1.00      1.00      1.00        16\n",
      "         435       1.00      1.00      1.00        16\n",
      "         440       1.00      1.00      1.00        16\n",
      "         443       1.00      1.00      1.00        16\n",
      "         446       1.00      1.00      1.00        16\n",
      "         447       0.94      1.00      0.97        16\n",
      "         448       1.00      1.00      1.00        16\n",
      "          45       0.94      1.00      0.97        16\n",
      "         450       1.00      1.00      1.00        16\n",
      "         451       1.00      1.00      1.00        16\n",
      "         457       1.00      1.00      1.00        16\n",
      "          46       1.00      1.00      1.00        16\n",
      "         463       1.00      1.00      1.00        16\n",
      "         464       1.00      1.00      1.00        16\n",
      "         469       1.00      1.00      1.00        16\n",
      "         471       0.94      1.00      0.97        16\n",
      "         473       1.00      1.00      1.00        16\n",
      "         474       1.00      1.00      1.00        16\n",
      "          48       1.00      1.00      1.00        16\n",
      "          49       1.00      1.00      1.00        16\n",
      "           5       1.00      1.00      1.00        16\n",
      "          50       1.00      1.00      1.00        16\n",
      "         505       1.00      1.00      1.00        16\n",
      "          51       1.00      1.00      1.00        16\n",
      "         515       0.93      0.88      0.90        16\n",
      "         516       1.00      1.00      1.00        16\n",
      "          52       1.00      1.00      1.00        16\n",
      "         523       1.00      1.00      1.00        16\n",
      "         524       1.00      1.00      1.00        16\n",
      "         532       1.00      1.00      1.00        16\n",
      "         543       1.00      1.00      1.00        16\n",
      "         548       1.00      1.00      1.00        16\n",
      "         568       1.00      1.00      1.00        16\n",
      "         578       1.00      1.00      1.00        16\n",
      "         582       1.00      1.00      1.00        16\n",
      "        5834       1.00      1.00      1.00        16\n",
      "         588       1.00      1.00      1.00        16\n",
      "         592       1.00      1.00      1.00        16\n",
      "         603       1.00      1.00      1.00        16\n",
      "         617       1.00      1.00      1.00        16\n",
      "          63       1.00      1.00      1.00        16\n",
      "         638       1.00      1.00      1.00        16\n",
      "         639       1.00      1.00      1.00        16\n",
      "          64       1.00      1.00      1.00        16\n",
      "         649       1.00      1.00      1.00        16\n",
      "          65       1.00      0.88      0.93        16\n",
      "         654       1.00      1.00      1.00        16\n",
      "         679       0.94      1.00      0.97        16\n",
      "         682       1.00      1.00      1.00        16\n",
      "         684       1.00      1.00      1.00        16\n",
      "         699       1.00      1.00      1.00        16\n",
      "          71       1.00      1.00      1.00        16\n",
      "          72       1.00      1.00      1.00        16\n",
      "         754       0.94      0.94      0.94        16\n",
      "          80       1.00      1.00      1.00        16\n",
      "          81       1.00      1.00      1.00        16\n",
      "         814       1.00      0.94      0.97        16\n",
      "         816       1.00      1.00      1.00        16\n",
      "          82       1.00      1.00      1.00        16\n",
      "          83       1.00      1.00      1.00        16\n",
      "          85       0.92      0.75      0.83        16\n",
      "          86       1.00      1.00      1.00        16\n",
      "         869       1.00      1.00      1.00        16\n",
      "         873       0.88      0.94      0.91        16\n",
      "          88       1.00      1.00      1.00        16\n",
      "         885       1.00      1.00      1.00        16\n",
      "         909       1.00      1.00      1.00        16\n",
      "          92       1.00      1.00      1.00        16\n",
      "          93       1.00      0.81      0.90        16\n",
      "          94       1.00      1.00      1.00        16\n",
      "         949       1.00      1.00      1.00        16\n",
      "          95       1.00      1.00      1.00        16\n",
      "          96       1.00      1.00      1.00        16\n",
      "         964       1.00      1.00      1.00        16\n",
      "         970       1.00      1.00      1.00        16\n",
      "          99       0.94      1.00      0.97        16\n",
      "\n",
      "    accuracy                           0.99      4064\n",
      "   macro avg       0.99      0.99      0.99      4064\n",
      "weighted avg       0.99      0.99      0.99      4064\n",
      "\n",
      "Test Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        20\n",
      "          10       1.00      1.00      1.00        20\n",
      "         101       1.00      1.00      1.00        20\n",
      "          11       0.93      0.65      0.76        20\n",
      "         112       0.90      0.90      0.90        20\n",
      "         118       0.95      0.95      0.95        20\n",
      "        1208       1.00      1.00      1.00        20\n",
      "        1256       1.00      1.00      1.00        20\n",
      "         126       1.00      0.95      0.97        20\n",
      "         127       1.00      0.95      0.97        20\n",
      "         128       0.91      1.00      0.95        20\n",
      "         129       0.95      1.00      0.98        20\n",
      "        1291       1.00      1.00      1.00        20\n",
      "        1298       1.00      1.00      1.00        20\n",
      "          13       1.00      1.00      1.00        20\n",
      "        1316       1.00      1.00      1.00        20\n",
      "         132       0.95      1.00      0.98        20\n",
      "         135       1.00      1.00      1.00        20\n",
      "        1370       1.00      1.00      1.00        20\n",
      "         138       0.95      0.95      0.95        20\n",
      "        1384       1.00      1.00      1.00        20\n",
      "        1392       1.00      1.00      1.00        20\n",
      "          14       1.00      1.00      1.00        20\n",
      "         142       1.00      1.00      1.00        20\n",
      "         145       1.00      1.00      1.00        20\n",
      "        1498       1.00      1.00      1.00        20\n",
      "        1499       1.00      1.00      1.00        20\n",
      "          15       1.00      1.00      1.00        20\n",
      "         150       1.00      1.00      1.00        20\n",
      "        1515       1.00      1.00      1.00        20\n",
      "         152       0.95      0.90      0.92        20\n",
      "        1541       1.00      1.00      1.00        20\n",
      "        1542       1.00      1.00      1.00        20\n",
      "        1543       1.00      1.00      1.00        20\n",
      "        1547       1.00      1.00      1.00        20\n",
      "        1549       1.00      1.00      1.00        20\n",
      "         155       1.00      1.00      1.00        20\n",
      "        1567       1.00      1.00      1.00        20\n",
      "        1575       1.00      1.00      1.00        20\n",
      "        1583       1.00      1.00      1.00        20\n",
      "        1593       1.00      1.00      1.00        20\n",
      "          16       1.00      1.00      1.00        20\n",
      "        1602       1.00      1.00      1.00        20\n",
      "        1606       0.95      1.00      0.98        20\n",
      "        1610       1.00      1.00      1.00        20\n",
      "        1612       1.00      1.00      1.00        20\n",
      "        1628       1.00      1.00      1.00        20\n",
      "         164       0.95      1.00      0.98        20\n",
      "        1659       1.00      1.00      1.00        20\n",
      "         166       1.00      1.00      1.00        20\n",
      "        1674       1.00      1.00      1.00        20\n",
      "        1675       0.90      0.90      0.90        20\n",
      "          17       1.00      1.00      1.00        20\n",
      "         174       1.00      1.00      1.00        20\n",
      "          18       1.00      1.00      1.00        20\n",
      "         180       1.00      0.95      0.97        20\n",
      "        1815       1.00      0.95      0.97        20\n",
      "         183       1.00      1.00      1.00        20\n",
      "        1838       1.00      1.00      1.00        20\n",
      "        1848       1.00      1.00      1.00        20\n",
      "         185       1.00      1.00      1.00        20\n",
      "        1891       1.00      1.00      1.00        20\n",
      "          19       1.00      0.70      0.82        20\n",
      "        1925       1.00      1.00      1.00        20\n",
      "         195       1.00      1.00      1.00        20\n",
      "        1954       1.00      1.00      1.00        20\n",
      "        1959       1.00      1.00      1.00        20\n",
      "         197       1.00      1.00      1.00        20\n",
      "        1973       1.00      1.00      1.00        20\n",
      "        1974       1.00      1.00      1.00        20\n",
      "         198       1.00      1.00      1.00        20\n",
      "        1995       1.00      1.00      1.00        20\n",
      "           2       0.95      1.00      0.98        20\n",
      "          20       0.95      1.00      0.98        20\n",
      "        2009       1.00      1.00      1.00        20\n",
      "        2015       1.00      1.00      1.00        20\n",
      "        2026       1.00      1.00      1.00        20\n",
      "         203       1.00      1.00      1.00        20\n",
      "        2039       0.95      0.95      0.95        20\n",
      "        2041       1.00      1.00      1.00        20\n",
      "        2045       1.00      1.00      1.00        20\n",
      "        2053       1.00      1.00      1.00        20\n",
      "        2058       1.00      1.00      1.00        20\n",
      "        2063       1.00      1.00      1.00        20\n",
      "        2065       1.00      1.00      1.00        20\n",
      "        2072       1.00      1.00      1.00        20\n",
      "        2076       1.00      1.00      1.00        20\n",
      "         210       1.00      1.00      1.00        20\n",
      "        2119       1.00      1.00      1.00        20\n",
      "         212       1.00      1.00      1.00        20\n",
      "        2125       1.00      1.00      1.00        20\n",
      "        2129       1.00      1.00      1.00        20\n",
      "         213       0.91      1.00      0.95        20\n",
      "        2131       1.00      1.00      1.00        20\n",
      "        2132       0.90      0.95      0.93        20\n",
      "         214       1.00      1.00      1.00        20\n",
      "         216       1.00      1.00      1.00        20\n",
      "        2164       1.00      1.00      1.00        20\n",
      "        2166       1.00      1.00      1.00        20\n",
      "        2197       1.00      1.00      1.00        20\n",
      "          22       1.00      1.00      1.00        20\n",
      "        2209       1.00      0.95      0.97        20\n",
      "         226       1.00      1.00      1.00        20\n",
      "        2269       0.95      0.95      0.95        20\n",
      "          23       0.90      0.90      0.90        20\n",
      "        2328       1.00      1.00      1.00        20\n",
      "        2330       1.00      1.00      1.00        20\n",
      "        2361       1.00      1.00      1.00        20\n",
      "        2370       1.00      0.95      0.97        20\n",
      "        2379       0.81      0.85      0.83        20\n",
      "         239       1.00      1.00      1.00        20\n",
      "        2390       1.00      1.00      1.00        20\n",
      "          24       1.00      1.00      1.00        20\n",
      "         241       1.00      1.00      1.00        20\n",
      "         243       1.00      1.00      1.00        20\n",
      "        2440       1.00      1.00      1.00        20\n",
      "        2501       1.00      1.00      1.00        20\n",
      "        2507       1.00      1.00      1.00        20\n",
      "        2519       1.00      1.00      1.00        20\n",
      "        2553       1.00      1.00      1.00        20\n",
      "          26       1.00      1.00      1.00        20\n",
      "        2606       1.00      1.00      1.00        20\n",
      "        2692       1.00      1.00      1.00        20\n",
      "          27       1.00      1.00      1.00        20\n",
      "        2711       1.00      1.00      1.00        20\n",
      "         279       1.00      1.00      1.00        20\n",
      "          28       1.00      1.00      1.00        20\n",
      "         286       1.00      1.00      1.00        20\n",
      "        2870       0.95      1.00      0.98        20\n",
      "          29       1.00      1.00      1.00        20\n",
      "         292       1.00      1.00      1.00        20\n",
      "         293       1.00      1.00      1.00        20\n",
      "         297       1.00      1.00      1.00        20\n",
      "         302       0.91      1.00      0.95        20\n",
      "         306       1.00      1.00      1.00        20\n",
      "         307       1.00      0.95      0.97        20\n",
      "         308       1.00      1.00      1.00        20\n",
      "         309       1.00      1.00      1.00        20\n",
      "        3095       1.00      0.90      0.95        20\n",
      "          31       1.00      0.95      0.97        20\n",
      "         311       1.00      1.00      1.00        20\n",
      "         313       1.00      0.90      0.95        20\n",
      "         314       1.00      1.00      1.00        20\n",
      "         316       1.00      1.00      1.00        20\n",
      "         319       1.00      1.00      1.00        20\n",
      "          32       1.00      1.00      1.00        20\n",
      "         321       1.00      1.00      1.00        20\n",
      "         323       0.95      1.00      0.98        20\n",
      "        3233       0.80      1.00      0.89        20\n",
      "         329       1.00      1.00      1.00        20\n",
      "          33       1.00      1.00      1.00        20\n",
      "          34       0.84      0.80      0.82        20\n",
      "         343       1.00      1.00      1.00        20\n",
      "         350       1.00      1.00      1.00        20\n",
      "         356       1.00      1.00      1.00        20\n",
      "         358       1.00      1.00      1.00        20\n",
      "         359       1.00      1.00      1.00        20\n",
      "          36       1.00      1.00      1.00        20\n",
      "         362       1.00      1.00      1.00        20\n",
      "         365       1.00      1.00      1.00        20\n",
      "         367       1.00      1.00      1.00        20\n",
      "         371       1.00      1.00      1.00        20\n",
      "         377       1.00      1.00      1.00        20\n",
      "          39       1.00      1.00      1.00        20\n",
      "           4       1.00      1.00      1.00        20\n",
      "          40       1.00      1.00      1.00        20\n",
      "         404       1.00      1.00      1.00        20\n",
      "         405       1.00      1.00      1.00        20\n",
      "         411       1.00      1.00      1.00        20\n",
      "         412       1.00      1.00      1.00        20\n",
      "         413       1.00      1.00      1.00        20\n",
      "         414       1.00      1.00      1.00        20\n",
      "          42       1.00      1.00      1.00        20\n",
      "         423       1.00      1.00      1.00        20\n",
      "         425       1.00      1.00      1.00        20\n",
      "          43       0.95      1.00      0.98        20\n",
      "         432       1.00      1.00      1.00        20\n",
      "         433       1.00      1.00      1.00        20\n",
      "         434       1.00      1.00      1.00        20\n",
      "         435       1.00      1.00      1.00        20\n",
      "         440       1.00      1.00      1.00        20\n",
      "         443       1.00      1.00      1.00        20\n",
      "         446       1.00      1.00      1.00        20\n",
      "         447       0.91      1.00      0.95        20\n",
      "         448       1.00      1.00      1.00        20\n",
      "          45       1.00      1.00      1.00        20\n",
      "         450       1.00      1.00      1.00        20\n",
      "         451       1.00      1.00      1.00        20\n",
      "         457       1.00      1.00      1.00        20\n",
      "          46       1.00      1.00      1.00        20\n",
      "         463       1.00      1.00      1.00        20\n",
      "         464       0.95      1.00      0.98        20\n",
      "         469       1.00      1.00      1.00        20\n",
      "         471       1.00      1.00      1.00        20\n",
      "         473       1.00      1.00      1.00        20\n",
      "         474       1.00      1.00      1.00        20\n",
      "          48       1.00      1.00      1.00        20\n",
      "          49       1.00      1.00      1.00        20\n",
      "           5       1.00      1.00      1.00        20\n",
      "          50       1.00      1.00      1.00        20\n",
      "         505       1.00      1.00      1.00        20\n",
      "          51       1.00      1.00      1.00        20\n",
      "         515       0.90      0.95      0.93        20\n",
      "         516       1.00      1.00      1.00        20\n",
      "          52       1.00      1.00      1.00        20\n",
      "         523       1.00      1.00      1.00        20\n",
      "         524       1.00      1.00      1.00        20\n",
      "         532       1.00      1.00      1.00        20\n",
      "         543       1.00      1.00      1.00        20\n",
      "         548       1.00      1.00      1.00        20\n",
      "         568       1.00      1.00      1.00        20\n",
      "         578       1.00      1.00      1.00        20\n",
      "         582       1.00      1.00      1.00        20\n",
      "        5834       1.00      1.00      1.00        20\n",
      "         588       1.00      1.00      1.00        20\n",
      "         592       1.00      1.00      1.00        20\n",
      "         603       1.00      1.00      1.00        20\n",
      "         617       1.00      1.00      1.00        20\n",
      "          63       1.00      1.00      1.00        20\n",
      "         638       1.00      1.00      1.00        20\n",
      "         639       1.00      1.00      1.00        20\n",
      "          64       1.00      1.00      1.00        20\n",
      "         649       1.00      1.00      1.00        20\n",
      "          65       1.00      0.90      0.95        20\n",
      "         654       1.00      1.00      1.00        20\n",
      "         679       1.00      1.00      1.00        20\n",
      "         682       1.00      1.00      1.00        20\n",
      "         684       1.00      1.00      1.00        20\n",
      "         699       1.00      1.00      1.00        20\n",
      "          71       1.00      1.00      1.00        20\n",
      "          72       1.00      1.00      1.00        20\n",
      "         754       0.95      0.90      0.92        20\n",
      "          80       1.00      1.00      1.00        20\n",
      "          81       1.00      1.00      1.00        20\n",
      "         814       0.91      1.00      0.95        20\n",
      "         816       1.00      1.00      1.00        20\n",
      "          82       1.00      0.90      0.95        20\n",
      "          83       1.00      1.00      1.00        20\n",
      "          85       1.00      0.95      0.97        20\n",
      "          86       1.00      1.00      1.00        20\n",
      "         869       1.00      1.00      1.00        20\n",
      "         873       0.91      1.00      0.95        20\n",
      "          88       0.95      1.00      0.98        20\n",
      "         885       1.00      1.00      1.00        20\n",
      "         909       1.00      1.00      1.00        20\n",
      "          92       1.00      1.00      1.00        20\n",
      "          93       1.00      1.00      1.00        20\n",
      "          94       1.00      1.00      1.00        20\n",
      "         949       1.00      1.00      1.00        20\n",
      "          95       1.00      1.00      1.00        20\n",
      "          96       1.00      1.00      1.00        20\n",
      "         964       1.00      1.00      1.00        20\n",
      "         970       1.00      1.00      1.00        20\n",
      "          99       0.95      1.00      0.98        20\n",
      "\n",
      "    accuracy                           0.99      5080\n",
      "   macro avg       0.99      0.99      0.99      5080\n",
      "weighted avg       0.99      0.99      0.99      5080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_logistic_regression(X_train_5, y_train_5, X_val_5, y_val_5, X_test_5, y_test_5)\n",
    "train_logistic_regression(X_train_7, y_train_7, X_val_7, y_val_7, X_test_7, y_test_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "056e340f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmorin/anaconda3/envs/Ethan_MLB_final/lib/python3.14/site-packages/xgboost/training.py:183: UserWarning: [16:24:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1758007733415/work/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/kmorin/anaconda3/envs/Ethan_MLB_final/lib/python3.14/site-packages/xgboost/core.py:705: UserWarning: [16:28:03] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1758007733415/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Validation Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.62      0.69        16\n",
      "           1       1.00      0.88      0.93        16\n",
      "           2       0.94      0.94      0.94        16\n",
      "           3       0.67      0.50      0.57        16\n",
      "           4       0.44      0.50      0.47        16\n",
      "           5       0.50      0.50      0.50        16\n",
      "           6       1.00      1.00      1.00        16\n",
      "           7       1.00      0.94      0.97        16\n",
      "           8       0.76      0.81      0.79        16\n",
      "           9       0.80      0.75      0.77        16\n",
      "          10       0.93      0.81      0.87        16\n",
      "          11       0.67      0.88      0.76        16\n",
      "          12       0.93      0.88      0.90        16\n",
      "          13       0.93      0.81      0.87        16\n",
      "          14       0.71      0.75      0.73        16\n",
      "          15       0.87      0.81      0.84        16\n",
      "          16       0.71      0.94      0.81        16\n",
      "          17       0.81      0.81      0.81        16\n",
      "          18       0.76      1.00      0.86        16\n",
      "          19       0.70      0.88      0.78        16\n",
      "          20       0.94      1.00      0.97        16\n",
      "          21       0.75      0.75      0.75        16\n",
      "          22       0.85      0.69      0.76        16\n",
      "          23       0.87      0.81      0.84        16\n",
      "          24       1.00      1.00      1.00        16\n",
      "          25       0.94      0.94      0.94        16\n",
      "          26       0.80      0.75      0.77        16\n",
      "          27       0.81      0.81      0.81        16\n",
      "          28       0.88      0.94      0.91        16\n",
      "          29       0.78      0.88      0.82        16\n",
      "          30       0.52      0.69      0.59        16\n",
      "          31       0.80      1.00      0.89        16\n",
      "          32       0.93      0.88      0.90        16\n",
      "          33       0.80      1.00      0.89        16\n",
      "          34       0.81      0.81      0.81        16\n",
      "          35       0.80      0.75      0.77        16\n",
      "          36       1.00      0.69      0.81        16\n",
      "          37       0.83      0.94      0.88        16\n",
      "          38       0.72      0.81      0.76        16\n",
      "          39       0.83      0.94      0.88        16\n",
      "          40       0.94      0.94      0.94        16\n",
      "          41       0.93      0.88      0.90        16\n",
      "          42       0.79      0.94      0.86        16\n",
      "          43       0.69      0.69      0.69        16\n",
      "          44       0.76      1.00      0.86        16\n",
      "          45       0.82      0.88      0.85        16\n",
      "          46       0.78      0.88      0.82        16\n",
      "          47       0.85      0.69      0.76        16\n",
      "          48       0.84      1.00      0.91        16\n",
      "          49       0.94      0.94      0.94        16\n",
      "          50       0.76      0.81      0.79        16\n",
      "          51       0.67      0.62      0.65        16\n",
      "          52       0.88      0.88      0.88        16\n",
      "          53       0.67      0.38      0.48        16\n",
      "          54       0.65      0.81      0.72        16\n",
      "          55       1.00      0.81      0.90        16\n",
      "          56       1.00      0.94      0.97        16\n",
      "          57       0.88      0.94      0.91        16\n",
      "          58       0.71      0.75      0.73        16\n",
      "          59       0.80      1.00      0.89        16\n",
      "          60       0.81      0.81      0.81        16\n",
      "          61       0.72      0.81      0.76        16\n",
      "          62       0.36      0.25      0.30        16\n",
      "          63       0.78      0.88      0.82        16\n",
      "          64       0.75      0.75      0.75        16\n",
      "          65       0.74      0.88      0.80        16\n",
      "          66       1.00      1.00      1.00        16\n",
      "          67       0.94      0.94      0.94        16\n",
      "          68       0.61      0.69      0.65        16\n",
      "          69       0.88      0.94      0.91        16\n",
      "          70       0.79      0.94      0.86        16\n",
      "          71       0.82      0.88      0.85        16\n",
      "          72       0.67      0.75      0.71        16\n",
      "          73       0.57      0.50      0.53        16\n",
      "          74       0.78      0.88      0.82        16\n",
      "          75       0.82      0.56      0.67        16\n",
      "          76       1.00      0.88      0.93        16\n",
      "          77       0.88      0.94      0.91        16\n",
      "          78       0.77      0.62      0.69        16\n",
      "          79       0.71      0.75      0.73        16\n",
      "          80       0.78      0.88      0.82        16\n",
      "          81       0.94      1.00      0.97        16\n",
      "          82       0.81      0.81      0.81        16\n",
      "          83       0.71      0.75      0.73        16\n",
      "          84       0.87      0.81      0.84        16\n",
      "          85       0.72      0.81      0.76        16\n",
      "          86       0.63      0.75      0.69        16\n",
      "          87       0.88      0.94      0.91        16\n",
      "          88       0.68      0.94      0.79        16\n",
      "          89       0.89      1.00      0.94        16\n",
      "          90       1.00      0.94      0.97        16\n",
      "          91       0.82      0.88      0.85        16\n",
      "          92       0.57      0.81      0.67        16\n",
      "          93       0.94      0.94      0.94        16\n",
      "          94       0.61      0.69      0.65        16\n",
      "          95       0.72      0.81      0.76        16\n",
      "          96       0.82      0.88      0.85        16\n",
      "          97       0.79      0.94      0.86        16\n",
      "          98       0.85      0.69      0.76        16\n",
      "          99       1.00      0.81      0.90        16\n",
      "         100       0.76      0.81      0.79        16\n",
      "         101       0.87      0.81      0.84        16\n",
      "         102       1.00      0.81      0.90        16\n",
      "         103       0.85      0.69      0.76        16\n",
      "         104       0.65      0.69      0.67        16\n",
      "         105       0.73      0.69      0.71        16\n",
      "         106       0.81      0.81      0.81        16\n",
      "         107       0.88      0.94      0.91        16\n",
      "         108       0.62      0.62      0.62        16\n",
      "         109       0.67      0.75      0.71        16\n",
      "         110       0.82      0.56      0.67        16\n",
      "         111       0.93      0.81      0.87        16\n",
      "         112       0.55      0.69      0.61        16\n",
      "         113       0.73      0.69      0.71        16\n",
      "         114       1.00      0.75      0.86        16\n",
      "         115       1.00      0.88      0.93        16\n",
      "         116       0.88      0.94      0.91        16\n",
      "         117       0.84      1.00      0.91        16\n",
      "         118       0.93      0.81      0.87        16\n",
      "         119       1.00      0.88      0.93        16\n",
      "         120       0.93      0.81      0.87        16\n",
      "         121       1.00      0.75      0.86        16\n",
      "         122       0.82      0.88      0.85        16\n",
      "         123       0.80      0.75      0.77        16\n",
      "         124       0.80      0.75      0.77        16\n",
      "         125       0.93      0.88      0.90        16\n",
      "         126       0.86      0.75      0.80        16\n",
      "         127       0.84      1.00      0.91        16\n",
      "         128       0.93      0.88      0.90        16\n",
      "         129       0.85      0.69      0.76        16\n",
      "         130       0.79      0.94      0.86        16\n",
      "         131       0.88      0.94      0.91        16\n",
      "         132       0.67      1.00      0.80        16\n",
      "         133       0.68      0.94      0.79        16\n",
      "         134       0.87      0.81      0.84        16\n",
      "         135       0.84      1.00      0.91        16\n",
      "         136       0.68      0.81      0.74        16\n",
      "         137       1.00      0.88      0.93        16\n",
      "         138       0.81      0.81      0.81        16\n",
      "         139       0.92      0.69      0.79        16\n",
      "         140       0.75      0.75      0.75        16\n",
      "         141       0.65      0.69      0.67        16\n",
      "         142       0.88      0.88      0.88        16\n",
      "         143       0.92      0.69      0.79        16\n",
      "         144       0.94      0.94      0.94        16\n",
      "         145       0.79      0.69      0.73        16\n",
      "         146       0.67      0.75      0.71        16\n",
      "         147       0.71      0.94      0.81        16\n",
      "         148       0.65      0.81      0.72        16\n",
      "         149       0.94      1.00      0.97        16\n",
      "         150       0.83      0.94      0.88        16\n",
      "         151       0.71      0.62      0.67        16\n",
      "         152       0.86      0.75      0.80        16\n",
      "         153       0.88      0.88      0.88        16\n",
      "         154       0.84      1.00      0.91        16\n",
      "         155       0.94      0.94      0.94        16\n",
      "         156       1.00      0.88      0.93        16\n",
      "         157       0.79      0.69      0.73        16\n",
      "         158       0.76      0.81      0.79        16\n",
      "         159       0.88      0.94      0.91        16\n",
      "         160       0.88      0.94      0.91        16\n",
      "         161       0.88      0.88      0.88        16\n",
      "         162       0.89      1.00      0.94        16\n",
      "         163       0.75      0.75      0.75        16\n",
      "         164       0.67      0.62      0.65        16\n",
      "         165       0.79      0.94      0.86        16\n",
      "         166       0.67      0.88      0.76        16\n",
      "         167       0.82      0.56      0.67        16\n",
      "         168       0.92      0.75      0.83        16\n",
      "         169       0.81      0.81      0.81        16\n",
      "         170       0.75      0.56      0.64        16\n",
      "         171       0.52      0.75      0.62        16\n",
      "         172       0.67      0.62      0.65        16\n",
      "         173       0.65      0.94      0.77        16\n",
      "         174       0.88      0.94      0.91        16\n",
      "         175       0.86      0.75      0.80        16\n",
      "         176       0.89      1.00      0.94        16\n",
      "         177       1.00      0.88      0.93        16\n",
      "         178       1.00      0.62      0.77        16\n",
      "         179       0.93      0.81      0.87        16\n",
      "         180       0.81      0.81      0.81        16\n",
      "         181       0.83      0.94      0.88        16\n",
      "         182       0.80      0.75      0.77        16\n",
      "         183       0.67      0.62      0.65        16\n",
      "         184       0.93      0.81      0.87        16\n",
      "         185       0.69      0.56      0.62        16\n",
      "         186       0.88      0.88      0.88        16\n",
      "         187       0.92      0.69      0.79        16\n",
      "         188       0.93      0.88      0.90        16\n",
      "         189       0.83      0.94      0.88        16\n",
      "         190       0.89      1.00      0.94        16\n",
      "         191       1.00      1.00      1.00        16\n",
      "         192       1.00      0.75      0.86        16\n",
      "         193       0.85      0.69      0.76        16\n",
      "         194       0.88      0.88      0.88        16\n",
      "         195       1.00      0.75      0.86        16\n",
      "         196       0.76      0.81      0.79        16\n",
      "         197       0.56      0.62      0.59        16\n",
      "         198       0.82      0.88      0.85        16\n",
      "         199       0.75      0.56      0.64        16\n",
      "         200       1.00      0.75      0.86        16\n",
      "         201       0.86      0.75      0.80        16\n",
      "         202       0.72      0.81      0.76        16\n",
      "         203       0.67      0.50      0.57        16\n",
      "         204       1.00      1.00      1.00        16\n",
      "         205       1.00      0.88      0.93        16\n",
      "         206       0.63      0.75      0.69        16\n",
      "         207       0.94      0.94      0.94        16\n",
      "         208       0.93      0.88      0.90        16\n",
      "         209       0.81      0.81      0.81        16\n",
      "         210       0.93      0.81      0.87        16\n",
      "         211       1.00      0.75      0.86        16\n",
      "         212       0.86      0.75      0.80        16\n",
      "         213       0.92      0.75      0.83        16\n",
      "         214       0.64      0.88      0.74        16\n",
      "         215       0.93      0.88      0.90        16\n",
      "         216       0.88      0.88      0.88        16\n",
      "         217       0.83      0.94      0.88        16\n",
      "         218       0.88      0.94      0.91        16\n",
      "         219       0.83      0.94      0.88        16\n",
      "         220       0.93      0.81      0.87        16\n",
      "         221       1.00      0.75      0.86        16\n",
      "         222       0.92      0.69      0.79        16\n",
      "         223       0.83      0.62      0.71        16\n",
      "         224       0.73      0.69      0.71        16\n",
      "         225       0.77      0.62      0.69        16\n",
      "         226       1.00      0.94      0.97        16\n",
      "         227       1.00      0.81      0.90        16\n",
      "         228       0.67      0.88      0.76        16\n",
      "         229       0.79      0.69      0.73        16\n",
      "         230       0.94      0.94      0.94        16\n",
      "         231       0.81      0.81      0.81        16\n",
      "         232       1.00      0.75      0.86        16\n",
      "         233       1.00      1.00      1.00        16\n",
      "         234       0.72      0.81      0.76        16\n",
      "         235       0.94      1.00      0.97        16\n",
      "         236       0.92      0.69      0.79        16\n",
      "         237       1.00      0.94      0.97        16\n",
      "         238       0.89      0.50      0.64        16\n",
      "         239       0.94      0.94      0.94        16\n",
      "         240       0.94      1.00      0.97        16\n",
      "         241       0.80      1.00      0.89        16\n",
      "         242       0.68      0.81      0.74        16\n",
      "         243       1.00      0.94      0.97        16\n",
      "         244       0.73      0.50      0.59        16\n",
      "         245       1.00      1.00      1.00        16\n",
      "         246       0.69      0.69      0.69        16\n",
      "         247       0.93      0.88      0.90        16\n",
      "         248       0.89      1.00      0.94        16\n",
      "         249       0.88      0.88      0.88        16\n",
      "         250       0.76      0.81      0.79        16\n",
      "         251       0.76      0.81      0.79        16\n",
      "         252       0.93      0.81      0.87        16\n",
      "         253       0.73      0.69      0.71        16\n",
      "\n",
      "    accuracy                           0.82      4064\n",
      "   macro avg       0.83      0.82      0.82      4064\n",
      "weighted avg       0.83      0.82      0.82      4064\n",
      "\n",
      "XGBoost Test Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86        20\n",
      "           1       1.00      0.90      0.95        20\n",
      "           2       0.83      0.95      0.88        20\n",
      "           3       0.40      0.30      0.34        20\n",
      "           4       0.55      0.55      0.55        20\n",
      "           5       0.67      0.70      0.68        20\n",
      "           6       1.00      0.85      0.92        20\n",
      "           7       0.89      0.85      0.87        20\n",
      "           8       0.72      0.65      0.68        20\n",
      "           9       0.65      0.65      0.65        20\n",
      "          10       0.70      0.80      0.74        20\n",
      "          11       0.67      0.70      0.68        20\n",
      "          12       0.95      0.90      0.92        20\n",
      "          13       0.90      0.90      0.90        20\n",
      "          14       0.79      0.75      0.77        20\n",
      "          15       0.90      0.90      0.90        20\n",
      "          16       0.79      0.95      0.86        20\n",
      "          17       0.79      0.95      0.86        20\n",
      "          18       0.95      0.95      0.95        20\n",
      "          19       0.64      0.70      0.67        20\n",
      "          20       0.90      0.95      0.93        20\n",
      "          21       0.80      0.80      0.80        20\n",
      "          22       0.63      0.60      0.62        20\n",
      "          23       0.72      0.65      0.68        20\n",
      "          24       1.00      1.00      1.00        20\n",
      "          25       0.88      0.75      0.81        20\n",
      "          26       0.58      0.75      0.65        20\n",
      "          27       0.95      0.90      0.92        20\n",
      "          28       0.95      0.90      0.92        20\n",
      "          29       0.78      0.90      0.84        20\n",
      "          30       0.69      0.55      0.61        20\n",
      "          31       0.75      0.90      0.82        20\n",
      "          32       0.95      1.00      0.98        20\n",
      "          33       0.94      0.85      0.89        20\n",
      "          34       0.95      0.95      0.95        20\n",
      "          35       0.95      0.95      0.95        20\n",
      "          36       0.94      0.80      0.86        20\n",
      "          37       0.84      0.80      0.82        20\n",
      "          38       0.76      0.80      0.78        20\n",
      "          39       0.95      0.90      0.92        20\n",
      "          40       0.95      0.95      0.95        20\n",
      "          41       0.86      0.90      0.88        20\n",
      "          42       0.67      0.80      0.73        20\n",
      "          43       0.59      0.80      0.68        20\n",
      "          44       0.74      0.85      0.79        20\n",
      "          45       0.83      0.75      0.79        20\n",
      "          46       0.86      0.95      0.90        20\n",
      "          47       0.76      0.95      0.84        20\n",
      "          48       0.90      0.95      0.93        20\n",
      "          49       0.90      0.95      0.93        20\n",
      "          50       0.74      0.70      0.72        20\n",
      "          51       0.60      0.60      0.60        20\n",
      "          52       0.91      1.00      0.95        20\n",
      "          53       0.80      0.80      0.80        20\n",
      "          54       0.83      0.95      0.88        20\n",
      "          55       0.88      0.75      0.81        20\n",
      "          56       0.95      0.90      0.92        20\n",
      "          57       0.90      0.90      0.90        20\n",
      "          58       0.85      0.85      0.85        20\n",
      "          59       0.75      0.90      0.82        20\n",
      "          60       0.94      0.80      0.86        20\n",
      "          61       0.81      0.85      0.83        20\n",
      "          62       0.71      0.50      0.59        20\n",
      "          63       0.83      0.75      0.79        20\n",
      "          64       0.87      0.65      0.74        20\n",
      "          65       0.83      0.75      0.79        20\n",
      "          66       1.00      0.75      0.86        20\n",
      "          67       0.89      0.85      0.87        20\n",
      "          68       0.72      0.65      0.68        20\n",
      "          69       0.95      0.95      0.95        20\n",
      "          70       0.90      0.95      0.93        20\n",
      "          71       0.75      0.90      0.82        20\n",
      "          72       0.85      0.85      0.85        20\n",
      "          73       0.63      0.60      0.62        20\n",
      "          74       0.95      0.90      0.92        20\n",
      "          75       0.71      0.50      0.59        20\n",
      "          76       0.89      0.85      0.87        20\n",
      "          77       0.94      0.85      0.89        20\n",
      "          78       0.81      0.85      0.83        20\n",
      "          79       0.67      0.80      0.73        20\n",
      "          80       0.63      0.60      0.62        20\n",
      "          81       0.95      0.95      0.95        20\n",
      "          82       1.00      0.90      0.95        20\n",
      "          83       0.75      0.75      0.75        20\n",
      "          84       0.89      0.80      0.84        20\n",
      "          85       0.82      0.90      0.86        20\n",
      "          86       0.90      0.95      0.93        20\n",
      "          87       0.84      0.80      0.82        20\n",
      "          88       0.75      0.75      0.75        20\n",
      "          89       0.85      0.85      0.85        20\n",
      "          90       1.00      0.95      0.97        20\n",
      "          91       0.90      0.95      0.93        20\n",
      "          92       0.55      0.55      0.55        20\n",
      "          93       0.95      0.95      0.95        20\n",
      "          94       0.68      0.85      0.76        20\n",
      "          95       0.80      0.60      0.69        20\n",
      "          96       0.89      0.85      0.87        20\n",
      "          97       0.78      0.90      0.84        20\n",
      "          98       0.94      0.85      0.89        20\n",
      "          99       1.00      0.85      0.92        20\n",
      "         100       0.75      0.75      0.75        20\n",
      "         101       0.90      0.90      0.90        20\n",
      "         102       0.94      0.80      0.86        20\n",
      "         103       1.00      0.75      0.86        20\n",
      "         104       0.58      0.70      0.64        20\n",
      "         105       0.74      0.70      0.72        20\n",
      "         106       0.88      0.75      0.81        20\n",
      "         107       0.83      1.00      0.91        20\n",
      "         108       0.78      0.70      0.74        20\n",
      "         109       0.60      0.60      0.60        20\n",
      "         110       0.95      0.90      0.92        20\n",
      "         111       0.82      0.90      0.86        20\n",
      "         112       0.76      0.80      0.78        20\n",
      "         113       0.62      0.65      0.63        20\n",
      "         114       1.00      0.95      0.97        20\n",
      "         115       0.79      0.75      0.77        20\n",
      "         116       0.62      0.75      0.68        20\n",
      "         117       0.65      0.85      0.74        20\n",
      "         118       0.74      0.85      0.79        20\n",
      "         119       0.89      0.85      0.87        20\n",
      "         120       0.81      0.85      0.83        20\n",
      "         121       0.87      1.00      0.93        20\n",
      "         122       0.86      0.95      0.90        20\n",
      "         123       0.73      0.55      0.63        20\n",
      "         124       0.95      0.95      0.95        20\n",
      "         125       0.83      1.00      0.91        20\n",
      "         126       0.78      0.90      0.84        20\n",
      "         127       0.89      0.80      0.84        20\n",
      "         128       0.77      0.85      0.81        20\n",
      "         129       0.65      0.65      0.65        20\n",
      "         130       0.81      0.85      0.83        20\n",
      "         131       0.94      0.85      0.89        20\n",
      "         132       0.95      0.95      0.95        20\n",
      "         133       0.72      0.90      0.80        20\n",
      "         134       0.89      0.85      0.87        20\n",
      "         135       0.83      0.75      0.79        20\n",
      "         136       0.93      0.70      0.80        20\n",
      "         137       1.00      0.90      0.95        20\n",
      "         138       0.82      0.90      0.86        20\n",
      "         139       0.88      0.75      0.81        20\n",
      "         140       0.75      0.75      0.75        20\n",
      "         141       0.78      0.70      0.74        20\n",
      "         142       0.79      0.95      0.86        20\n",
      "         143       0.70      0.80      0.74        20\n",
      "         144       1.00      0.75      0.86        20\n",
      "         145       0.82      0.70      0.76        20\n",
      "         146       0.68      0.85      0.76        20\n",
      "         147       0.86      0.90      0.88        20\n",
      "         148       0.67      0.80      0.73        20\n",
      "         149       0.86      0.95      0.90        20\n",
      "         150       0.95      0.95      0.95        20\n",
      "         151       0.48      0.60      0.53        20\n",
      "         152       0.62      0.65      0.63        20\n",
      "         153       0.79      0.95      0.86        20\n",
      "         154       0.77      0.85      0.81        20\n",
      "         155       0.88      0.70      0.78        20\n",
      "         156       0.82      0.70      0.76        20\n",
      "         157       0.85      0.85      0.85        20\n",
      "         158       0.76      0.65      0.70        20\n",
      "         159       0.64      0.80      0.71        20\n",
      "         160       0.95      0.90      0.92        20\n",
      "         161       0.67      0.70      0.68        20\n",
      "         162       0.85      0.85      0.85        20\n",
      "         163       1.00      0.85      0.92        20\n",
      "         164       0.56      0.70      0.62        20\n",
      "         165       0.83      1.00      0.91        20\n",
      "         166       0.76      0.95      0.84        20\n",
      "         167       0.85      0.85      0.85        20\n",
      "         168       0.61      0.85      0.71        20\n",
      "         169       0.89      0.85      0.87        20\n",
      "         170       0.73      0.55      0.63        20\n",
      "         171       0.86      0.60      0.71        20\n",
      "         172       0.76      0.80      0.78        20\n",
      "         173       0.72      0.65      0.68        20\n",
      "         174       0.78      0.90      0.84        20\n",
      "         175       0.89      0.80      0.84        20\n",
      "         176       0.95      1.00      0.98        20\n",
      "         177       0.86      0.95      0.90        20\n",
      "         178       0.76      0.80      0.78        20\n",
      "         179       0.86      0.95      0.90        20\n",
      "         180       0.89      0.85      0.87        20\n",
      "         181       0.76      0.95      0.84        20\n",
      "         182       1.00      0.60      0.75        20\n",
      "         183       0.75      0.60      0.67        20\n",
      "         184       0.86      0.90      0.88        20\n",
      "         185       0.83      0.75      0.79        20\n",
      "         186       0.90      0.95      0.93        20\n",
      "         187       0.93      0.70      0.80        20\n",
      "         188       0.93      0.70      0.80        20\n",
      "         189       0.94      0.80      0.86        20\n",
      "         190       0.81      0.85      0.83        20\n",
      "         191       0.77      0.85      0.81        20\n",
      "         192       0.86      0.90      0.88        20\n",
      "         193       0.79      0.75      0.77        20\n",
      "         194       0.86      0.90      0.88        20\n",
      "         195       0.94      0.75      0.83        20\n",
      "         196       0.80      0.80      0.80        20\n",
      "         197       0.84      0.80      0.82        20\n",
      "         198       0.75      0.75      0.75        20\n",
      "         199       0.64      0.70      0.67        20\n",
      "         200       0.80      0.80      0.80        20\n",
      "         201       0.69      0.90      0.78        20\n",
      "         202       0.68      0.75      0.71        20\n",
      "         203       0.71      0.50      0.59        20\n",
      "         204       0.86      0.90      0.88        20\n",
      "         205       1.00      1.00      1.00        20\n",
      "         206       0.82      0.90      0.86        20\n",
      "         207       0.95      0.90      0.92        20\n",
      "         208       0.95      1.00      0.98        20\n",
      "         209       0.86      0.90      0.88        20\n",
      "         210       0.95      0.95      0.95        20\n",
      "         211       1.00      0.95      0.97        20\n",
      "         212       0.85      0.85      0.85        20\n",
      "         213       0.82      0.70      0.76        20\n",
      "         214       0.84      0.80      0.82        20\n",
      "         215       0.74      0.85      0.79        20\n",
      "         216       0.95      0.90      0.92        20\n",
      "         217       0.95      0.90      0.92        20\n",
      "         218       0.95      0.90      0.92        20\n",
      "         219       0.83      1.00      0.91        20\n",
      "         220       0.91      1.00      0.95        20\n",
      "         221       0.88      0.70      0.78        20\n",
      "         222       0.82      0.70      0.76        20\n",
      "         223       0.88      0.70      0.78        20\n",
      "         224       0.75      0.75      0.75        20\n",
      "         225       0.90      0.90      0.90        20\n",
      "         226       0.87      1.00      0.93        20\n",
      "         227       1.00      0.90      0.95        20\n",
      "         228       0.80      0.80      0.80        20\n",
      "         229       0.88      0.70      0.78        20\n",
      "         230       0.90      0.90      0.90        20\n",
      "         231       0.74      0.70      0.72        20\n",
      "         232       0.84      0.80      0.82        20\n",
      "         233       0.94      0.85      0.89        20\n",
      "         234       0.64      0.80      0.71        20\n",
      "         235       0.95      0.90      0.92        20\n",
      "         236       0.69      0.55      0.61        20\n",
      "         237       0.90      0.95      0.93        20\n",
      "         238       0.72      0.65      0.68        20\n",
      "         239       0.90      0.90      0.90        20\n",
      "         240       0.90      0.95      0.93        20\n",
      "         241       0.74      0.85      0.79        20\n",
      "         242       0.62      0.65      0.63        20\n",
      "         243       1.00      0.95      0.97        20\n",
      "         244       0.84      0.80      0.82        20\n",
      "         245       1.00      1.00      1.00        20\n",
      "         246       0.68      0.65      0.67        20\n",
      "         247       0.84      0.80      0.82        20\n",
      "         248       0.95      1.00      0.98        20\n",
      "         249       0.95      0.90      0.92        20\n",
      "         250       0.88      0.70      0.78        20\n",
      "         251       0.76      0.80      0.78        20\n",
      "         252       0.84      0.80      0.82        20\n",
      "         253       0.73      0.80      0.76        20\n",
      "\n",
      "    accuracy                           0.82      5080\n",
      "   macro avg       0.82      0.82      0.82      5080\n",
      "weighted avg       0.82      0.82      0.82      5080\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmorin/anaconda3/envs/Ethan_MLB_final/lib/python3.14/site-packages/xgboost/training.py:183: UserWarning: [16:28:18] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1758007733415/work/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Validation Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.93        16\n",
      "           1       0.94      0.94      0.94        16\n",
      "           2       1.00      1.00      1.00        16\n",
      "           3       0.93      0.81      0.87        16\n",
      "           4       0.81      0.81      0.81        16\n",
      "           5       0.86      0.75      0.80        16\n",
      "           6       0.89      1.00      0.94        16\n",
      "           7       0.84      1.00      0.91        16\n",
      "           8       0.80      1.00      0.89        16\n",
      "           9       1.00      1.00      1.00        16\n",
      "          10       0.84      1.00      0.91        16\n",
      "          11       0.93      0.81      0.87        16\n",
      "          12       1.00      0.94      0.97        16\n",
      "          13       0.94      1.00      0.97        16\n",
      "          14       1.00      0.88      0.93        16\n",
      "          15       1.00      0.94      0.97        16\n",
      "          16       0.89      1.00      0.94        16\n",
      "          17       1.00      1.00      1.00        16\n",
      "          18       1.00      1.00      1.00        16\n",
      "          19       0.93      0.81      0.87        16\n",
      "          20       0.83      0.94      0.88        16\n",
      "          21       1.00      1.00      1.00        16\n",
      "          22       0.88      0.88      0.88        16\n",
      "          23       0.88      0.88      0.88        16\n",
      "          24       0.94      1.00      0.97        16\n",
      "          25       1.00      0.94      0.97        16\n",
      "          26       0.92      0.75      0.83        16\n",
      "          27       0.83      0.94      0.88        16\n",
      "          28       0.94      1.00      0.97        16\n",
      "          29       1.00      0.94      0.97        16\n",
      "          30       0.86      0.75      0.80        16\n",
      "          31       0.94      1.00      0.97        16\n",
      "          32       1.00      0.94      0.97        16\n",
      "          33       1.00      1.00      1.00        16\n",
      "          34       1.00      0.94      0.97        16\n",
      "          35       1.00      1.00      1.00        16\n",
      "          36       1.00      0.94      0.97        16\n",
      "          37       0.83      0.94      0.88        16\n",
      "          38       0.93      0.88      0.90        16\n",
      "          39       0.94      1.00      0.97        16\n",
      "          40       1.00      0.94      0.97        16\n",
      "          41       1.00      0.94      0.97        16\n",
      "          42       0.70      0.88      0.78        16\n",
      "          43       0.79      0.94      0.86        16\n",
      "          44       0.93      0.88      0.90        16\n",
      "          45       1.00      1.00      1.00        16\n",
      "          46       1.00      1.00      1.00        16\n",
      "          47       0.94      0.94      0.94        16\n",
      "          48       0.94      1.00      0.97        16\n",
      "          49       0.94      1.00      0.97        16\n",
      "          50       1.00      0.75      0.86        16\n",
      "          51       0.88      0.88      0.88        16\n",
      "          52       1.00      0.94      0.97        16\n",
      "          53       0.93      0.88      0.90        16\n",
      "          54       1.00      1.00      1.00        16\n",
      "          55       1.00      1.00      1.00        16\n",
      "          56       1.00      0.94      0.97        16\n",
      "          57       1.00      1.00      1.00        16\n",
      "          58       0.94      0.94      0.94        16\n",
      "          59       1.00      1.00      1.00        16\n",
      "          60       0.88      0.94      0.91        16\n",
      "          61       0.83      0.94      0.88        16\n",
      "          62       0.67      0.62      0.65        16\n",
      "          63       0.89      1.00      0.94        16\n",
      "          64       0.94      1.00      0.97        16\n",
      "          65       1.00      1.00      1.00        16\n",
      "          66       1.00      1.00      1.00        16\n",
      "          67       1.00      1.00      1.00        16\n",
      "          68       0.94      0.94      0.94        16\n",
      "          69       1.00      1.00      1.00        16\n",
      "          70       0.93      0.88      0.90        16\n",
      "          71       0.93      0.88      0.90        16\n",
      "          72       0.93      0.81      0.87        16\n",
      "          73       1.00      0.75      0.86        16\n",
      "          74       0.94      0.94      0.94        16\n",
      "          75       1.00      0.94      0.97        16\n",
      "          76       0.94      0.94      0.94        16\n",
      "          77       1.00      1.00      1.00        16\n",
      "          78       1.00      0.94      0.97        16\n",
      "          79       1.00      0.94      0.97        16\n",
      "          80       1.00      1.00      1.00        16\n",
      "          81       0.88      0.94      0.91        16\n",
      "          82       1.00      1.00      1.00        16\n",
      "          83       1.00      0.88      0.93        16\n",
      "          84       1.00      1.00      1.00        16\n",
      "          85       0.94      0.94      0.94        16\n",
      "          86       1.00      1.00      1.00        16\n",
      "          87       0.94      1.00      0.97        16\n",
      "          88       0.76      1.00      0.86        16\n",
      "          89       1.00      1.00      1.00        16\n",
      "          90       1.00      1.00      1.00        16\n",
      "          91       1.00      1.00      1.00        16\n",
      "          92       0.78      0.88      0.82        16\n",
      "          93       1.00      1.00      1.00        16\n",
      "          94       0.80      1.00      0.89        16\n",
      "          95       0.89      1.00      0.94        16\n",
      "          96       1.00      0.94      0.97        16\n",
      "          97       1.00      0.94      0.97        16\n",
      "          98       1.00      0.94      0.97        16\n",
      "          99       0.94      1.00      0.97        16\n",
      "         100       1.00      1.00      1.00        16\n",
      "         101       0.94      1.00      0.97        16\n",
      "         102       1.00      1.00      1.00        16\n",
      "         103       0.79      0.94      0.86        16\n",
      "         104       0.88      0.94      0.91        16\n",
      "         105       1.00      1.00      1.00        16\n",
      "         106       1.00      1.00      1.00        16\n",
      "         107       0.94      1.00      0.97        16\n",
      "         108       0.94      1.00      0.97        16\n",
      "         109       0.75      0.75      0.75        16\n",
      "         110       0.94      0.94      0.94        16\n",
      "         111       1.00      1.00      1.00        16\n",
      "         112       0.78      0.88      0.82        16\n",
      "         113       0.87      0.81      0.84        16\n",
      "         114       0.88      0.88      0.88        16\n",
      "         115       1.00      0.94      0.97        16\n",
      "         116       0.94      1.00      0.97        16\n",
      "         117       1.00      1.00      1.00        16\n",
      "         118       1.00      0.94      0.97        16\n",
      "         119       0.89      1.00      0.94        16\n",
      "         120       0.94      0.94      0.94        16\n",
      "         121       1.00      0.94      0.97        16\n",
      "         122       0.94      1.00      0.97        16\n",
      "         123       0.94      0.94      0.94        16\n",
      "         124       1.00      0.81      0.90        16\n",
      "         125       1.00      1.00      1.00        16\n",
      "         126       1.00      0.88      0.93        16\n",
      "         127       0.88      0.94      0.91        16\n",
      "         128       1.00      1.00      1.00        16\n",
      "         129       0.88      0.94      0.91        16\n",
      "         130       0.89      1.00      0.94        16\n",
      "         131       1.00      0.94      0.97        16\n",
      "         132       1.00      1.00      1.00        16\n",
      "         133       1.00      0.88      0.93        16\n",
      "         134       0.89      1.00      0.94        16\n",
      "         135       0.94      1.00      0.97        16\n",
      "         136       0.94      1.00      0.97        16\n",
      "         137       0.94      1.00      0.97        16\n",
      "         138       0.88      0.88      0.88        16\n",
      "         139       0.92      0.69      0.79        16\n",
      "         140       1.00      1.00      1.00        16\n",
      "         141       0.84      1.00      0.91        16\n",
      "         142       1.00      0.88      0.93        16\n",
      "         143       1.00      1.00      1.00        16\n",
      "         144       1.00      1.00      1.00        16\n",
      "         145       0.94      1.00      0.97        16\n",
      "         146       1.00      0.88      0.93        16\n",
      "         147       1.00      0.94      0.97        16\n",
      "         148       1.00      0.94      0.97        16\n",
      "         149       1.00      1.00      1.00        16\n",
      "         150       1.00      1.00      1.00        16\n",
      "         151       0.77      0.62      0.69        16\n",
      "         152       0.94      1.00      0.97        16\n",
      "         153       0.93      0.88      0.90        16\n",
      "         154       1.00      1.00      1.00        16\n",
      "         155       0.83      0.94      0.88        16\n",
      "         156       1.00      1.00      1.00        16\n",
      "         157       0.94      1.00      0.97        16\n",
      "         158       0.88      0.94      0.91        16\n",
      "         159       0.94      1.00      0.97        16\n",
      "         160       1.00      1.00      1.00        16\n",
      "         161       0.84      1.00      0.91        16\n",
      "         162       0.84      1.00      0.91        16\n",
      "         163       0.94      0.94      0.94        16\n",
      "         164       1.00      0.94      0.97        16\n",
      "         165       0.88      0.94      0.91        16\n",
      "         166       0.78      0.88      0.82        16\n",
      "         167       1.00      0.94      0.97        16\n",
      "         168       1.00      0.94      0.97        16\n",
      "         169       1.00      1.00      1.00        16\n",
      "         170       0.83      0.62      0.71        16\n",
      "         171       0.80      1.00      0.89        16\n",
      "         172       0.83      0.62      0.71        16\n",
      "         173       0.78      0.88      0.82        16\n",
      "         174       0.94      0.94      0.94        16\n",
      "         175       1.00      0.94      0.97        16\n",
      "         176       1.00      0.94      0.97        16\n",
      "         177       1.00      1.00      1.00        16\n",
      "         178       1.00      0.88      0.93        16\n",
      "         179       1.00      1.00      1.00        16\n",
      "         180       0.94      0.94      0.94        16\n",
      "         181       1.00      0.94      0.97        16\n",
      "         182       0.94      1.00      0.97        16\n",
      "         183       0.88      0.94      0.91        16\n",
      "         184       1.00      1.00      1.00        16\n",
      "         185       0.92      0.75      0.83        16\n",
      "         186       1.00      1.00      1.00        16\n",
      "         187       1.00      0.94      0.97        16\n",
      "         188       0.94      1.00      0.97        16\n",
      "         189       1.00      1.00      1.00        16\n",
      "         190       0.94      1.00      0.97        16\n",
      "         191       1.00      0.94      0.97        16\n",
      "         192       1.00      1.00      1.00        16\n",
      "         193       0.94      0.94      0.94        16\n",
      "         194       0.94      1.00      0.97        16\n",
      "         195       1.00      0.94      0.97        16\n",
      "         196       1.00      1.00      1.00        16\n",
      "         197       0.88      0.88      0.88        16\n",
      "         198       1.00      1.00      1.00        16\n",
      "         199       0.94      1.00      0.97        16\n",
      "         200       1.00      1.00      1.00        16\n",
      "         201       0.80      0.75      0.77        16\n",
      "         202       0.82      0.88      0.85        16\n",
      "         203       0.89      1.00      0.94        16\n",
      "         204       0.94      1.00      0.97        16\n",
      "         205       0.88      0.94      0.91        16\n",
      "         206       1.00      0.94      0.97        16\n",
      "         207       1.00      1.00      1.00        16\n",
      "         208       1.00      0.94      0.97        16\n",
      "         209       0.94      0.94      0.94        16\n",
      "         210       0.94      0.94      0.94        16\n",
      "         211       1.00      1.00      1.00        16\n",
      "         212       1.00      0.88      0.93        16\n",
      "         213       1.00      0.94      0.97        16\n",
      "         214       0.88      0.88      0.88        16\n",
      "         215       0.94      0.94      0.94        16\n",
      "         216       1.00      0.94      0.97        16\n",
      "         217       0.94      0.94      0.94        16\n",
      "         218       0.94      0.94      0.94        16\n",
      "         219       1.00      1.00      1.00        16\n",
      "         220       1.00      0.81      0.90        16\n",
      "         221       1.00      0.88      0.93        16\n",
      "         222       1.00      1.00      1.00        16\n",
      "         223       1.00      0.81      0.90        16\n",
      "         224       0.88      0.94      0.91        16\n",
      "         225       0.89      1.00      0.94        16\n",
      "         226       1.00      1.00      1.00        16\n",
      "         227       1.00      1.00      1.00        16\n",
      "         228       1.00      0.94      0.97        16\n",
      "         229       0.94      1.00      0.97        16\n",
      "         230       0.94      0.94      0.94        16\n",
      "         231       1.00      0.81      0.90        16\n",
      "         232       1.00      1.00      1.00        16\n",
      "         233       0.94      1.00      0.97        16\n",
      "         234       0.94      1.00      0.97        16\n",
      "         235       1.00      1.00      1.00        16\n",
      "         236       0.93      0.81      0.87        16\n",
      "         237       1.00      1.00      1.00        16\n",
      "         238       0.83      0.94      0.88        16\n",
      "         239       1.00      1.00      1.00        16\n",
      "         240       1.00      1.00      1.00        16\n",
      "         241       0.89      1.00      0.94        16\n",
      "         242       1.00      0.94      0.97        16\n",
      "         243       1.00      0.94      0.97        16\n",
      "         244       1.00      0.81      0.90        16\n",
      "         245       0.89      1.00      0.94        16\n",
      "         246       0.92      0.75      0.83        16\n",
      "         247       0.94      0.94      0.94        16\n",
      "         248       0.93      0.88      0.90        16\n",
      "         249       0.94      1.00      0.97        16\n",
      "         250       0.82      0.88      0.85        16\n",
      "         251       1.00      0.94      0.97        16\n",
      "         252       0.94      1.00      0.97        16\n",
      "         253       0.78      0.88      0.82        16\n",
      "\n",
      "    accuracy                           0.94      4064\n",
      "   macro avg       0.94      0.94      0.94      4064\n",
      "weighted avg       0.94      0.94      0.94      4064\n",
      "\n",
      "XGBoost Test Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.95      0.86        20\n",
      "           1       1.00      0.95      0.97        20\n",
      "           2       1.00      1.00      1.00        20\n",
      "           3       0.93      0.70      0.80        20\n",
      "           4       0.65      0.75      0.70        20\n",
      "           5       0.89      0.85      0.87        20\n",
      "           6       0.86      0.95      0.90        20\n",
      "           7       0.94      0.85      0.89        20\n",
      "           8       0.90      0.95      0.93        20\n",
      "           9       0.86      0.90      0.88        20\n",
      "          10       0.83      0.95      0.88        20\n",
      "          11       0.86      0.95      0.90        20\n",
      "          12       1.00      0.95      0.97        20\n",
      "          13       0.95      0.95      0.95        20\n",
      "          14       0.91      1.00      0.95        20\n",
      "          15       0.83      0.95      0.88        20\n",
      "          16       1.00      0.95      0.97        20\n",
      "          17       1.00      1.00      1.00        20\n",
      "          18       0.95      1.00      0.98        20\n",
      "          19       0.94      0.85      0.89        20\n",
      "          20       0.90      0.95      0.93        20\n",
      "          21       0.95      1.00      0.98        20\n",
      "          22       0.89      0.85      0.87        20\n",
      "          23       1.00      0.85      0.92        20\n",
      "          24       0.95      1.00      0.98        20\n",
      "          25       1.00      0.95      0.97        20\n",
      "          26       0.90      0.95      0.93        20\n",
      "          27       0.87      1.00      0.93        20\n",
      "          28       0.95      1.00      0.98        20\n",
      "          29       0.95      1.00      0.98        20\n",
      "          30       0.89      0.85      0.87        20\n",
      "          31       0.80      1.00      0.89        20\n",
      "          32       1.00      1.00      1.00        20\n",
      "          33       0.95      0.95      0.95        20\n",
      "          34       1.00      0.95      0.97        20\n",
      "          35       0.95      1.00      0.98        20\n",
      "          36       1.00      0.95      0.97        20\n",
      "          37       0.89      0.85      0.87        20\n",
      "          38       1.00      1.00      1.00        20\n",
      "          39       0.95      1.00      0.98        20\n",
      "          40       0.95      1.00      0.98        20\n",
      "          41       0.91      1.00      0.95        20\n",
      "          42       0.75      0.90      0.82        20\n",
      "          43       0.89      0.85      0.87        20\n",
      "          44       0.76      0.95      0.84        20\n",
      "          45       0.95      0.95      0.95        20\n",
      "          46       1.00      1.00      1.00        20\n",
      "          47       0.95      0.95      0.95        20\n",
      "          48       0.91      1.00      0.95        20\n",
      "          49       1.00      0.90      0.95        20\n",
      "          50       0.94      0.85      0.89        20\n",
      "          51       0.89      0.85      0.87        20\n",
      "          52       1.00      0.95      0.97        20\n",
      "          53       1.00      0.95      0.97        20\n",
      "          54       1.00      1.00      1.00        20\n",
      "          55       0.95      0.95      0.95        20\n",
      "          56       1.00      1.00      1.00        20\n",
      "          57       0.91      1.00      0.95        20\n",
      "          58       0.95      0.95      0.95        20\n",
      "          59       0.95      1.00      0.98        20\n",
      "          60       1.00      0.95      0.97        20\n",
      "          61       0.91      1.00      0.95        20\n",
      "          62       1.00      0.60      0.75        20\n",
      "          63       1.00      0.90      0.95        20\n",
      "          64       1.00      0.90      0.95        20\n",
      "          65       1.00      1.00      1.00        20\n",
      "          66       1.00      1.00      1.00        20\n",
      "          67       1.00      0.95      0.97        20\n",
      "          68       0.87      1.00      0.93        20\n",
      "          69       1.00      1.00      1.00        20\n",
      "          70       1.00      1.00      1.00        20\n",
      "          71       0.95      0.90      0.92        20\n",
      "          72       0.89      0.80      0.84        20\n",
      "          73       0.95      0.90      0.92        20\n",
      "          74       1.00      0.95      0.97        20\n",
      "          75       0.91      1.00      0.95        20\n",
      "          76       0.95      1.00      0.98        20\n",
      "          77       1.00      0.95      0.97        20\n",
      "          78       1.00      0.85      0.92        20\n",
      "          79       1.00      1.00      1.00        20\n",
      "          80       1.00      0.90      0.95        20\n",
      "          81       0.95      0.90      0.92        20\n",
      "          82       0.95      0.90      0.92        20\n",
      "          83       0.90      0.90      0.90        20\n",
      "          84       1.00      1.00      1.00        20\n",
      "          85       0.87      1.00      0.93        20\n",
      "          86       0.90      0.95      0.93        20\n",
      "          87       1.00      0.95      0.97        20\n",
      "          88       0.83      1.00      0.91        20\n",
      "          89       1.00      1.00      1.00        20\n",
      "          90       0.95      0.95      0.95        20\n",
      "          91       1.00      1.00      1.00        20\n",
      "          92       0.75      0.90      0.82        20\n",
      "          93       1.00      1.00      1.00        20\n",
      "          94       0.90      0.90      0.90        20\n",
      "          95       0.90      0.90      0.90        20\n",
      "          96       0.82      0.90      0.86        20\n",
      "          97       0.94      0.85      0.89        20\n",
      "          98       1.00      0.95      0.97        20\n",
      "          99       1.00      1.00      1.00        20\n",
      "         100       0.90      0.90      0.90        20\n",
      "         101       1.00      0.95      0.97        20\n",
      "         102       1.00      1.00      1.00        20\n",
      "         103       0.86      0.95      0.90        20\n",
      "         104       0.86      0.95      0.90        20\n",
      "         105       0.91      1.00      0.95        20\n",
      "         106       1.00      1.00      1.00        20\n",
      "         107       0.95      1.00      0.98        20\n",
      "         108       0.94      0.85      0.89        20\n",
      "         109       0.78      0.70      0.74        20\n",
      "         110       0.87      1.00      0.93        20\n",
      "         111       0.91      1.00      0.95        20\n",
      "         112       0.85      0.85      0.85        20\n",
      "         113       0.86      0.90      0.88        20\n",
      "         114       1.00      1.00      1.00        20\n",
      "         115       1.00      0.85      0.92        20\n",
      "         116       1.00      0.85      0.92        20\n",
      "         117       0.91      1.00      0.95        20\n",
      "         118       0.95      1.00      0.98        20\n",
      "         119       0.95      1.00      0.98        20\n",
      "         120       1.00      0.95      0.97        20\n",
      "         121       1.00      1.00      1.00        20\n",
      "         122       0.95      0.95      0.95        20\n",
      "         123       1.00      0.80      0.89        20\n",
      "         124       0.90      0.95      0.93        20\n",
      "         125       1.00      1.00      1.00        20\n",
      "         126       0.86      0.95      0.90        20\n",
      "         127       1.00      0.90      0.95        20\n",
      "         128       1.00      1.00      1.00        20\n",
      "         129       0.87      1.00      0.93        20\n",
      "         130       1.00      0.80      0.89        20\n",
      "         131       1.00      1.00      1.00        20\n",
      "         132       0.95      1.00      0.98        20\n",
      "         133       0.83      0.95      0.88        20\n",
      "         134       0.85      0.85      0.85        20\n",
      "         135       1.00      0.90      0.95        20\n",
      "         136       1.00      1.00      1.00        20\n",
      "         137       1.00      0.95      0.97        20\n",
      "         138       0.95      0.95      0.95        20\n",
      "         139       0.86      0.95      0.90        20\n",
      "         140       0.90      0.90      0.90        20\n",
      "         141       1.00      0.85      0.92        20\n",
      "         142       0.95      1.00      0.98        20\n",
      "         143       0.95      1.00      0.98        20\n",
      "         144       0.95      0.95      0.95        20\n",
      "         145       1.00      0.95      0.97        20\n",
      "         146       0.95      0.90      0.92        20\n",
      "         147       1.00      1.00      1.00        20\n",
      "         148       0.87      1.00      0.93        20\n",
      "         149       0.90      0.90      0.90        20\n",
      "         150       1.00      1.00      1.00        20\n",
      "         151       0.67      0.80      0.73        20\n",
      "         152       0.87      1.00      0.93        20\n",
      "         153       1.00      1.00      1.00        20\n",
      "         154       0.95      1.00      0.98        20\n",
      "         155       1.00      0.85      0.92        20\n",
      "         156       1.00      0.80      0.89        20\n",
      "         157       1.00      0.95      0.97        20\n",
      "         158       0.95      0.95      0.95        20\n",
      "         159       0.90      0.95      0.93        20\n",
      "         160       1.00      0.95      0.97        20\n",
      "         161       0.86      0.95      0.90        20\n",
      "         162       0.86      0.90      0.88        20\n",
      "         163       0.91      1.00      0.95        20\n",
      "         164       0.90      0.95      0.93        20\n",
      "         165       1.00      0.95      0.97        20\n",
      "         166       0.87      1.00      0.93        20\n",
      "         167       1.00      0.90      0.95        20\n",
      "         168       1.00      0.95      0.97        20\n",
      "         169       1.00      1.00      1.00        20\n",
      "         170       0.87      0.65      0.74        20\n",
      "         171       1.00      1.00      1.00        20\n",
      "         172       0.95      1.00      0.98        20\n",
      "         173       0.95      0.95      0.95        20\n",
      "         174       1.00      1.00      1.00        20\n",
      "         175       0.95      0.90      0.92        20\n",
      "         176       1.00      1.00      1.00        20\n",
      "         177       0.86      0.90      0.88        20\n",
      "         178       0.87      1.00      0.93        20\n",
      "         179       1.00      1.00      1.00        20\n",
      "         180       0.95      0.95      0.95        20\n",
      "         181       0.89      0.85      0.87        20\n",
      "         182       0.95      0.95      0.95        20\n",
      "         183       0.95      0.95      0.95        20\n",
      "         184       1.00      1.00      1.00        20\n",
      "         185       0.87      1.00      0.93        20\n",
      "         186       1.00      1.00      1.00        20\n",
      "         187       1.00      0.95      0.97        20\n",
      "         188       1.00      0.90      0.95        20\n",
      "         189       0.95      0.90      0.92        20\n",
      "         190       0.95      0.95      0.95        20\n",
      "         191       1.00      0.95      0.97        20\n",
      "         192       0.95      0.95      0.95        20\n",
      "         193       1.00      1.00      1.00        20\n",
      "         194       0.90      0.95      0.93        20\n",
      "         195       1.00      0.90      0.95        20\n",
      "         196       1.00      1.00      1.00        20\n",
      "         197       0.83      0.95      0.88        20\n",
      "         198       0.91      1.00      0.95        20\n",
      "         199       0.95      0.95      0.95        20\n",
      "         200       1.00      0.95      0.97        20\n",
      "         201       0.83      0.75      0.79        20\n",
      "         202       0.90      0.95      0.93        20\n",
      "         203       1.00      0.95      0.97        20\n",
      "         204       0.95      1.00      0.98        20\n",
      "         205       1.00      1.00      1.00        20\n",
      "         206       1.00      1.00      1.00        20\n",
      "         207       1.00      1.00      1.00        20\n",
      "         208       0.95      0.95      0.95        20\n",
      "         209       1.00      0.85      0.92        20\n",
      "         210       0.95      1.00      0.98        20\n",
      "         211       0.94      0.85      0.89        20\n",
      "         212       1.00      0.90      0.95        20\n",
      "         213       1.00      1.00      1.00        20\n",
      "         214       1.00      0.85      0.92        20\n",
      "         215       0.87      1.00      0.93        20\n",
      "         216       0.95      1.00      0.98        20\n",
      "         217       1.00      1.00      1.00        20\n",
      "         218       0.86      0.90      0.88        20\n",
      "         219       1.00      1.00      1.00        20\n",
      "         220       1.00      1.00      1.00        20\n",
      "         221       1.00      0.95      0.97        20\n",
      "         222       0.95      0.95      0.95        20\n",
      "         223       0.94      0.85      0.89        20\n",
      "         224       0.90      0.95      0.93        20\n",
      "         225       1.00      0.95      0.97        20\n",
      "         226       0.95      1.00      0.98        20\n",
      "         227       1.00      1.00      1.00        20\n",
      "         228       1.00      0.90      0.95        20\n",
      "         229       0.90      0.95      0.93        20\n",
      "         230       1.00      1.00      1.00        20\n",
      "         231       0.95      0.90      0.92        20\n",
      "         232       1.00      0.90      0.95        20\n",
      "         233       1.00      0.95      0.97        20\n",
      "         234       0.91      1.00      0.95        20\n",
      "         235       1.00      0.95      0.97        20\n",
      "         236       0.89      0.80      0.84        20\n",
      "         237       1.00      1.00      1.00        20\n",
      "         238       1.00      0.85      0.92        20\n",
      "         239       1.00      1.00      1.00        20\n",
      "         240       0.95      1.00      0.98        20\n",
      "         241       0.90      0.95      0.93        20\n",
      "         242       0.95      0.90      0.92        20\n",
      "         243       1.00      0.90      0.95        20\n",
      "         244       1.00      0.85      0.92        20\n",
      "         245       0.90      0.95      0.93        20\n",
      "         246       1.00      0.95      0.97        20\n",
      "         247       1.00      0.95      0.97        20\n",
      "         248       1.00      0.95      0.97        20\n",
      "         249       1.00      1.00      1.00        20\n",
      "         250       1.00      0.90      0.95        20\n",
      "         251       1.00      0.95      0.97        20\n",
      "         252       1.00      1.00      1.00        20\n",
      "         253       0.86      0.90      0.88        20\n",
      "\n",
      "    accuracy                           0.94      5080\n",
      "   macro avg       0.95      0.94      0.94      5080\n",
      "weighted avg       0.95      0.94      0.94      5080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_xgboost(X_train_5, y_train_5, X_val_5, y_val_5, X_test_5, y_test_5)\n",
    "train_xgboost(X_train_7, y_train_7, X_val_7, y_val_7, X_test_7, y_test_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "04bf1087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix for test set\n",
    "# cm = confusion_matrix(y_test, y_test_pred)\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.colorbar()\n",
    "# tick_marks = np.arange(len(set(y_test)))\n",
    "# plt.xticks(tick_marks, set(y_test), rotation=45)\n",
    "# plt.yticks(tick_marks, set(y_test))\n",
    "# plt.ylabel('True label')\n",
    "# plt.xlabel('Predicted label')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "51299d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import optim\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e65952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, y):\n",
    "    return (preds == y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e2d856db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1(out_dim,preds,y):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Initialize F1Score for binary classification\n",
    "    f1 = F1Score(task=\"multiclass\", num_classes=out_dim, average='macro').to(device)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    return f1(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9bb9d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PanspaceCNN(pl.LightningModule):\n",
    "    def __init__(self, input_size, out_dim, learning_rate):\n",
    "        \"\"\"\n",
    "        Initializes the PyTorch model for Panspace.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the square CGR image (e.g., 64 for k=6).\n",
    "            out_dim (int): The size of the output vector.\n",
    "        \"\"\"\n",
    "        super(PanspaceCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        # The number of filters and kernel sizes are example values and can be tuned.\n",
    "        self.save_hyperparameters()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        # Max pooling layers\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        # Calculate the size of the flattened tensor after convolutions and pooling.\n",
    "        # This assumes a 1xinput_size x input_size input image.\n",
    "        final_conv_size = input_size // (2**3)  # After 3 pooling layers\n",
    "        self.fc1 = nn.Linear(128 * final_conv_size * final_conv_size, 512)\n",
    "        self.fc2 = nn.Linear(512, out_dim)\n",
    "\n",
    "        # Batch normalization and activation functions\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): A tensor representing the CGR image of shape (batch_size, 1, H, W).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor with predicted probabilities for each class of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # Apply convolutions, batch normalization, and pooling\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        # Flatten the tensor\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Apply fully connected layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Apply softmax\n",
    "        # x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, torch.argmax(y,dim=1))\n",
    "        f1 = F1(self.hparams.out_dim,preds,torch.argmax(y,dim=1))\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        self.log('val_f1', f1, prog_bar=True)\n",
    "        return {'val_loss': loss, 'val_acc': acc, 'val_f1': f1}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, torch.argmax(y,dim=1))\n",
    "        f1 = F1(self.hparams.out_dim,preds,torch.argmax(y,dim=1))\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        self.log('test_f1', f1, prog_bar=True)\n",
    "        return {'test_loss': loss, 'test_acc': acc, 'test_f1': f1}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "da276f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# scale classes for one-hot encoding\n",
    "# ----------------------------------\n",
    "\n",
    "def scale(y,classes):\n",
    "    y_scaled = []\n",
    "    for label in y:\n",
    "        index = np.where(classes == label)\n",
    "        y_scaled.append(index[0][0])\n",
    "    y_scale = np.array(y_scaled)\n",
    "    return y_scale\n",
    "\n",
    "def prepare_data(X_train,X_val,X_test,y_train,y_val,y_test,size=32,out_dim=254):\n",
    "    y_train = y_train.astype(int)\n",
    "    y_val = y_val.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    classes = np.sort(np.unique(y_train))\n",
    "\n",
    "    y_train_scale = scale(y_train,classes)\n",
    "    y_val_scale = scale(y_val,classes)\n",
    "    y_test_scale = scale(y_test,classes)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Convert data to torch tensors\n",
    "    # -----------------------------\n",
    "    X_train_tensor = torch.tensor(X_train.reshape(-1,size,size)).unsqueeze(1).float()  # (N, 1, size, size)\n",
    "    X_val_tensor = torch.tensor(X_val.reshape(-1,size,size)).unsqueeze(1).float()\n",
    "    X_test_tensor = torch.tensor(X_test.reshape(-1,size,size)).unsqueeze(1).float()\n",
    "    y_train_tensor = torch.tensor(y_train_scale)\n",
    "    y_val_tensor = torch.tensor(y_val_scale)\n",
    "    y_test_tensor = torch.tensor(y_test_scale)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Encode labels to categorical\n",
    "    # -----------------------------\n",
    "    y_train_enc = F.one_hot(y_train_tensor, num_classes = out_dim).float()\n",
    "    y_val_enc = F.one_hot(y_val_tensor, num_classes = out_dim).float()\n",
    "    y_test_enc = F.one_hot(y_test_tensor, num_classes= out_dim).float()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Create PyTorch Datasets and Loaders\n",
    "    # -----------------------------\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_enc)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_enc)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_enc)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "    return train_loader,val_loader,test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "db7079f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Set the hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-7, 1e-2, log=True)\n",
    "    \n",
    "    # Create the model with trial hyperparameters\n",
    "    model = PanspaceCNN(\n",
    "        input_size=128,\n",
    "        out_dim=254,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        verbose=False,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    # Optuna pruning callback\n",
    "    pruning_callback = PyTorchLightningPruningCallback(trial, monitor='val_loss')\n",
    "    \n",
    "    # Logger\n",
    "    logger = TensorBoardLogger(save_dir=os.getcwd(), name=f\"optuna_logs/trial_{trial.number}\")\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        callbacks=[early_stop_callback, pruning_callback],\n",
    "        logger=logger,\n",
    "        enable_progress_bar=False,  \n",
    "        enable_model_summary=False  \n",
    "    )\n",
    "    \n",
    "    # Preparing the data\n",
    "    train_loader, val_loader, test_loader = prepare_data(X_train_7,X_val_7,X_test_7,y_train_7,y_val_7,y_test_7,128,254)\n",
    "    \n",
    "    # Training the model\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    # Final validation loss\n",
    "    return trainer.callback_metrics['val_loss'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b47ab432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(n_trials=50):\n",
    "    pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    "    study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"  Value: {trial.value}\")\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "        \n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "955f0f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best_model(study,X_train,X_val,X_test,y_train,y_val,y_test,size,num_classes):\n",
    "    # Getting the best hyperparameters\n",
    "    best_params = study.best_trial.params\n",
    "    \n",
    "    # Creating the model with the best hyperparameters\n",
    "    model = PanspaceCNN(\n",
    "        input_size=size,\n",
    "        out_dim=num_classes,\n",
    "        learning_rate=best_params['learning_rate']\n",
    "    )\n",
    "    \n",
    "    # Creating trainer instance\n",
    "    trainer = pl.Trainer(max_epochs=150)\n",
    "    \n",
    "    # Preparing the data\n",
    "    train_loader, val_loader, test_loader = prepare_data(X_train,X_val,X_test,y_train,y_val,y_test,size,num_classes)\n",
    "    \n",
    "    # Training the model with the best hyperparameters\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    # Testing the model with the test data\n",
    "    results = trainer.test(model, dataloaders=test_loader)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "65dce016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-03 15:59:39,018] A new study created in memory with name: no-name-4bcbb982-1aba-4e98-8ca7-32af0f9574a8\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2025-11-03 16:16:09,297] Trial 0 finished with value: 5.537333011627197 and parameters: {'learning_rate': 0.000343417155197835}. Best is trial 0 with value: 5.537333011627197.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  51%|     | 129/254 [34:57<33:52,  0.06it/s, v_num=8, train_loss=5.540, val_loss=5.540, val_acc=0.00394, val_f1=0.000135]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2025-11-03 16:32:50,632] Trial 1 finished with value: 5.463733196258545 and parameters: {'learning_rate': 2.073647366715919e-07}. Best is trial 1 with value: 5.463733196258545.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-11-03 16:49:04,777] Trial 2 finished with value: 5.537347316741943 and parameters: {'learning_rate': 0.0004792266424093759}. Best is trial 1 with value: 5.463733196258545.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2025-11-03 17:06:21,356] Trial 3 finished with value: 5.122861862182617 and parameters: {'learning_rate': 3.6515126524359046e-06}. Best is trial 3 with value: 5.122861862182617.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2025-11-03 17:23:30,391] Trial 4 finished with value: 5.537402153015137 and parameters: {'learning_rate': 0.00012381033468771127}. Best is trial 3 with value: 5.122861862182617.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2025-11-03 17:40:44,542] Trial 5 finished with value: 5.341952800750732 and parameters: {'learning_rate': 5.688272216625475e-07}. Best is trial 3 with value: 5.122861862182617.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-11-03 17:44:27,479] Trial 6 pruned. Trial was pruned at epoch 10.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2025-11-03 18:00:57,890] Trial 7 finished with value: 5.240983486175537 and parameters: {'learning_rate': 9.100724558681012e-06}. Best is trial 3 with value: 5.122861862182617.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-11-03 18:04:31,924] Trial 8 pruned. Trial was pruned at epoch 10.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2025-11-03 18:20:57,170] Trial 9 finished with value: 5.170802116394043 and parameters: {'learning_rate': 1.5354623495747193e-06}. Best is trial 3 with value: 5.122861862182617.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-11-03 18:24:35,771] Trial 10 pruned. Trial was pruned at epoch 10.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2025-11-03 18:40:58,340] Trial 11 finished with value: 5.102008819580078 and parameters: {'learning_rate': 1.4682464847209767e-06}. Best is trial 11 with value: 5.102008819580078.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2025-11-03 18:57:18,962] Trial 12 finished with value: 5.097236633300781 and parameters: {'learning_rate': 2.7006153026513106e-06}. Best is trial 12 with value: 5.097236633300781.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-11-03 19:00:52,760] Trial 13 pruned. Trial was pruned at epoch 10.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-11-03 19:04:30,329] Trial 14 pruned. Trial was pruned at epoch 10.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-11-03 19:08:28,352] Trial 15 pruned. Trial was pruned at epoch 11.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2025-11-03 19:24:54,951] Trial 16 finished with value: 5.180093765258789 and parameters: {'learning_rate': 1.641779294799627e-05}. Best is trial 12 with value: 5.097236633300781.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2025-11-03 19:41:22,134] Trial 17 finished with value: 5.155777454376221 and parameters: {'learning_rate': 1.4484773380998577e-06}. Best is trial 12 with value: 5.097236633300781.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-11-03 19:44:56,633] Trial 18 pruned. Trial was pruned at epoch 10.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2025-11-03 20:01:36,652] Trial 19 finished with value: 5.156925201416016 and parameters: {'learning_rate': 2.1784779107828487e-06}. Best is trial 12 with value: 5.097236633300781.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-11-03 20:05:19,052] Trial 20 pruned. Trial was pruned at epoch 10.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-11-03 20:09:16,649] Trial 21 pruned. Trial was pruned at epoch 10.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-11-03 20:12:57,573] Trial 22 pruned. Trial was pruned at epoch 10.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-11-03 20:16:33,101] Trial 23 pruned. Trial was pruned at epoch 10.\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[I 2025-11-03 20:20:09,605] Trial 24 pruned. Trial was pruned at epoch 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 5.097236633300781\n",
      "  Params: \n",
      "    learning_rate: 2.7006153026513106e-06\n"
     ]
    }
   ],
   "source": [
    "study = run_optimization(n_trials=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a9fab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type        | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | conv1 | Conv2d      | 320    | train\n",
      "1 | conv2 | Conv2d      | 18.5 K | train\n",
      "2 | conv3 | Conv2d      | 73.9 K | train\n",
      "3 | pool  | MaxPool2d   | 0      | train\n",
      "4 | fc1   | Linear      | 1.0 M  | train\n",
      "5 | fc2   | Linear      | 130 K  | train\n",
      "6 | bn1   | BatchNorm2d | 64     | train\n",
      "7 | bn2   | BatchNorm2d | 128    | train\n",
      "8 | bn3   | BatchNorm2d | 256    | train\n",
      "9 | relu  | ReLU        | 0      | train\n",
      "----------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.090     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmorin/anaconda3/envs/Ethan_MLB_final/lib/python3.14/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: PossibleUserWarning:\n",
      "\n",
      "The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "\n",
      "/home/kmorin/anaconda3/envs/Ethan_MLB_final/lib/python3.14/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: PossibleUserWarning:\n",
      "\n",
      "The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149: 100%|| 254/254 [00:02<00:00, 91.20it/s, v_num=7, train_loss=0.255, val_loss=0.373, val_acc=0.894, val_f1=0.821]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=150` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149: 100%|| 254/254 [00:03<00:00, 84.08it/s, v_num=7, train_loss=0.255, val_loss=0.373, val_acc=0.894, val_f1=0.821]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/kmorin/anaconda3/envs/Ethan_MLB_final/lib/python3.14/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: PossibleUserWarning:\n",
      "\n",
      "The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|| 80/80 [00:00<00:00, 130.49it/s]\n",
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "        test_acc            0.9007874131202698\n",
      "         test_f1            0.8333085775375366\n",
      "        test_loss           0.3248129189014435\n",
      "\n",
      "Test results with best hyperparameters: [{'test_loss': 0.3248129189014435, 'test_acc': 0.9007874131202698, 'test_f1': 0.8333085775375366}]\n"
     ]
    }
   ],
   "source": [
    "# Best trial:\n",
    "#   Value: 0.973054051399231\n",
    "#   Params: \n",
    "#     learning_rate: 4.786642076991138e-05\n",
    "\n",
    "# Visualize the results\n",
    "try:\n",
    "    # Plot optimization history\n",
    "    optuna.visualization.plot_optimization_history(study)\n",
    "    \n",
    "    # Plot parameter importances\n",
    "    optuna.visualization.plot_param_importances(study)\n",
    "    \n",
    "    # Plot parallel coordinate plot\n",
    "    optuna.visualization.plot_parallel_coordinate(study)\n",
    "except ImportError:\n",
    "    print(\"Visualization requires plotly. Install with: pip install plotly\")\n",
    "  \n",
    "  # Test the best model\n",
    "results = test_best_model(study,X_train_5,X_val_5,X_test_5,y_train_5,y_val_5,y_test_5,32,254)\n",
    "print(f\"Test results with best hyperparameters: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "79e55892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type        | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | conv1 | Conv2d      | 320    | train\n",
      "1 | conv2 | Conv2d      | 18.5 K | train\n",
      "2 | conv3 | Conv2d      | 73.9 K | train\n",
      "3 | pool  | MaxPool2d   | 0      | train\n",
      "4 | fc1   | Linear      | 16.8 M | train\n",
      "5 | fc2   | Linear      | 130 K  | train\n",
      "6 | bn1   | BatchNorm2d | 64     | train\n",
      "7 | bn2   | BatchNorm2d | 128    | train\n",
      "8 | bn3   | BatchNorm2d | 256    | train\n",
      "9 | relu  | ReLU        | 0      | train\n",
      "----------------------------------------------\n",
      "17.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "17.0 M    Total params\n",
      "68.005    Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmorin/anaconda3/envs/Ethan_MLB_final/lib/python3.14/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: PossibleUserWarning:\n",
      "\n",
      "The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmorin/anaconda3/envs/Ethan_MLB_final/lib/python3.14/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: PossibleUserWarning:\n",
      "\n",
      "The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149: 100%|| 254/254 [00:17<00:00, 14.71it/s, v_num=9, train_loss=4.450, val_loss=4.470, val_acc=0.225, val_f1=0.139]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=150` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149: 100%|| 254/254 [00:20<00:00, 12.60it/s, v_num=9, train_loss=4.450, val_loss=4.470, val_acc=0.225, val_f1=0.139]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/kmorin/anaconda3/envs/Ethan_MLB_final/lib/python3.14/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: PossibleUserWarning:\n",
      "\n",
      "The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|| 80/80 [00:02<00:00, 35.18it/s]\n",
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "        test_acc            0.2287401556968689\n",
      "         test_f1            0.1434086114168167\n",
      "        test_loss           4.4602813720703125\n",
      "\n",
      "Test results with best hyperparameters: [{'test_loss': 4.4602813720703125, 'test_acc': 0.2287401556968689, 'test_f1': 0.1434086114168167}]\n"
     ]
    }
   ],
   "source": [
    "# Best trial:\n",
    "#   Value: 5.097236633300781\n",
    "#   Params: \n",
    "#     learning_rate: 2.7006153026513106e-06\n",
    "# Visualize the results\n",
    "try:\n",
    "    # Plot optimization history\n",
    "    optuna.visualization.plot_optimization_history(study)\n",
    "    \n",
    "    # Plot parameter importances\n",
    "    optuna.visualization.plot_param_importances(study)\n",
    "    \n",
    "    # Plot parallel coordinate plot\n",
    "    optuna.visualization.plot_parallel_coordinate(study)\n",
    "except ImportError:\n",
    "    print(\"Visualization requires plotly. Install with: pip install plotly\")\n",
    "  \n",
    "  # Test the best model\n",
    "results = test_best_model(study,X_train_7,X_val_7,X_test_7,y_train_7,y_val_7,y_test_7,128,254)\n",
    "print(f\"Test results with best hyperparameters: {results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ethan_MLB_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
