{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16e43eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import optim\n",
    "from torchmetrics import F1Score\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8b2d01",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8529b76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scicomp/scratch/rqu4/tmp/ipykernel_738662/1929073556.py:21: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  sample_id = row[0]\n",
      "/scicomp/scratch/rqu4/tmp/ipykernel_738662/1929073556.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label = row[1]\n"
     ]
    }
   ],
   "source": [
    "# --- Load data ---\n",
    "mlst_train_df = pd.read_csv('../assets/training_set_mlst.csv')\n",
    "mlst_test_df = pd.read_csv('../assets/test_set_mlst.csv')\n",
    "mlst_val_df = pd.read_csv('../assets/validation_set_mlst.csv')\n",
    "\n",
    "serotype_train_df = pd.read_csv('../assets/training_set_serotype.csv')\n",
    "serotype_test_df = pd.read_csv('../assets/test_set_serotype.csv')\n",
    "serotype_val_df = pd.read_csv('../assets/validation_set_serotype.csv')\n",
    "\n",
    "subspecies_train_df = pd.read_csv('../assets/training_set_subspecies.csv')\n",
    "subspecies_test_df = pd.read_csv('../assets/test_set_subspecies.csv')\n",
    "subspecies_val_df = pd.read_csv('../assets/validation_set_subspecies.csv')\n",
    "\n",
    "kmc5_arrays = os.path.expanduser('~/PROJECTS/GaTech/FCGR_classifier/salmonella_kmc5_arrays/')\n",
    "kmc7_arrays = os.path.expanduser('~/PROJECTS/GaTech/FCGR_classifier/salmonella_kmc7_arrays/')\n",
    "\n",
    "def load_kmer_arrays(df, array_dir, suffix):\n",
    "    arrays = []\n",
    "    labels = []\n",
    "    for idx, row in df.iterrows():\n",
    "        sample_id = row[0]\n",
    "        label = row[1]\n",
    "        array_path = os.path.join(array_dir, f\"{sample_id}{suffix}.npy\")\n",
    "        if os.path.exists(array_path):\n",
    "            array = np.load(array_path).flatten()\n",
    "            arrays.append(array)\n",
    "            labels.append(label)\n",
    "        else:\n",
    "            print(f\"Warning: Array file {array_path} not found.\")\n",
    "    return np.array(arrays), np.array(labels)\n",
    "\n",
    "# MLST\n",
    "X_train_mlst_5, y_train_mlst_5 = load_kmer_arrays(mlst_train_df, kmc5_arrays, '_k5_k5')\n",
    "X_val_mlst_5, y_val_mlst_5 = load_kmer_arrays(mlst_val_df, kmc5_arrays, '_k5_k5')\n",
    "X_test_mlst_5, y_test_mlst_5 = load_kmer_arrays(mlst_test_df, kmc5_arrays, '_k5_k5')\n",
    "\n",
    "X_train_mlst_7, y_train_mlst_7 = load_kmer_arrays(mlst_train_df, kmc7_arrays, '_k7_k7')\n",
    "X_val_mlst_7, y_val_mlst_7 = load_kmer_arrays(mlst_val_df, kmc7_arrays, '_k7_k7')\n",
    "X_test_mlst_7, y_test_mlst_7 = load_kmer_arrays(mlst_test_df, kmc7_arrays, '_k7_k7')\n",
    "\n",
    "# Serotype\n",
    "X_train_sero_5, y_train_sero_5 = load_kmer_arrays(serotype_train_df, kmc5_arrays, '_k5_k5')\n",
    "X_val_sero_5, y_val_sero_5 = load_kmer_arrays(serotype_val_df, kmc5_arrays, '_k5_k5')\n",
    "X_test_sero_5, y_test_sero_5 = load_kmer_arrays(serotype_test_df, kmc5_arrays, '_k5_k5')\n",
    "\n",
    "X_train_sero_7, y_train_sero_7 = load_kmer_arrays(serotype_train_df, kmc7_arrays, '_k7_k7')\n",
    "X_val_sero_7, y_val_sero_7 = load_kmer_arrays(serotype_val_df, kmc7_arrays, '_k7_k7')\n",
    "X_test_sero_7, y_test_sero_7 = load_kmer_arrays(serotype_test_df, kmc7_arrays, '_k7_k7')\n",
    "\n",
    "# Subspecies\n",
    "X_train_sub_5, y_train_sub_5 = load_kmer_arrays(subspecies_train_df, kmc5_arrays, '_k5_k5')\n",
    "X_val_sub_5, y_val_sub_5 = load_kmer_arrays(subspecies_val_df, kmc5_arrays, '_k5_k5')\n",
    "X_test_sub_5, y_test_sub_5 = load_kmer_arrays(subspecies_test_df, kmc5_arrays, '_k5_k5')\n",
    "\n",
    "X_train_sub_7, y_train_sub_7 = load_kmer_arrays(subspecies_train_df, kmc7_arrays, '_k7_k7')\n",
    "X_val_sub_7, y_val_sub_7 = load_kmer_arrays(subspecies_val_df, kmc7_arrays, '_k7_k7')\n",
    "X_test_sub_7, y_test_sub_7 = load_kmer_arrays(subspecies_test_df, kmc7_arrays, '_k7_k7')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1017da2d",
   "metadata": {},
   "source": [
    "# Performance metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, y):\n",
    "    return (preds == y).float().mean()\n",
    "\n",
    "def F1(out_dim,preds,y):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Initialize F1Score for binary classification\n",
    "    f1 = F1Score(task=\"multiclass\", num_classes=out_dim, average='macro').to(device)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    return f1(preds, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de155dca",
   "metadata": {},
   "source": [
    "# Panspace CNN model (encoder portion of their model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df6f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PanspaceCNN(pl.LightningModule):\n",
    "    def __init__(self, input_size, out_dim, learning_rate):\n",
    "        \"\"\"\n",
    "        Initializes the PyTorch model for Panspace.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the square CGR image (e.g., 64 for k=6).\n",
    "            out_dim (int): The size of the output vector.\n",
    "        \"\"\"\n",
    "        super(PanspaceCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        # The number of filters and kernel sizes are example values and can be tuned.\n",
    "        self.save_hyperparameters()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        # Max pooling layers\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        # Calculate the size of the flattened tensor after convolutions and pooling.\n",
    "        # This assumes a 1xinput_size x input_size input image.\n",
    "        final_conv_size = input_size // (2**3)  # After 3 pooling layers\n",
    "        self.fc1 = nn.Linear(128 * final_conv_size * final_conv_size, 512)\n",
    "        self.fc2 = nn.Linear(512, out_dim)\n",
    "\n",
    "        # Batch normalization and activation functions\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): A tensor representing the CGR image of shape (batch_size, 1, H, W).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor with predicted probabilities for each class of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # Apply convolutions, batch normalization, and pooling\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        # Flatten the tensor\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Apply fully connected layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, torch.argmax(y,dim=1))\n",
    "        f1 = F1(self.hparams.out_dim,preds,torch.argmax(y,dim=1))\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        self.log('val_f1', f1, prog_bar=True)\n",
    "        return {'val_loss': loss, 'val_acc': acc, 'val_f1': f1}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, torch.argmax(y,dim=1))\n",
    "        f1 = F1(self.hparams.out_dim,preds,torch.argmax(y,dim=1))\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        self.log('test_f1', f1, prog_bar=True)\n",
    "        return {'test_loss': loss, 'test_acc': acc, 'test_f1': f1}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6467a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# scale classes for one-hot encoding\n",
    "# ----------------------------------\n",
    "\n",
    "def scale(y,classes):\n",
    "    y_scaled = []\n",
    "    for label in y:\n",
    "        index = np.where(classes == label)\n",
    "        y_scaled.append(index[0][0])\n",
    "    y_scale = np.array(y_scaled)\n",
    "    return y_scale\n",
    "\n",
    "def prepare_data(X_train,X_val,X_test,y_train,y_val,y_test,size=32,out_dim=254):\n",
    "    y_train = y_train.astype(int)\n",
    "    y_val = y_val.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    classes = np.sort(np.unique(y_train))\n",
    "\n",
    "    y_train_scale = scale(y_train,classes)\n",
    "    y_val_scale = scale(y_val,classes)\n",
    "    y_test_scale = scale(y_test,classes)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Convert data to torch tensors\n",
    "    # -----------------------------\n",
    "    X_train_tensor = torch.tensor(X_train.reshape(-1,size,size)).unsqueeze(1).float()  # (N, 1, size, size)\n",
    "    X_val_tensor = torch.tensor(X_val.reshape(-1,size,size)).unsqueeze(1).float()\n",
    "    X_test_tensor = torch.tensor(X_test.reshape(-1,size,size)).unsqueeze(1).float()\n",
    "    y_train_tensor = torch.tensor(y_train_scale)\n",
    "    y_val_tensor = torch.tensor(y_val_scale)\n",
    "    y_test_tensor = torch.tensor(y_test_scale)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Encode labels to categorical\n",
    "    # -----------------------------\n",
    "    y_train_enc = F.one_hot(y_train_tensor, num_classes = out_dim).float()\n",
    "    y_val_enc = F.one_hot(y_val_tensor, num_classes = out_dim).float()\n",
    "    y_test_enc = F.one_hot(y_test_tensor, num_classes= out_dim).float()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Create PyTorch Datasets and Loaders\n",
    "    # -----------------------------\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_enc)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_enc)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_enc)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "    return train_loader,val_loader,test_loader\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Set the hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-7, 1e-2, log=True)\n",
    "    \n",
    "    # Create the model with trial hyperparameters\n",
    "    model = PanspaceCNN(\n",
    "        input_size=128,\n",
    "        out_dim=254,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        verbose=False,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    # Optuna pruning callback\n",
    "    pruning_callback = PyTorchLightningPruningCallback(trial, monitor='val_loss')\n",
    "    \n",
    "    # Logger\n",
    "    logger = TensorBoardLogger(save_dir=os.getcwd(), name=f\"optuna_logs/trial_{trial.number}\")\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        callbacks=[early_stop_callback, pruning_callback],\n",
    "        logger=logger,\n",
    "        enable_progress_bar=False,  \n",
    "        enable_model_summary=False  \n",
    "    )\n",
    "    \n",
    "    # Preparing the data\n",
    "    train_loader, val_loader, test_loader = prepare_data(X_train_7,X_val_7,X_test_7,y_train_7,y_val_7,y_test_7,128,254)\n",
    "    \n",
    "    # Training the model\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    # Final validation loss\n",
    "    return trainer.callback_metrics['val_loss'].item()\n",
    "\n",
    "def run_optimization(n_trials=50):\n",
    "    pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    "    study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"  Value: {trial.value}\")\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "        \n",
    "    return study\n",
    "\n",
    "def test_best_model(study,X_train,X_val,X_test,y_train,y_val,y_test,size,num_classes):\n",
    "    # Getting the best hyperparameters\n",
    "    best_params = study.best_trial.params\n",
    "    \n",
    "    # Creating the model with the best hyperparameters\n",
    "    model = PanspaceCNN(\n",
    "        input_size=size,\n",
    "        out_dim=num_classes,\n",
    "        learning_rate=best_params['learning_rate']\n",
    "    )\n",
    "    \n",
    "    # Creating trainer instance\n",
    "    trainer = pl.Trainer(max_epochs=150)\n",
    "    \n",
    "    # Preparing the data\n",
    "    train_loader, val_loader, test_loader = prepare_data(X_train,X_val,X_test,y_train,y_val,y_test,size,num_classes)\n",
    "    \n",
    "    # Training the model with the best hyperparameters\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    # Testing the model with the test data\n",
    "    results = trainer.test(model, dataloaders=test_loader)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556d31d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
