{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a6a6f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!module load cuda/11.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e9f2742-d63e-41c0-911e-542d863d91a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 21:10:19.188557: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-06-25 21:10:19.188669: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-06-25 21:10:19.350748: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-25 21:10:23.473891: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-06-25 21:10:23.475042: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-06-25 21:10:23.475095: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Import functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acd79d09-f95c-4cf5-9fe8-b389f73b84b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accession</th>\n",
       "      <th>organism_organism_name</th>\n",
       "      <th>genus</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GCF_001461805.1</td>\n",
       "      <td>Enterobacter_lignolyticus</td>\n",
       "      <td>Enterobacter</td>\n",
       "      <td>lignolyticus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GCA_001461805.1</td>\n",
       "      <td>Enterobacter_lignolyticus</td>\n",
       "      <td>Enterobacter</td>\n",
       "      <td>lignolyticus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GCF_000164865.1</td>\n",
       "      <td>Enterobacter_lignolyticus_SCF1</td>\n",
       "      <td>Enterobacter</td>\n",
       "      <td>lignolyticus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GCA_000164865.1</td>\n",
       "      <td>Enterobacter_lignolyticus_SCF1</td>\n",
       "      <td>Enterobacter</td>\n",
       "      <td>lignolyticus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GCF_001856865.2</td>\n",
       "      <td>Kluyvera_intestini</td>\n",
       "      <td>Kluyvera</td>\n",
       "      <td>intestini</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accession          organism_organism_name         genus       species\n",
       "0  GCF_001461805.1       Enterobacter_lignolyticus  Enterobacter  lignolyticus\n",
       "1  GCA_001461805.1       Enterobacter_lignolyticus  Enterobacter  lignolyticus\n",
       "2  GCF_000164865.1  Enterobacter_lignolyticus_SCF1  Enterobacter  lignolyticus\n",
       "3  GCA_000164865.1  Enterobacter_lignolyticus_SCF1  Enterobacter  lignolyticus\n",
       "4  GCF_001856865.2              Kluyvera_intestini      Kluyvera     intestini"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Genus/species labels\n",
    "taxon_labels = '/scicomp/home-pure/rqu4/PROJECTS/GaTech/FCGR_classifier/data_final/enterobacteriaceae/metadata/metadata_tax_assignments.csv'\n",
    "\n",
    "df_tax = pd.read_csv(taxon_labels, sep=',')\n",
    "\n",
    "df_tax['genus'] = df_tax['organism_organism_name'].str.split('_').str[0]\n",
    "df_tax['species'] = df_tax['organism_organism_name'].str.split('_').str[1]\n",
    "df_tax['species'] = df_tax['species'].replace(\"sp.\", np.nan)\n",
    "df_tax.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cae2e770-fb2d-4796-acc6-49d0ec0d13af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GCF_000005845.2_ASM584v2_genomic_k5_k5.npy', 'GCF_000006925.2_ASM692v2_genomic_k5_k5.npy', 'GCF_000008105.1_ASM810v1_genomic_k5_k5.npy', 'GCF_000007445.1_ASM744v1_genomic_k5_k5.npy', 'GCF_000007405.1_ASM740v1_genomic_k5_k5.npy']\n"
     ]
    }
   ],
   "source": [
    "# FCGR_arrays\n",
    "FCGR_array_dir = '/scicomp/home-pure/rqu4/PROJECTS/GaTech/FCGR_classifier/data_final/enterobacteriaceae/data/FCGR_arrays'\n",
    "\n",
    "contents = os.listdir(FCGR_array_dir)\n",
    "\n",
    "print(contents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e710da27-f771-4c14-9fcd-0dbf860ebdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32)\n",
      "[[ 2975.  3687.  6749. ... 10000. 10000. 10000.]\n",
      " [ 5661.     0.  3735. ... 10000. 10000. 10000.]\n",
      " [10000.  6415.  3771. ... 10000.  4935. 10000.]\n",
      " ...\n",
      " [    0.     0.     0. ...     0.     0.     0.]\n",
      " [    0.  3669.     0. ...     0.     0.     0.]\n",
      " [    0.     0.     0. ...     0.     0.     0.]]\n"
     ]
    }
   ],
   "source": [
    "# # Example FCGR array\n",
    "\n",
    "e_FCGR = FCGR_array_dir + '/' + 'GCF_000005845.2_ASM584v2_genomic_k5_k5.npy'\n",
    "\n",
    "array = np.load(e_FCGR)\n",
    "\n",
    "print(array.shape)\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a5ead57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original df_tax shape: (869903, 4)\n",
      "Original unique species count: 170\n",
      "Original species value counts (top 10):\n",
      "species\n",
      "enterica        390912\n",
      "coli            387273\n",
      "sonnei           25211\n",
      "flexneri         21970\n",
      "hormaechei       14640\n",
      "freundii          4888\n",
      "sakazakii         3068\n",
      "cloacae           2620\n",
      "asburiae          1444\n",
      "roggenkampii      1408\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Filtered df_tax_filtered shape (min 12 reps per species): (864748, 4)\n",
      "Unique species count in df_tax_filtered: 72\n",
      "Filtered species value counts (top 10):\n",
      "species\n",
      "enterica        390912\n",
      "coli            387273\n",
      "sonnei           25211\n",
      "flexneri         21970\n",
      "hormaechei       14640\n",
      "freundii          4888\n",
      "sakazakii         3068\n",
      "cloacae           2620\n",
      "asburiae          1444\n",
      "roggenkampii      1408\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Starting FCGR array loading and matching with filtered species...\n",
      "\n",
      "Successfully processed 6406 FCGR arrays.\n",
      "Skipped 4528 FCGR arrays (either no tax data in filtered set, or wrong shape).\n",
      "\n",
      "WARNING: Classes with only 1 member found after initial filtering. These will be removed for stratification.\n",
      "  - Class 37 (muytjensii)\n",
      "  - Class 29 (huaxiensis)\n",
      "  - Class 40 (oligotrophicus)\n",
      "  - Class 45 (regensburgei)\n",
      "  - Class 49 (sedlakii)\n",
      "  - Class 53 (subterraneus)\n",
      "  - Class 39 (odontotermitis)\n",
      "  - Class 58 (wuhouensis)\n",
      "  - Class 22 (europaeus)\n",
      "  - Class 26 (gillenii)\n",
      "  - Class 56 (vonholyi)\n",
      "  - Class 54 (turicensis)\n",
      "  - Class 17 (davisae)\n",
      "  - Class 30 (intestinihominis)\n",
      "  - Class 4 (Williamhamiltonella)\n",
      "  - Class 2 (Regiella)\n",
      "Adjusted data shape for splitting: X=(6390, 1024), y=(6390, 44)\n",
      "Number of classes after final adjustment: 44\n",
      "\n",
      "Shape of input features for training: (5112, 1024)\n",
      "Shape of target labels for training: (5112, 44)\n",
      "Number of samples for training: 5112\n",
      "Number of samples for testing: 1278\n",
      "Final number of unique species classes used: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 21:12:47.812877: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-06-25 21:12:47.814083: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2025-06-25 21:12:47.815028: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2025-06-25 21:12:47.815945: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2025-06-25 21:12:47.816788: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2025-06-25 21:12:47.817656: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2025-06-25 21:12:47.818598: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2025-06-25 21:12:47.819447: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2025-06-25 21:12:47.819504: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Summary ---\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 44)                2860      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 306,412\n",
      "Trainable params: 306,412\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--- Training Model ---\n",
      "Epoch 1/50\n",
      "144/144 [==============================] - 2s 8ms/step - loss: 901.0341 - accuracy: 0.5139 - val_loss: 262.6064 - val_accuracy: 0.6328\n",
      "Epoch 2/50\n",
      "144/144 [==============================] - 1s 5ms/step - loss: 37.1292 - accuracy: 0.4150 - val_loss: 3.6513 - val_accuracy: 0.6113\n",
      "Epoch 3/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 3.5548 - accuracy: 0.5980 - val_loss: 3.4186 - val_accuracy: 0.6113\n",
      "Epoch 4/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 3.3429 - accuracy: 0.5980 - val_loss: 3.2358 - val_accuracy: 0.6113\n",
      "Epoch 5/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 3.1595 - accuracy: 0.5980 - val_loss: 3.0635 - val_accuracy: 0.6113\n",
      "Epoch 6/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 2.9835 - accuracy: 0.5983 - val_loss: 2.9012 - val_accuracy: 0.6113\n",
      "Epoch 7/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 2.8240 - accuracy: 0.5983 - val_loss: 2.7482 - val_accuracy: 0.6113\n",
      "Epoch 8/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 2.6758 - accuracy: 0.5983 - val_loss: 2.6055 - val_accuracy: 0.6113\n",
      "Epoch 9/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 2.5391 - accuracy: 0.5980 - val_loss: 2.4740 - val_accuracy: 0.6113\n",
      "Epoch 10/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 2.4128 - accuracy: 0.5980 - val_loss: 2.3534 - val_accuracy: 0.6113\n",
      "Epoch 11/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 2.2979 - accuracy: 0.5980 - val_loss: 2.2439 - val_accuracy: 0.6113\n",
      "Epoch 12/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 2.1945 - accuracy: 0.5980 - val_loss: 2.1466 - val_accuracy: 0.6113\n",
      "Epoch 13/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 2.1025 - accuracy: 0.5980 - val_loss: 2.0599 - val_accuracy: 0.6113\n",
      "Epoch 14/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 2.0214 - accuracy: 0.5980 - val_loss: 1.9842 - val_accuracy: 0.6113\n",
      "Epoch 15/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.9506 - accuracy: 0.5980 - val_loss: 1.9181 - val_accuracy: 0.6113\n",
      "Epoch 16/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.8894 - accuracy: 0.5980 - val_loss: 1.8610 - val_accuracy: 0.6113\n",
      "Epoch 17/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.8368 - accuracy: 0.5980 - val_loss: 1.8121 - val_accuracy: 0.6113\n",
      "Epoch 18/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.7920 - accuracy: 0.5980 - val_loss: 1.7708 - val_accuracy: 0.6113\n",
      "Epoch 19/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.7541 - accuracy: 0.5980 - val_loss: 1.7359 - val_accuracy: 0.6113\n",
      "Epoch 20/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.7222 - accuracy: 0.5980 - val_loss: 1.7066 - val_accuracy: 0.6113\n",
      "Epoch 21/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.6956 - accuracy: 0.5980 - val_loss: 1.6820 - val_accuracy: 0.6113\n",
      "Epoch 22/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.6733 - accuracy: 0.5980 - val_loss: 1.6618 - val_accuracy: 0.6113\n",
      "Epoch 23/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.6548 - accuracy: 0.5980 - val_loss: 1.6449 - val_accuracy: 0.6113\n",
      "Epoch 24/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.6394 - accuracy: 0.5980 - val_loss: 1.6310 - val_accuracy: 0.6113\n",
      "Epoch 25/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.6265 - accuracy: 0.5980 - val_loss: 1.6193 - val_accuracy: 0.6113\n",
      "Epoch 26/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.6158 - accuracy: 0.5980 - val_loss: 1.6098 - val_accuracy: 0.6113\n",
      "Epoch 27/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.6068 - accuracy: 0.5980 - val_loss: 1.6018 - val_accuracy: 0.6113\n",
      "Epoch 28/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5993 - accuracy: 0.5980 - val_loss: 1.5952 - val_accuracy: 0.6113\n",
      "Epoch 29/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5930 - accuracy: 0.5980 - val_loss: 1.5897 - val_accuracy: 0.6113\n",
      "Epoch 30/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5876 - accuracy: 0.5980 - val_loss: 1.5851 - val_accuracy: 0.6113\n",
      "Epoch 31/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5831 - accuracy: 0.5980 - val_loss: 1.5812 - val_accuracy: 0.6113\n",
      "Epoch 32/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5792 - accuracy: 0.5980 - val_loss: 1.5779 - val_accuracy: 0.6113\n",
      "Epoch 33/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5759 - accuracy: 0.5980 - val_loss: 1.5751 - val_accuracy: 0.6113\n",
      "Epoch 34/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5730 - accuracy: 0.5980 - val_loss: 1.5729 - val_accuracy: 0.6113\n",
      "Epoch 35/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5705 - accuracy: 0.5980 - val_loss: 1.5709 - val_accuracy: 0.6113\n",
      "Epoch 36/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5683 - accuracy: 0.5980 - val_loss: 1.5692 - val_accuracy: 0.6113\n",
      "Epoch 37/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5664 - accuracy: 0.5980 - val_loss: 1.5678 - val_accuracy: 0.6113\n",
      "Epoch 38/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5647 - accuracy: 0.5980 - val_loss: 1.5666 - val_accuracy: 0.6113\n",
      "Epoch 39/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5632 - accuracy: 0.5980 - val_loss: 1.5656 - val_accuracy: 0.6113\n",
      "Epoch 40/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5618 - accuracy: 0.5980 - val_loss: 1.5645 - val_accuracy: 0.6113\n",
      "Epoch 41/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5607 - accuracy: 0.5980 - val_loss: 1.5638 - val_accuracy: 0.6113\n",
      "Epoch 42/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5596 - accuracy: 0.5980 - val_loss: 1.5631 - val_accuracy: 0.6113\n",
      "Epoch 43/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5586 - accuracy: 0.5980 - val_loss: 1.5625 - val_accuracy: 0.6113\n",
      "Epoch 44/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5578 - accuracy: 0.5980 - val_loss: 1.5620 - val_accuracy: 0.6113\n",
      "Epoch 45/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5570 - accuracy: 0.5980 - val_loss: 1.5616 - val_accuracy: 0.6113\n",
      "Epoch 46/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5563 - accuracy: 0.5980 - val_loss: 1.5613 - val_accuracy: 0.6113\n",
      "Epoch 47/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5556 - accuracy: 0.5980 - val_loss: 1.5609 - val_accuracy: 0.6113\n",
      "Epoch 48/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5551 - accuracy: 0.5980 - val_loss: 1.5605 - val_accuracy: 0.6113\n",
      "Epoch 49/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5545 - accuracy: 0.5980 - val_loss: 1.5603 - val_accuracy: 0.6113\n",
      "Epoch 50/50\n",
      "144/144 [==============================] - 1s 6ms/step - loss: 1.5540 - accuracy: 0.5980 - val_loss: 1.5601 - val_accuracy: 0.6113\n",
      "\n",
      "--- Evaluating Model ---\n",
      "Test Loss: 1.5535\n",
      "Test Accuracy: 0.5994\n",
      "\n",
      "--- Sample Predictions ---\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "Sample 1: True Species: chuandaensis, Predicted Species: chuandaensis\n",
      "Sample 2: True Species: chuandaensis, Predicted Species: chuandaensis\n",
      "Sample 3: True Species: chuandaensis, Predicted Species: chuandaensis\n",
      "Sample 4: True Species: dublinensis, Predicted Species: chuandaensis\n",
      "Sample 5: True Species: chuandaensis, Predicted Species: chuandaensis\n",
      "Sample 6: True Species: chuandaensis, Predicted Species: chuandaensis\n",
      "Sample 7: True Species: chuandaensis, Predicted Species: chuandaensis\n",
      "Sample 8: True Species: dublinensis, Predicted Species: chuandaensis\n",
      "Sample 9: True Species: chuandaensis, Predicted Species: chuandaensis\n",
      "Sample 10: True Species: chuandaensis, Predicted Species: chuandaensis\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "FCGR_array_dir = '/scicomp/home-pure/rqu4/PROJECTS/GaTech/FCGR_classifier/data_final/enterobacteriaceae/data/FCGR_arrays' # Your actual path\n",
    "MIN_REPRESENTATIVES = 12\n",
    "SPECIES_COLUMN = 'species' # Ensure this matches your actual species column name in df_tax\n",
    "ACCESSION_COLUMN = 'accession' # Ensure this matches your actual accession column name in df_tax\n",
    "\n",
    "# --- Assumed: df_tax is already loaded here ---\n",
    "# Example: df_tax = pd.read_csv('your_taxonomic_data.csv')\n",
    "\n",
    "print(f\"Original df_tax shape: {df_tax.shape}\")\n",
    "print(f\"Original unique species count: {df_tax[SPECIES_COLUMN].nunique()}\")\n",
    "print(f\"Original species value counts (top 10):\\n{df_tax[SPECIES_COLUMN].value_counts().head(10)}\")\n",
    "\n",
    "# --- Filter df_tax for species with at least MIN_REPRESENTATIVES ---\n",
    "species_counts = df_tax[SPECIES_COLUMN].value_counts()\n",
    "species_to_keep = species_counts[species_counts >= MIN_REPRESENTATIVES].index.tolist()\n",
    "df_tax_filtered = df_tax[df_tax[SPECIES_COLUMN].isin(species_to_keep)].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "print(f\"\\nFiltered df_tax_filtered shape (min {MIN_REPRESENTATIVES} reps per species): {df_tax_filtered.shape}\")\n",
    "print(f\"Unique species count in df_tax_filtered: {df_tax_filtered[SPECIES_COLUMN].nunique()}\")\n",
    "print(f\"Filtered species value counts (top 10):\\n{df_tax_filtered[SPECIES_COLUMN].value_counts().head(10)}\")\n",
    "\n",
    "# --- FCGR Data Loading (modified to use df_tax_filtered) ---\n",
    "fcgr_data = []\n",
    "species_labels = []\n",
    "\n",
    "fcgr_files = [f for f in os.listdir(FCGR_array_dir) if f.endswith('.npy')]\n",
    "\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "print(\"\\nStarting FCGR array loading and matching with filtered species...\")\n",
    "# Create a set of accessions and a dictionary for faster lookup\n",
    "filtered_accessions_set = set(df_tax_filtered[ACCESSION_COLUMN].tolist())\n",
    "species_lookup = df_tax_filtered.set_index(ACCESSION_COLUMN)[SPECIES_COLUMN].to_dict()\n",
    "\n",
    "for file_name in fcgr_files:\n",
    "    # Extract the accession from the filename, e.g., 'GCF_000005845.2'\n",
    "    file_accession = '_'.join(file_name.split('_')[:2])\n",
    "\n",
    "    # Check if the accession is in our filtered set\n",
    "    if file_accession in filtered_accessions_set:\n",
    "        species = species_lookup.get(file_accession)\n",
    "        \n",
    "        # This check is mostly for robustness, should not be None if filtered_accessions_set is used correctly\n",
    "        if species is None:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        array_path = os.path.join(FCGR_array_dir, file_name)\n",
    "\n",
    "        try:\n",
    "            fcgr_array = np.load(array_path)\n",
    "            \n",
    "            if fcgr_array.shape == (32, 32):\n",
    "                fcgr_data.append(fcgr_array)\n",
    "                species_labels.append(species)\n",
    "                processed_count += 1\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing {file_name}: {e}\")\n",
    "            skipped_count += 1\n",
    "    else:\n",
    "        # Accession not in filtered tax data, so skip\n",
    "        skipped_count += 1\n",
    "\n",
    "print(f\"\\nSuccessfully processed {processed_count} FCGR arrays.\")\n",
    "print(f\"Skipped {skipped_count} FCGR arrays (either no tax data in filtered set, or wrong shape).\")\n",
    "\n",
    "if not fcgr_data:\n",
    "    raise ValueError(\n",
    "        \"No FCGR data loaded after processing. \"\n",
    "        \"Please check your FCGR_array_dir, df_tax loading, accession matching logic, \"\n",
    "        \"and the species filtering threshold. No data matched the criteria.\"\n",
    "    )\n",
    "\n",
    "X = np.array(fcgr_data)\n",
    "y = np.array(species_labels)\n",
    "\n",
    "# --- Preprocess Labels and Features ---\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "# num_classes = len(label_encoder.classes_) # This might be your issue if used directly in model\n",
    "\n",
    "y_one_hot = to_categorical(y_encoded, num_classes=len(label_encoder.classes_)) # Use len(label_encoder.classes_) here for original one-hot\n",
    "X_flattened = X.reshape(X.shape[0], -1)\n",
    "\n",
    "# Check for problematic classes (only 1 member) before splitting for stratify\n",
    "class_counts = Counter(y_encoded)\n",
    "single_member_classes = [cls for cls, count in class_counts.items() if count == 1]\n",
    "\n",
    "if single_member_classes:\n",
    "    print(\"\\nWARNING: Classes with only 1 member found after initial filtering. These will be removed for stratification.\")\n",
    "    for cls in single_member_classes:\n",
    "        original_label = label_encoder.inverse_transform([cls])[0]\n",
    "        print(f\"  - Class {cls} ({original_label})\")\n",
    "\n",
    "    indices_to_keep = []\n",
    "    for i, cls in enumerate(y_encoded):\n",
    "        if cls not in single_member_classes:\n",
    "            indices_to_keep.append(i)\n",
    "\n",
    "    X_to_split = X_flattened[indices_to_keep]\n",
    "    y_encoded_to_split = y_encoded[indices_to_keep]\n",
    "    y_one_hot_to_split = y_one_hot[indices_to_keep]\n",
    "\n",
    "    # --- THIS IS THE CRITICAL PART ---\n",
    "    # After filtering, re-encode y_encoded_to_split to get new, contiguous numerical labels\n",
    "    # and accurately count the NEW number of classes.\n",
    "    final_label_encoder = LabelEncoder() # Create a NEW LabelEncoder for the filtered data\n",
    "    y_encoded_to_split_reencoded = final_label_encoder.fit_transform(y_encoded_to_split)\n",
    "    num_classes_final = len(final_label_encoder.classes_) # This is the correct number of classes\n",
    "    \n",
    "    # And re-one-hot encode with the new number of classes\n",
    "    y_one_hot_to_split = to_categorical(y_encoded_to_split_reencoded, num_classes=num_classes_final)\n",
    "\n",
    "    # Update the data to be used for splitting\n",
    "    y_encoded_to_split = y_encoded_to_split_reencoded # Use the re-encoded y for stratify\n",
    "\n",
    "    print(f\"Adjusted data shape for splitting: X={X_to_split.shape}, y={y_one_hot_to_split.shape}\")\n",
    "    print(f\"Number of classes after final adjustment: {num_classes_final}\") # Use this for model output\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo single-member classes found. Proceeding with split.\")\n",
    "    X_to_split = X_flattened\n",
    "    y_one_hot_to_split = y_one_hot\n",
    "    y_encoded_to_split = y_encoded\n",
    "    num_classes_final = num_classes # This was set earlier from original label_encoder.classes_\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_to_split, y_one_hot_to_split, test_size=0.2, random_state=42, stratify=y_encoded_to_split\n",
    ")\n",
    "\n",
    "print(f\"\\nShape of input features for training: {X_train.shape}\")\n",
    "print(f\"Shape of target labels for training: {y_train.shape}\")\n",
    "print(f\"Number of samples for training: {X_train.shape[0]}\")\n",
    "print(f\"Number of samples for testing: {X_test.shape[0]}\")\n",
    "print(f\"Final number of unique species classes used: {num_classes_final}\")\n",
    "\n",
    "\n",
    "# --- Build the MLP Model ---\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes_final, activation='softmax') # Use the CORRECTED number of classes here\n",
    "])\n",
    "\n",
    "# --- Train the Model ---\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"\\n--- Model Summary ---\")\n",
    "model.summary()\n",
    "\n",
    "print(\"\\n--- Training Model ---\")\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# --- Evaluate the Model ---\n",
    "print(\"\\n--- Evaluating Model ---\")\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# --- Optional: Make predictions on test data ---\n",
    "print(\"\\n--- Sample Predictions ---\")\n",
    "predictions = model.predict(X_test)\n",
    "predicted_species_indices = np.argmax(predictions, axis=1)\n",
    "true_species_indices = np.argmax(y_test, axis=1)\n",
    "\n",
    "for i in range(min(10, len(X_test))):\n",
    "    # Map back to original labels using the *original* label_encoder\n",
    "    predicted_species = label_encoder.inverse_transform([predicted_species_indices[i]])[0]\n",
    "    true_species = label_encoder.inverse_transform([true_species_indices[i]])[0]\n",
    "    print(f\"Sample {i+1}: True Species: {true_species}, Predicted Species: {predicted_species}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2ad01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "640dbd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] XGBoost Training Samples: 5112, Test Samples: 1278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scicomp/home-pure/rqu4/cache/python/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [21:43:47] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=100,\n",
       "              n_jobs=-1, num_class=44, num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=100,\n",
       "              n_jobs=-1, num_class=44, num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='mlogloss',\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=100,\n",
       "              n_jobs=-1, num_class=44, num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# --- Train/Test Split for XGBoost ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_to_split, y_encoded_to_split, test_size=0.2, random_state=42, stratify=y_encoded_to_split\n",
    ")\n",
    "\n",
    "print(f\"\\n[INFO] XGBoost Training Samples: {X_train.shape[0]}, Test Samples: {X_test.shape[0]}\")\n",
    "\n",
    "# --- Train XGBoost Classifier ---\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',         # or 'multi:softprob' if you want probabilities\n",
    "    num_class=num_classes_final,\n",
    "    eval_metric='mlogloss',\n",
    "    use_label_encoder=False,\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70f1fd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         1\n",
      "           5       1.00      1.00      1.00        15\n",
      "           6       1.00      0.67      0.80         3\n",
      "           7       0.85      1.00      0.92        11\n",
      "           8       1.00      0.33      0.50         3\n",
      "           9       1.00      0.88      0.93         8\n",
      "          10       1.00      1.00      1.00         4\n",
      "          11       1.00      1.00      1.00         1\n",
      "          13       0.00      0.00      0.00         1\n",
      "          14       1.00      0.73      0.84        11\n",
      "          15       0.98      1.00      0.99       766\n",
      "          16       1.00      1.00      1.00         1\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      0.33      0.50         6\n",
      "          20       1.00      1.00      1.00         4\n",
      "          21       1.00      1.00      1.00       232\n",
      "          23       0.00      0.00      0.00         1\n",
      "          24       0.73      0.48      0.58        23\n",
      "          25       0.88      1.00      0.94        38\n",
      "          27       1.00      1.00      1.00         1\n",
      "          28       0.99      1.00      0.99        80\n",
      "          31       0.86      1.00      0.92         6\n",
      "          32       0.86      1.00      0.92         6\n",
      "          33       0.70      0.88      0.78         8\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       1.00      1.00      1.00         1\n",
      "          36       1.00      1.00      1.00         2\n",
      "          38       1.00      1.00      1.00         1\n",
      "          42       1.00      0.56      0.71         9\n",
      "          43       0.00      0.00      0.00         1\n",
      "          46       1.00      1.00      1.00         1\n",
      "          47       0.92      1.00      0.96        11\n",
      "          48       1.00      1.00      1.00         5\n",
      "          50       0.00      0.00      0.00         1\n",
      "          51       0.00      0.00      0.00         0\n",
      "          52       0.91      1.00      0.95        10\n",
      "          57       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.97      1278\n",
      "   macro avg       0.83      0.79      0.80      1278\n",
      "weighted avg       0.97      0.97      0.97      1278\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scicomp/home-pure/rqu4/cache/python/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/scicomp/home-pure/rqu4/cache/python/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/scicomp/home-pure/rqu4/cache/python/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/scicomp/home-pure/rqu4/cache/python/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/scicomp/home-pure/rqu4/cache/python/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/scicomp/home-pure/rqu4/cache/python/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# Ensure labels are standard Python ints\n",
    "used_labels = [int(i) for i in unique_labels(y_test, y_pred)]\n",
    "\n",
    "# Ensure class names are properly extracted as strings\n",
    "target_names = [str(final_label_encoder.classes_[i]) for i in used_labels]\n",
    "\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    labels=used_labels,\n",
    "    target_names=target_names\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b71011c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample Predictions ---\n",
      "Sample 1: True Species: 15, Predicted Species: 15\n",
      "Sample 2: True Species: 15, Predicted Species: 15\n",
      "Sample 3: True Species: 15, Predicted Species: 15\n",
      "Sample 4: True Species: 21, Predicted Species: 21\n",
      "Sample 5: True Species: 15, Predicted Species: 15\n",
      "Sample 6: True Species: 15, Predicted Species: 15\n",
      "Sample 7: True Species: 15, Predicted Species: 15\n",
      "Sample 8: True Species: 21, Predicted Species: 21\n",
      "Sample 9: True Species: 15, Predicted Species: 15\n",
      "Sample 10: True Species: 15, Predicted Species: 15\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Sample Predictions ---\")\n",
    "for i in range(min(10, len(X_test))):\n",
    "    pred_label = final_label_encoder.inverse_transform([y_pred[i]])[0]\n",
    "    true_label = final_label_encoder.inverse_transform([y_test[i]])[0]\n",
    "    print(f\"Sample {i+1}: True Species: {true_label}, Predicted Species: {pred_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9459f89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Classes before filtering: 44\n",
      "[INFO] Classes after filtering (>=12 samples): 9\n",
      "[INFO] Samples after filtering: 5935\n",
      "\n",
      "[INFO] XGBoost Training Samples: 4748, Test Samples: 1187\n",
      "[INFO] Number of classes (filtered): 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scicomp/home-pure/rqu4/cache/python/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [22:08:37] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CLASSIFICATION REPORT]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       1.00      1.00      1.00        15\n",
      "           5       0.92      1.00      0.96        11\n",
      "          12       1.00      0.82      0.90        11\n",
      "          13       0.99      1.00      0.99       766\n",
      "          18       1.00      1.00      1.00       232\n",
      "          20       1.00      0.65      0.79        23\n",
      "          21       1.00      1.00      1.00        38\n",
      "          23       0.98      1.00      0.99        80\n",
      "          36       1.00      0.91      0.95        11\n",
      "\n",
      "    accuracy                           0.99      1187\n",
      "   macro avg       0.99      0.93      0.95      1187\n",
      "weighted avg       0.99      0.99      0.99      1187\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# --- Step 0: Filter classes with at least 12 samples ---\n",
    "\n",
    "# Assume:\n",
    "# - X_to_split: your full feature matrix, shape (N_samples, N_features)\n",
    "# - y_encoded_to_split: your original encoded labels (int or str)\n",
    "# Note: y_encoded_to_split can be original labels, but encoding below will handle mapping\n",
    "\n",
    "# Convert to numpy arrays if not already\n",
    "X_to_split = np.array(X_to_split)\n",
    "y_encoded_to_split = np.array(y_encoded_to_split)\n",
    "\n",
    "# Count samples per class\n",
    "(unique, counts) = np.unique(y_encoded_to_split, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "\n",
    "# Select classes with >=12 samples\n",
    "classes_to_keep = [cls for cls, count in class_counts.items() if count >= 50]\n",
    "\n",
    "# Filter data\n",
    "indices_to_keep = [i for i, label in enumerate(y_encoded_to_split) if label in classes_to_keep]\n",
    "X_filtered = X_to_split[indices_to_keep]\n",
    "y_filtered = y_encoded_to_split[indices_to_keep]\n",
    "\n",
    "print(f\"[INFO] Classes before filtering: {len(class_counts)}\")\n",
    "print(f\"[INFO] Classes after filtering (>=12 samples): {len(classes_to_keep)}\")\n",
    "print(f\"[INFO] Samples after filtering: {len(y_filtered)}\")\n",
    "\n",
    "# --- Step 1: Encode filtered labels to contiguous integers ---\n",
    "final_label_encoder = LabelEncoder()\n",
    "y_filtered_contiguous = final_label_encoder.fit_transform(y_filtered)\n",
    "\n",
    "num_classes_final = len(final_label_encoder.classes_)\n",
    "\n",
    "# --- Step 2: Train/Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filtered,\n",
    "    y_filtered_contiguous,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_filtered_contiguous\n",
    ")\n",
    "\n",
    "print(f\"\\n[INFO] XGBoost Training Samples: {X_train.shape[0]}, Test Samples: {X_test.shape[0]}\")\n",
    "print(f\"[INFO] Number of classes (filtered): {num_classes_final}\")\n",
    "\n",
    "# --- Step 3: Train XGBoost Classifier ---\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',  # use 'multi:softprob' if you want probabilities\n",
    "    num_class=num_classes_final,\n",
    "    eval_metric='mlogloss',\n",
    "    use_label_encoder=False,\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# --- Step 4: Predict and evaluate ---\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "used_labels = [int(i) for i in unique_labels(y_test, y_pred)]\n",
    "target_names = [str(final_label_encoder.classes_[i]) for i in used_labels]\n",
    "\n",
    "print(\"\\n[CLASSIFICATION REPORT]\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    labels=used_labels,\n",
    "    target_names=target_names,\n",
    "    zero_division=0\n",
    "))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
